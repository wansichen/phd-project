{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53ff053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch import functional as F\n",
    "\n",
    "from torch.nn import Module\n",
    "from scipy.stats import pareto \n",
    "from torch.distributions.pareto import Pareto\n",
    "from torch.nn import init\n",
    "seed=0\n",
    "#torch.cuda.manual_seed_all(seed)\n",
    "with torch.cuda.device('cuda:0'):\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb10dcd",
   "metadata": {},
   "source": [
    "trying out getting the weights and bias's gradient\n",
    "should be close to 0 if the value are close to equilibrium\n",
    "am thinking if they have any relation to the fat tail nature of the distribution(Adam\n",
    "and if the weight and bias are related in any way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcd3d1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tuning=1\n",
    "## initial =1\n",
    "class Linear(Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
    "\n",
    "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
    "          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
    "        - Output: :math:`(*, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Linear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, bias: bool = True,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        #if reset=True:\n",
    "            \n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n",
    "        # https://github.com/pytorch/pytorch/issues/57109\n",
    "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound =weight_tuning/ math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "            init.uniform_(self.weight, -bound,bound)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f6ad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "#torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from torch.autograd.functional import jacobian\n",
    "#import torch.autograd.functional\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "torch.set_default_dtype(torch.float64)\n",
    "batch_size=100\n",
    "input_size=784\n",
    "output_size=784\n",
    "hidden_state_size=10\n",
    "N=28\n",
    "no_of_layer=2\n",
    "\n",
    "\n",
    "\n",
    "font = {\n",
    "        'size'   : 26}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_shape,hidden_layer_shape,encoder_output_shape,n,xinit=torch.eye(batch_size,input_size),xfinal=torch.eye(batch_size,input_size),hidden=torch.eye(10,input_size)):\n",
    "        super().__init__()\n",
    "        self.encoder_input_layer = Linear(\n",
    "            in_features=input_shape, out_features=hidden_layer_shape\n",
    "        )\n",
    "        self.hidden_layer=Linear(\n",
    "            in_features=hidden_layer_shape, out_features=hidden_layer_shape\n",
    "        )\n",
    "        self.encoder_output_layer = Linear(\n",
    "            in_features=hidden_layer_shape, out_features=encoder_output_shape\n",
    "        )\n",
    "        self.decoder_input_layer = Linear(\n",
    "            in_features=encoder_output_shape, out_features=hidden_layer_shape\n",
    "        )\n",
    "        self.decoder_output_layer = Linear(\n",
    "            in_features=hidden_layer_shape, out_features=input_shape\n",
    "        )\n",
    "        #self.encoder_input_layer.weight=torch.nn.Parameter(torch.rand(self.encoder_input_layer.weight.size))\n",
    "        \n",
    "        self.number_of_layers=n\n",
    "        self.xinit=xinit\n",
    "        self.xfinal=xfinal\n",
    "        self.hidden=hidden\n",
    "\n",
    "    def forward(self, features):\n",
    "        reconstructed=self.internals(features)\n",
    "        return reconstructed\n",
    "    \n",
    "    def internals(self,features):\n",
    "        code=self.encoder(features)\n",
    "        self.hidden=code\n",
    "        #print(\"hidden state:\"+str(code))\n",
    "        reconstructed=self.decoder(code)\n",
    "        return reconstructed\n",
    "    \n",
    "    def encoder(self,features):\n",
    "        activation = self.encoder_input_layer(features)\n",
    "        x = torch.relu(activation)\n",
    "        self.xinit=x\n",
    "        for i in range(self.number_of_layers):\n",
    "            x=torch.relu(self.hidden_layer(x))\n",
    "        code = self.encoder_output_layer(x)\n",
    "        result = torch.relu(code)\n",
    "        return code\n",
    "\n",
    "    def decoder(self,code):\n",
    "        activation = self.decoder_input_layer(code)\n",
    "        x = torch.relu(activation)\n",
    "        for i in range(self.number_of_layers):\n",
    "             x=torch.relu(self.hidden_layer(x))\n",
    "        self.xfinal=x\n",
    "        activation = self.decoder_output_layer(x)\n",
    "        reconstructed = torch.relu(activation)\n",
    "        return reconstructed\n",
    "    \n",
    "    def xfinals(self):\n",
    "        return self.xfinal\n",
    "    \n",
    "    def xinits(self):\n",
    "        return self.xinit\n",
    "    \n",
    "    def hiddens(self):\n",
    "        return self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3201a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#transform=\n",
    "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
    "                                                transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
    "                                               transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "]))  \n",
    "train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                           batch_size=batch_size,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                          batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b670ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "model = AE(input_size,output_size,hidden_state_size,no_of_layer).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "\n",
    "optimizer =torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#print(model.encoder_input_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b6b02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "epochs=3000\n",
    "asymptotic_repetition=150\n",
    "\n",
    "\n",
    "#projected_jacobian=[]\n",
    "ave_len=5\n",
    "\n",
    "\n",
    "no_of_images,no_of_repetition=20,50\n",
    "spectral_calculation=5\n",
    "colour=np.arange(1,no_of_repetition+1)\n",
    "perturbation_strength=10**(-6)\n",
    "noise_strength=10**(-3)\n",
    "interval=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f1be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(inputs,strength):\n",
    "    #print(inputs.size())\n",
    "    noise = torch.randn_like(inputs)*strength*torch.max(inputs)\n",
    "    result=inputs + noise\n",
    "    return result\n",
    "def add_powerlaw_noise(input_size,batch_size,strength,images):\n",
    "    ##input size here is the side of the image 28 \n",
    "    m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n",
    "    res=m.sample(images.size())\n",
    "\n",
    "    temp=res/np.sqrt(res.var())/N/batch_size*strength\n",
    "    noise=temp-temp.mean()\n",
    "    #if print_bool:\n",
    "     #   plt.title(powerlaw_noise print)\n",
    "      #  plt.hist(noise.flatten(),density=True, bins='auto', histtype='stepfilled')\n",
    "        \n",
    "      #  plt.show()\n",
    "    #print(noise[:,:,:,:,0].size())\n",
    "    return noise[:,:,0]+images\n",
    "\n",
    "def validation(test_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "     #valid_loss = criterion(outputs, batch_features.view(batch_size,-1).to(device))\n",
    "        loss=0\n",
    "        for batch_features, _ in test_loader:\n",
    "            #input=add_noise(batch_features,).view(100, -1).to(device)\n",
    "            input_image=add_gaussian_noise(batch_features,noise_strength).view(batch_size, -1).to(device)\n",
    "            #batch_feature = batch_features.view(batch_size, -1)\n",
    "            #input_image =add_powerlaw_noise(input_size,batch_size,noise_strength,batch_feature).view(batch_size, -1).to(device)\n",
    "            batch_feature = batch_features.view(batch_size, -1).to(device)\n",
    "            outputs = model(input_image)\n",
    "            valid_loss = criterion(outputs, input_image.to(device)) \n",
    "            #accuracy+=kl_div(outputs,batch_features)/batch_size\n",
    "            loss += valid_loss.item()\n",
    "    return loss/len(test_loader)\n",
    "            \n",
    "def kl_div(output_image,input_image):\n",
    "    accuracy=0\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    for i in range(len(input_image)):\n",
    "        input_spectrum=torch.histogram(input_image[i].cpu(), bins=256, density=True)\n",
    "        output_spectrum=torch.histogram(output_image[i].cpu(), bins=256, density=True)\n",
    "        accuracy+=kl_loss(input_spectrum[i],output_spectrum[i])\n",
    "    return accuracy/len(input_image)\n",
    "\n",
    "def iteration(model,initial_inputs,no_of_repetition,epoch):\n",
    "    y_pred=initial_inputs.to(device)\n",
    "    for i in range(no_of_repetition):\n",
    "        x_train=y_pred\n",
    "        #if i==0:\n",
    "           # name=\"progress asymptotic iteration:\"+str(i)+\" \"\n",
    "           # visualisation(x_train[0].cpu().detach(),epoch,name,False)\n",
    "        #if i%5==1:\n",
    "            #name=\"progress asymptotic iteration:\"+str(i)+\" \"\n",
    "            #visualisation(x_train[0].cpu().detach(),epoch,name,False)\n",
    "        y_pred=model.internals(x_train)\n",
    "    return x_train,y_pred,model \n",
    "\n",
    "def asymptotic_jacobian(model,initial_input,no_of_images,no_of_repetition):\n",
    "    ave_jac=0\n",
    "    overall_distribution=np.asarray([])\n",
    "\n",
    "    jacobian=[]\n",
    "    sorteds=np.asarray([])\n",
    "    for j in range(no_of_repetition):\n",
    "        #print(\"here\")\n",
    "        x_train=initial_input.to(device)\n",
    "        y_pred=x_train\n",
    "        mean_jacobian=0\n",
    "        for i in range(no_of_images):\n",
    "            x=x_train[i]\n",
    "            res=torch.autograd.functional.jacobian(model.internals,x)\n",
    "            distribution=np.asarray([])\n",
    "            jacobian.append(res)\n",
    "            sorted=np.asarray([])\n",
    "            #if j==no_of_repetition-1:\n",
    "             #   distribution,sorted=spectral_distribution(res.cpu())\n",
    "              #  overall_distribution=np.concatenate((overall_distribution,distribution),axis=0)\n",
    "              #  spectral=spectral_radius(sorted)\n",
    "              #  sorteds=np.append(sorteds,spectral)\n",
    "\n",
    "            norm=torch.norm(res).cpu()\n",
    "            mean_jacobian+=1/np.sqrt(output_size)*norm\n",
    "        mean_jacobian=mean_jacobian/no_of_images\n",
    "        ave_jac+=mean_jacobian\n",
    "    average_jacobian=ave_jac/no_of_repetition\n",
    "    return average_jacobian,overall_distribution,jacobian,sorteds\n",
    "\n",
    "def spectral_radius(jacobian,no_of_repetition,no_of_images):\n",
    "    #result=[]\n",
    "   \n",
    "    #output=initial_input.to(device)\n",
    "   # product=torch.eye(n=input_size,m=output_size).to(device)\n",
    "    \n",
    "    spectral=0\n",
    "\n",
    "    \n",
    "        \n",
    "    #res=torch.autograd.functional.jacobian(model.internals,initial_input)\n",
    "    #product=torch.matmul(product,res)\n",
    "    #output=model(initial_input)\n",
    "   # initial_input=output\n",
    "    \n",
    "    s,v=torch.linalg.eig(jacobian)\n",
    " #   #print(s)\n",
    "    abs=torch.abs(s)\n",
    "    \n",
    "    spectral=torch.max(abs).item()\n",
    "    return spectral\n",
    "\n",
    "def poincare_plot(model,initial_input,dimension_vector,no_of_repetition,colour,epoch):\n",
    "    xt=[]\n",
    "    xtminus=[]\n",
    "    \n",
    "    output=initial_input\n",
    "    \n",
    "    for i in range(no_of_repetition):\n",
    "        \n",
    "        initial_input=output\n",
    "        output=model(initial_input)\n",
    "        \n",
    "        xt.append(1/output_size*torch.dot(output,dimension_vector).item())\n",
    "        xtminus.append(1/output_size*torch.dot(initial_input,dimension_vector).item())\n",
    "    \n",
    "    return xt,xtminus\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "def asymptotic_distance(xinfinity_unperturbed,xinfinity_perturbed,perturbation):\n",
    "    result=[]\n",
    "    for i in range(len(xinfinity_unperturbed)):\n",
    "    \n",
    "        sum=0\n",
    "        \n",
    "        for j in range(len(xinfinity_unperturbed[i])):\n",
    "            \n",
    "            temp=np.linalg.norm(xinfinity_unperturbed[i][j]-xinfinity_perturbed[i][j])\n",
    "            sum+=temp\n",
    "        \n",
    "        result.append(1/output_size*1/len(xinfinity_unperturbed[i])*sum) \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def calculation(model,no_of_images,no_of_repetition,ave_jacobian,spectral_radiuses,image,epoch):\n",
    "    spectral=0\n",
    "    \n",
    "    model_clone=AE(input_size,input_size,hidden_state_size,no_of_layer).to(device)\n",
    "    model_clone.load_state_dict(copy.deepcopy(model.state_dict()))\n",
    "    distributions=np.asarray([])\n",
    "    x_train,y_pred,model_clone=iteration(model_clone,image,asymptotic_repetition,epoch)   \n",
    "    #ave_jac,distributions,jacobian,sorteds= asymptotic_jacobian(model_clone,x_train,no_of_images,no_of_repetition)\n",
    "    \n",
    "    #np.savetxt(\"jacobian epoch\"+str(epoch)+\".txt\",sorteds)\n",
    "    #print(\"distribution size\")\n",
    "    #print(distributions.size)\n",
    "    #print(\"sorted size\")\n",
    "    #print(sorteds.size)\n",
    "    #average_jacobian.append(ave_jac)\n",
    "    #x = [ele.real for ele in distributions]\n",
    "    ## extract imaginary part\n",
    "    #y = [ele.imag for ele in distributions]\n",
    "    #plt.title(\"real and imaginary part of eigenvalue\")\n",
    "    #plt.scatter(x, y)\n",
    "    #plt.ylabel('Imaginary')\n",
    "    #plt.xlabel('Real')\n",
    "    #plt.xscale(\"log\")\n",
    "    #plt.yscale(\"log\")\n",
    "    #plt.xlim(-1,1)\n",
    "    #plt.ylim(-1,1)\n",
    "    #plt.savefig(\"epoch:\"+str(epoch+1)+\"number of iteration:\"+str(no_of_repetition)+\"eigenvalue scatter plot.jpg\",bbox_inches = 'tight')\n",
    "    #plt.show()\n",
    "    #plt.title(\"modulus of eigenvalue in log log plot \")\n",
    "    #plt.hist(sorteds, density=True, bins='auto', histtype='stepfilled')\n",
    "    #plt.yscale(\"log\")\n",
    "    #plt.xscale(\"log\")\n",
    "\n",
    "    #plt.savefig(\"epoch:\"+str(epoch+1)+\"number of iteration:\"+str(no_of_repetition)+\"eigenvalue distribution.jpg\",bbox_inches = 'tight')\n",
    "    #plt.show()\n",
    "\n",
    "   # for i in range(no_of_images):\n",
    "    \n",
    "        #spectral+=spectral_radius(sorteds,no_of_repetition,no_of_images)\n",
    "    #spectral=sorteds.mean()\n",
    "    \n",
    "    #spectral_radiuses.append(spectral)\n",
    "    #print(spectral/no_of_images)\n",
    "    \n",
    "    return x_train,y_pred,average_jacobian,spectral_radiuses,model_clone\n",
    "\n",
    "\n",
    "def asymptotic_iteration(model_new,initial_inputs,perturbed_inputs,no_of_repetition,no_of_image,cutoff):\n",
    "    \n",
    "    y_pred_unperturbed=initial_inputs.to(device)\n",
    "    y_pred_perturbed=perturbed_inputs.to(device)\n",
    "    #cutoff=[]\n",
    "   \n",
    "    #for j in range(no_of_image):\n",
    "     \n",
    "    #cutoff.append(torch.norm(y_pred_unperturbed[j]-y_pred_perturbed[j]))\n",
    "        #print(cutoff[j])\n",
    "    distance=[]\n",
    "    hidden_layer=[]\n",
    "    hiddens=[]\n",
    "    chaos=0\n",
    " \n",
    "    for i in range(no_of_repetition):\n",
    "        x_train_unperturbed=y_pred_unperturbed\n",
    "        y_pred_unperturbed=model_new.internals(x_train_unperturbed)\n",
    "        \n",
    "       \n",
    "        x_train_perturbed=y_pred_perturbed\n",
    "        y_pred_perturbed=model_new.internals(x_train_perturbed)\n",
    "        \n",
    "       # print(y_pred_perturbed,y_pred_unperturbed)\n",
    "        \n",
    "        hidden = model.hiddens()\n",
    "        #print(hidden)\n",
    "        #print(torch.sum(hidden,dim=1).cpu().detach().numpy()/1000)\n",
    "        hidden_layer.append(torch.sum(hidden,dim=1)[0].cpu().detach().numpy()/10)\n",
    "        hiddens.append(torch.sum(hidden,dim=1).cpu().detach().numpy()/10)\n",
    "        \n",
    "        #visualisation(y_pred.view(batch_size,-1)[0].cpu().detach(),epoch)\n",
    "\n",
    "\n",
    "        #diff=0\n",
    "        diff=torch.zeros(1,device=device)\n",
    "        temp=0\n",
    "        #print(len(cutoff))\n",
    "        for j in range(no_of_image):\n",
    "            result=torch.norm(y_pred_unperturbed[j]-y_pred_perturbed[j])\n",
    "            #print(np.isnan(result.cpu().detach().numpy()))\n",
    "            #print(result)\n",
    "            if np.isinf(result.cpu().detach().numpy()) or np.isnan(result.cpu().detach().numpy())  :\n",
    "                result=torch.Tensor([1*10**38]).to(device)\n",
    "                print(\"infinity\")\n",
    "            temp=result\n",
    "            diff+=result\n",
    "            \n",
    "            #print(j)\n",
    "            if result>cutoff[j] and i==no_of_repetition-1:\n",
    "                chaos+=1\n",
    "            if np.isinf(diff.cpu().detach().numpy()) or np.isnan(diff.cpu().detach().numpy()):\n",
    "                diff=torch.Tensor([1*10**38]).to(device)\n",
    "                print(\"infinity\")\n",
    "            #print(\"asymptotic calculation\")\n",
    "            #print(result.cpu())\n",
    "            #print(diff.cpu())\n",
    "            \n",
    "        if diff.cpu().detach().numpy()<2**-52:\n",
    "            diff=2**-52\n",
    "            distance.append(diff)\n",
    "            continue\n",
    "        distance.append(diff.cpu().detach().numpy()/no_of_image)\n",
    "\n",
    "    return distance,hidden_layer,chaos/no_of_image\n",
    "\n",
    "def visualisation(xfinals,epoch,name,bool):\n",
    "    plt.imshow(xfinals.reshape(N,N), cmap=\"gray\")\n",
    "    if bool:\n",
    "        plt.savefig(str(name)+\" epoch:\"+str(epoch+1)+\".jpg\")\n",
    "    plt.show()\n",
    "    print(str(name)+\" epoch:\"+str(epoch+1)+\".jpg\")\n",
    "    \n",
    "def divergence(values):\n",
    "    result=np.abs(values[-1]-values[-2])\n",
    "    if result>np.abs(values[2]-values[1]):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def spectral_radius(sorted):\n",
    "    if len(sorted)==0:\n",
    "        return 0\n",
    "    return sorted[0]\n",
    "\n",
    "def spectral_distribution(input_matrix):\n",
    "    result=[]\n",
    "    count=0\n",
    "    s,v=torch.linalg.eig(input_matrix)\n",
    "    #return s\n",
    "    for i in range(len(s)):\n",
    "        if np.abs(s[i].cpu())<2**-52:\n",
    "            count+=1\n",
    "        \n",
    "    sorted, indices=torch.sort(torch.abs(s),dim=-1,descending=True)\n",
    "    #for index in indices.cpu():\n",
    "     #   if index<=len(s)-count:\n",
    "     #       result.append(s[index])\n",
    "    #print(sorted)\n",
    "    return s,sorted[:-count]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba37975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "tensor([[-0.0091, -0.0206,  0.0049,  ..., -0.0145,  0.0098,  0.0304],\n",
      "        [-0.0014,  0.0130, -0.0193,  ...,  0.0019,  0.0019, -0.0106],\n",
      "        [ 0.0357,  0.0199, -0.0254,  ..., -0.0209, -0.0188,  0.0257],\n",
      "        ...,\n",
      "        [ 0.0251, -0.0329,  0.0226,  ...,  0.0303, -0.0213, -0.0316],\n",
      "        [ 0.0053, -0.0288, -0.0250,  ...,  0.0107, -0.0338, -0.0322],\n",
      "        [-0.0098, -0.0326, -0.0048,  ...,  0.0038,  0.0050,  0.0329]],\n",
      "       device='cuda:0')\n",
      "tensor([-8.4108e-03, -5.6222e-03,  3.2076e-02,  3.4713e-03,  2.0069e-02,\n",
      "         8.7890e-03,  2.7513e-02, -3.4093e-02,  3.7776e-03,  9.9353e-03,\n",
      "         3.2594e-02,  3.0325e-02,  1.7343e-04, -1.4643e-02, -2.9198e-03,\n",
      "         3.2795e-02, -3.3520e-03,  1.2115e-02, -2.4669e-02, -2.2301e-02,\n",
      "         2.2518e-02,  3.5688e-02, -1.8941e-03,  1.3472e-02, -1.8726e-03,\n",
      "         3.1455e-02,  7.4987e-03,  3.1342e-02, -2.3619e-02,  3.1357e-03,\n",
      "         1.8558e-02, -4.8236e-03,  1.2206e-02,  3.8405e-03,  2.9065e-02,\n",
      "         2.6670e-02, -1.6449e-02,  6.6514e-03, -5.1599e-03,  4.7223e-03,\n",
      "        -2.4029e-02, -2.5597e-02,  2.7296e-02, -2.2987e-02, -3.3654e-02,\n",
      "         2.8363e-02,  5.5648e-03,  9.1546e-03,  9.5039e-03,  3.0615e-02,\n",
      "        -1.5025e-02,  1.1198e-03, -3.1663e-02,  7.2134e-03,  5.3673e-03,\n",
      "         2.5771e-02,  2.8205e-03, -1.8994e-02, -2.6144e-02,  1.4249e-02,\n",
      "        -2.6265e-02, -5.9278e-03, -2.7459e-02,  2.4100e-02,  1.8137e-02,\n",
      "         2.1173e-02, -1.9872e-02, -2.4082e-02,  2.2919e-02, -1.0773e-02,\n",
      "        -2.7359e-02, -4.5376e-04,  1.5367e-03,  5.7133e-04,  5.9003e-05,\n",
      "         3.3373e-02,  4.2973e-03,  1.3358e-02,  1.0236e-02,  2.6844e-02,\n",
      "         2.7016e-03, -2.6678e-02, -1.3519e-02, -4.1615e-03,  1.7281e-02,\n",
      "         1.5570e-02, -4.9991e-03,  9.0089e-03, -2.7326e-02, -1.5968e-02,\n",
      "         3.2857e-02,  2.9215e-02, -2.3093e-02, -1.2534e-02,  1.5442e-02,\n",
      "        -1.5953e-02,  4.7966e-03, -2.1937e-02,  2.3496e-02, -3.4870e-03,\n",
      "        -2.2149e-02,  3.2029e-02, -8.4322e-03, -2.8347e-03, -2.6380e-02,\n",
      "        -3.1924e-02, -3.0474e-02, -8.3228e-03,  2.3723e-02,  3.3807e-03,\n",
      "        -3.4983e-02,  3.3348e-02, -6.5740e-03, -1.6230e-02, -3.4012e-02,\n",
      "         2.1566e-02, -6.0572e-03, -8.1212e-03,  1.1438e-02,  6.4282e-03,\n",
      "        -1.6786e-02,  8.0402e-03,  2.2470e-03, -3.0502e-02,  2.3271e-03,\n",
      "         9.7606e-03, -2.5968e-02, -1.1929e-02,  3.2155e-02,  1.6540e-02,\n",
      "        -1.0853e-04, -3.5332e-02, -2.1858e-02,  2.8465e-02,  1.5532e-02,\n",
      "         7.9033e-03,  1.7856e-02,  5.7109e-03, -6.1833e-03, -3.4810e-02,\n",
      "        -2.2457e-02, -2.9860e-02,  3.4889e-02, -3.3322e-02,  2.1266e-03,\n",
      "         3.3763e-03,  2.2756e-02,  2.4162e-02,  8.3665e-03,  1.8002e-02,\n",
      "         2.2147e-02, -2.2807e-03,  1.4459e-02,  2.5480e-02, -1.0356e-02,\n",
      "        -1.9237e-02,  9.6139e-03, -2.4924e-02, -1.0537e-02, -1.9915e-02,\n",
      "        -8.5587e-03, -2.7617e-02, -1.9213e-03,  1.8281e-02, -3.3846e-02,\n",
      "         2.8767e-02, -7.7049e-03, -9.3118e-03,  3.0566e-02, -4.3490e-03,\n",
      "        -2.1496e-02,  2.3452e-02, -8.9264e-03, -7.7602e-03, -9.7227e-03,\n",
      "         6.1827e-03,  1.9675e-02, -1.4973e-02,  2.5216e-02, -2.8634e-02,\n",
      "        -3.2935e-02,  2.5722e-02,  8.3559e-03,  6.0655e-03,  3.2255e-02,\n",
      "         2.4711e-02, -7.2672e-03,  1.0953e-02,  3.0850e-02,  1.9164e-03,\n",
      "        -2.9738e-02, -7.3662e-03, -1.2212e-02,  1.1214e-02, -3.5418e-02,\n",
      "         2.9204e-02,  1.9470e-02, -2.1324e-02, -3.3227e-02,  5.4892e-03,\n",
      "         8.7688e-03,  7.6234e-03, -1.6567e-02, -2.1063e-02,  4.8010e-03,\n",
      "         3.3422e-03,  2.7291e-02, -3.2656e-02, -2.3568e-02, -2.6986e-02,\n",
      "        -1.9152e-02, -2.5698e-02,  2.1069e-02, -3.5316e-02, -3.9717e-04,\n",
      "         2.6591e-03,  7.7932e-03,  7.7912e-03,  3.4919e-02, -1.2233e-02,\n",
      "        -2.9685e-02, -1.7551e-03,  1.5697e-02,  1.2882e-02, -8.3079e-03,\n",
      "         1.6793e-02,  2.2743e-02,  2.0460e-02,  2.8148e-02, -9.5956e-03,\n",
      "        -6.3455e-03, -4.2730e-03, -3.1415e-02, -1.5665e-02, -1.2040e-02,\n",
      "        -2.8655e-03,  1.8414e-03,  2.0889e-02,  3.3641e-02, -4.8851e-03,\n",
      "         2.2046e-02, -2.1914e-02,  3.1693e-02,  1.8363e-02, -3.3805e-02,\n",
      "         3.5498e-02,  8.5409e-03, -5.7872e-03,  2.5175e-02, -3.4544e-02,\n",
      "         3.4749e-02, -2.9358e-02,  3.3374e-02,  6.5767e-03, -3.5331e-02,\n",
      "        -1.6723e-02, -1.9929e-02,  2.8680e-02,  2.9775e-02,  8.4120e-03,\n",
      "         3.3529e-02, -1.7800e-02, -2.7148e-02, -1.3562e-02, -1.8778e-02,\n",
      "        -1.6217e-02, -1.9946e-04, -1.0747e-02, -2.8889e-02,  1.3900e-02,\n",
      "         3.3704e-02, -1.6398e-02,  1.2167e-02,  3.4025e-02,  1.5843e-02,\n",
      "        -2.9052e-02,  5.6364e-03,  1.3191e-02, -2.3818e-02, -2.5445e-02,\n",
      "         1.4039e-03,  2.4765e-02,  3.4565e-02, -2.2945e-02, -2.3265e-02,\n",
      "        -2.4308e-02, -1.6451e-02,  2.2420e-02,  5.6463e-03, -2.0426e-02,\n",
      "        -2.9704e-02, -2.1888e-02, -2.0291e-02,  1.4855e-02, -3.0277e-02,\n",
      "        -6.7174e-03, -3.4445e-02,  1.6274e-02,  5.6198e-03,  1.8885e-02,\n",
      "        -2.6831e-02, -1.3157e-02, -2.2429e-02,  3.5582e-02, -1.8543e-02,\n",
      "         3.4449e-02, -2.5360e-03, -6.0232e-03, -1.9441e-02, -3.5387e-02,\n",
      "         2.3408e-02,  1.2516e-02,  2.3743e-02, -1.2274e-03, -3.2340e-02,\n",
      "         2.5362e-02,  1.8431e-02, -8.1007e-03, -1.3328e-02,  7.1918e-03,\n",
      "        -1.0736e-02, -2.7010e-02,  1.0310e-02,  2.4966e-02, -1.1608e-02,\n",
      "         2.1903e-02,  4.3652e-03,  2.6540e-02, -1.3558e-02, -1.3435e-02,\n",
      "        -5.5738e-04, -2.0438e-02, -1.2914e-02, -6.2456e-03, -3.4630e-02,\n",
      "        -2.1664e-02, -7.1032e-04, -1.6237e-02, -2.5068e-02,  1.5984e-02,\n",
      "         5.4699e-03,  2.5170e-02, -3.2477e-02, -3.1980e-02, -3.2575e-02,\n",
      "         3.4180e-02, -1.3823e-02,  7.3857e-03, -1.1602e-02,  9.1529e-04,\n",
      "        -1.1847e-03, -1.6333e-02,  2.4963e-02, -2.3917e-02,  3.2854e-02,\n",
      "         6.7369e-03,  2.9124e-02, -3.4014e-02,  2.3404e-02,  5.8610e-03,\n",
      "        -2.9709e-02,  2.0802e-02, -3.4299e-02,  2.6419e-02,  3.1004e-02,\n",
      "         2.8134e-02,  2.2530e-02, -1.9313e-02, -9.7335e-03,  3.5677e-02,\n",
      "        -2.8303e-03, -2.2486e-02,  3.1350e-03,  4.4913e-03,  6.8661e-03,\n",
      "        -3.3930e-02, -1.8386e-02, -1.7563e-04,  2.5170e-02, -3.4409e-02,\n",
      "         1.7675e-02,  1.3918e-02, -3.3323e-02, -3.3115e-02, -1.1396e-02,\n",
      "         3.2418e-02, -3.0866e-02,  1.1615e-02,  1.3104e-02,  5.6221e-03,\n",
      "         1.8972e-02, -4.2211e-03, -2.1016e-02, -3.5005e-02, -1.9255e-02,\n",
      "         3.3866e-02, -3.1683e-02, -7.5176e-03,  1.7666e-02,  1.5919e-02,\n",
      "         4.3680e-03,  3.9267e-03,  2.2969e-02,  4.6006e-03,  3.0953e-02,\n",
      "         3.4237e-02,  2.3785e-03,  5.2449e-03,  2.0466e-02, -2.3207e-02,\n",
      "        -2.1570e-02, -1.3600e-02, -8.8049e-03, -1.7582e-02,  2.6854e-02,\n",
      "         2.9294e-02,  4.3305e-03, -3.1361e-02,  8.4648e-03, -6.8334e-03,\n",
      "        -1.1883e-02, -2.8032e-02, -2.0823e-02,  3.4038e-02,  2.2333e-02,\n",
      "        -3.1463e-02,  1.1898e-02,  2.1490e-03,  2.5436e-03,  3.9604e-03,\n",
      "         5.1444e-03, -2.6229e-02,  3.4399e-02, -7.4367e-04,  1.3754e-03,\n",
      "         9.7335e-03,  2.5559e-03, -6.0794e-03,  2.7225e-02,  1.9736e-02,\n",
      "         2.0561e-02, -1.9732e-02,  7.1164e-03, -3.0427e-02, -1.3850e-02,\n",
      "        -3.2271e-02,  5.3964e-03,  2.1691e-02, -2.4956e-02, -3.0858e-02,\n",
      "         5.4216e-03,  3.5255e-03,  3.2032e-02,  1.6977e-02, -2.0772e-02,\n",
      "         2.8133e-03, -1.0867e-02,  2.0457e-02,  9.7654e-03, -2.9759e-02,\n",
      "         2.5407e-02,  3.3666e-02, -3.4042e-02,  9.8538e-03,  1.6534e-02,\n",
      "         2.2046e-02,  8.6196e-03, -2.7971e-03,  2.5997e-02, -3.4429e-02,\n",
      "         7.8777e-03,  2.9498e-02, -2.1785e-02, -1.5227e-04, -2.5429e-02,\n",
      "         3.3466e-02,  3.0468e-02,  2.9928e-03,  1.5483e-02,  4.4318e-04,\n",
      "        -3.2329e-02,  3.1205e-02, -5.1140e-03, -1.8682e-02, -2.6575e-02,\n",
      "         9.6120e-03,  2.7007e-02, -2.7612e-02, -3.2879e-02, -1.2491e-02,\n",
      "         5.5431e-03,  3.5703e-02, -9.4847e-03,  3.0667e-02,  7.2256e-03,\n",
      "         1.6931e-02, -1.9549e-02, -7.2504e-04, -7.6218e-03, -2.2248e-03,\n",
      "         3.2650e-02, -8.1727e-03, -2.3280e-02,  1.8852e-02, -1.7653e-02,\n",
      "        -3.1380e-02, -3.2092e-02, -1.4009e-02, -3.5713e-02, -1.6323e-02,\n",
      "         8.0321e-03, -2.9174e-02, -2.5960e-02, -2.7083e-02, -1.2642e-02,\n",
      "        -6.7658e-03,  1.6030e-02,  1.2596e-02, -5.1865e-04,  4.6189e-03,\n",
      "         9.6498e-03,  2.5265e-02, -3.4515e-02, -3.1845e-02,  1.1894e-02,\n",
      "         2.5791e-02,  3.2021e-02,  1.7898e-02, -3.3982e-03,  2.8246e-02,\n",
      "         8.7193e-03,  2.1814e-03,  1.7623e-02, -1.3077e-02, -2.6717e-02,\n",
      "        -1.8967e-02,  2.0861e-02, -3.5495e-02,  1.5087e-02,  3.1892e-02,\n",
      "         2.9301e-02, -3.1921e-02, -5.9592e-03,  6.9808e-03, -1.6163e-02,\n",
      "        -1.8040e-03,  2.5452e-02,  3.2243e-02,  2.2247e-03,  3.0744e-02,\n",
      "         9.6777e-03, -3.1887e-02, -2.8492e-02,  2.4816e-03, -1.1217e-02,\n",
      "        -7.0478e-03,  2.5237e-03,  3.4491e-02,  9.8387e-03,  2.2834e-03,\n",
      "         2.4092e-02, -2.1370e-02, -2.5924e-02, -1.4033e-02, -2.2021e-02,\n",
      "        -2.8663e-02, -1.3043e-02, -2.6472e-02,  2.2196e-02,  2.5430e-02,\n",
      "        -6.8492e-03,  3.3367e-02, -2.4357e-02,  1.8933e-02, -3.0074e-02,\n",
      "        -3.1436e-02,  3.3477e-02,  3.0726e-02, -1.4442e-02, -2.3731e-02,\n",
      "        -3.9427e-03, -3.1664e-02,  1.3579e-02,  3.8733e-03, -2.8051e-02,\n",
      "         2.5932e-02, -3.3266e-02,  1.2758e-03,  2.6656e-02, -1.4365e-02,\n",
      "        -1.3841e-05,  1.3629e-02, -3.4394e-02,  2.9409e-02, -8.9704e-03,\n",
      "        -2.1010e-02, -6.5443e-04,  7.1818e-05, -3.1441e-02,  1.4310e-02,\n",
      "         2.7681e-03, -1.0315e-02,  2.2604e-02,  2.1491e-02, -1.7181e-02,\n",
      "         3.1693e-02,  1.2501e-02, -2.3605e-02,  2.8732e-02, -2.3945e-02,\n",
      "         2.0824e-02, -2.8495e-03,  7.8441e-03,  2.3680e-02,  2.8459e-02,\n",
      "        -2.2992e-02, -1.4302e-02, -3.1105e-02, -3.5136e-02,  3.2123e-02,\n",
      "        -1.4118e-02,  1.3668e-02, -2.7360e-02, -2.0815e-02,  1.4634e-02,\n",
      "         3.1789e-02, -3.1439e-02,  9.5703e-03,  6.6771e-03,  2.0923e-02,\n",
      "         3.4125e-02, -1.7864e-02,  1.1640e-02,  2.8125e-02,  1.6410e-02,\n",
      "        -1.3967e-02,  1.6635e-03, -2.2815e-02, -1.1995e-02, -9.9645e-03,\n",
      "         3.0443e-02,  1.4087e-02, -3.0335e-02, -2.9610e-02,  1.8319e-03,\n",
      "        -3.2181e-02,  2.9509e-02,  1.3994e-03, -2.9109e-02, -2.3850e-02,\n",
      "        -1.3368e-02, -2.0482e-02, -1.8029e-02,  1.9191e-02, -2.9613e-02,\n",
      "        -5.8985e-03,  1.4759e-02, -1.9461e-02,  2.9292e-02, -2.4278e-02,\n",
      "         9.3436e-03, -6.6312e-03,  2.3454e-02,  2.7831e-02, -3.3768e-02,\n",
      "         1.7228e-02, -2.3293e-02, -3.1028e-02,  3.2630e-02,  2.7413e-02,\n",
      "         5.3093e-03,  3.0578e-02, -2.4658e-02,  7.4769e-03,  1.6337e-02,\n",
      "         2.8773e-02, -2.6800e-02,  1.5797e-02,  1.8324e-02, -3.4105e-02,\n",
      "        -5.9275e-04,  1.6121e-02, -1.8530e-02, -1.9963e-02, -1.6909e-02,\n",
      "         2.6823e-03, -3.2388e-03,  1.2987e-02, -2.6967e-02, -9.7011e-03,\n",
      "         2.1016e-02,  1.0312e-02,  2.5246e-04, -1.6509e-02, -2.4080e-02,\n",
      "        -2.4994e-02,  3.4806e-02,  1.9282e-03, -2.9600e-02, -1.4754e-02,\n",
      "         1.1663e-02,  1.1996e-02, -3.3322e-02,  6.8558e-03, -2.0738e-03,\n",
      "        -1.6511e-03,  9.3290e-03, -2.7783e-02, -2.6794e-02,  1.5143e-02,\n",
      "         2.8474e-02,  1.1324e-02,  1.4408e-02,  1.5869e-02, -1.7751e-02,\n",
      "        -1.3038e-02,  2.2586e-02,  9.5684e-03, -2.0075e-02, -3.2575e-02,\n",
      "        -1.4673e-03,  1.7256e-02, -8.3789e-03, -3.5170e-02, -1.7573e-02,\n",
      "        -1.7301e-02,  1.6307e-02,  5.8029e-03,  1.3867e-02,  2.6553e-02,\n",
      "         2.1864e-02, -2.4875e-02, -3.5005e-02,  1.5288e-02, -3.5495e-02,\n",
      "         7.6793e-03, -1.6786e-02, -3.1277e-03, -9.6532e-03,  8.8234e-03,\n",
      "        -3.2388e-02, -2.9278e-02,  4.3177e-03, -5.0362e-04, -2.9376e-03,\n",
      "        -3.0782e-02,  7.5052e-03,  2.2367e-02, -3.2568e-02, -1.1538e-02,\n",
      "         1.8258e-02, -1.3094e-02, -9.8356e-03,  1.0780e-02,  2.3531e-02,\n",
      "        -2.5135e-02,  3.3097e-02,  8.0828e-04,  2.7376e-03,  1.3463e-03,\n",
      "        -8.3093e-03,  1.2746e-02,  1.5005e-02, -2.0387e-02,  2.3682e-02,\n",
      "        -2.7749e-02,  6.7827e-03,  2.2175e-03,  1.5791e-02, -2.1368e-02,\n",
      "         2.6678e-02, -2.7990e-02, -9.4722e-03,  3.0250e-02, -1.8590e-02,\n",
      "         1.7083e-02, -2.2022e-02, -2.4317e-03,  2.4751e-04, -2.1651e-02,\n",
      "        -2.7662e-03,  3.5481e-02, -1.5155e-02, -2.4638e-02], device='cuda:0')\n",
      "tensor([[ 0.0292, -0.0281, -0.0131,  ...,  0.0235,  0.0152,  0.0186],\n",
      "        [ 0.0117, -0.0002, -0.0028,  ...,  0.0089, -0.0301, -0.0265],\n",
      "        [-0.0054, -0.0181,  0.0220,  ...,  0.0153,  0.0093,  0.0277],\n",
      "        ...,\n",
      "        [-0.0041,  0.0103, -0.0230,  ..., -0.0198, -0.0184,  0.0324],\n",
      "        [-0.0150,  0.0309,  0.0354,  ...,  0.0132,  0.0103,  0.0057],\n",
      "        [ 0.0313, -0.0268, -0.0112,  ..., -0.0169,  0.0073, -0.0073]],\n",
      "       device='cuda:0')\n",
      "tensor([ 4.2343e-03,  1.5687e-02,  7.6118e-03, -6.8607e-03, -2.1180e-02,\n",
      "         2.1285e-02,  3.1728e-02,  8.8935e-03,  2.1683e-02, -1.5473e-02,\n",
      "        -2.7155e-02, -2.6059e-02, -1.3359e-02,  1.8219e-02,  7.7144e-03,\n",
      "         2.0617e-02,  1.8876e-02, -6.9505e-03, -6.4111e-03, -6.8594e-03,\n",
      "         1.2144e-02,  6.9564e-03,  1.4684e-02, -3.1021e-02,  8.5674e-03,\n",
      "        -2.9181e-02,  2.8978e-02,  3.0498e-02, -9.8128e-03, -1.4378e-02,\n",
      "        -2.1819e-02,  1.2385e-02, -6.5096e-03, -8.3000e-03,  1.8286e-02,\n",
      "         2.9047e-02, -2.1182e-03,  7.6481e-03,  1.5886e-02,  2.3515e-02,\n",
      "         1.0439e-03,  1.3303e-02, -1.9550e-02,  3.3454e-02, -1.4677e-02,\n",
      "        -2.3268e-02, -7.8724e-03,  9.4613e-03, -3.3682e-02, -3.3864e-02,\n",
      "         2.3961e-02,  2.3985e-02, -1.5999e-02, -5.5758e-04, -2.2105e-02,\n",
      "        -9.2140e-03,  2.2773e-03,  6.8460e-03,  3.0412e-02,  1.7629e-02,\n",
      "        -2.6805e-02, -5.0966e-03,  9.0506e-03,  8.7497e-03,  6.2927e-03,\n",
      "        -5.4667e-03,  6.2728e-03, -1.5066e-02, -7.4063e-03, -1.7823e-02,\n",
      "        -2.9039e-02,  4.3333e-03,  2.3723e-03,  1.4681e-03,  3.3493e-02,\n",
      "         1.3520e-02, -3.2212e-02,  6.6474e-03, -1.6921e-02, -1.9721e-02,\n",
      "         1.0453e-02,  7.1808e-03,  2.5743e-02, -1.9935e-02,  2.6031e-02,\n",
      "         2.4566e-02, -9.4981e-03,  5.4218e-03, -2.8558e-02,  3.4411e-02,\n",
      "         3.2017e-02,  1.1013e-03, -2.1760e-02, -1.1789e-02, -9.2078e-03,\n",
      "         3.0421e-02,  3.2369e-02, -3.3395e-02,  2.0755e-02,  2.4392e-02,\n",
      "         2.2563e-02, -2.0303e-02, -2.5747e-02, -1.4124e-02,  2.2280e-02,\n",
      "         1.6195e-02,  9.3333e-03,  6.0428e-03, -3.8342e-03, -1.6974e-03,\n",
      "         3.5192e-02,  2.8310e-02, -2.4927e-02,  9.4168e-03, -3.4519e-02,\n",
      "        -2.0817e-02, -3.2725e-02,  4.1468e-03,  3.2929e-02, -2.0020e-02,\n",
      "        -1.5872e-02, -2.2262e-02, -1.9601e-02,  3.2390e-03,  3.2989e-02,\n",
      "        -5.0163e-04,  1.1407e-02,  1.5958e-02,  7.0020e-03, -3.2125e-02,\n",
      "         1.2357e-02, -3.5526e-02, -2.0424e-02,  3.4736e-02,  1.1321e-02,\n",
      "        -3.1800e-02,  9.4745e-03,  2.0054e-02,  3.4978e-02,  9.3852e-04,\n",
      "        -2.6868e-02,  1.5314e-02,  1.2238e-02, -2.2793e-02,  1.9460e-02,\n",
      "         1.8286e-02, -2.8420e-02,  1.3575e-02, -8.4124e-03, -3.2032e-02,\n",
      "        -2.4601e-03,  6.6738e-04, -7.1045e-03, -2.8815e-03, -2.5060e-02,\n",
      "         1.6263e-02, -2.3405e-02, -2.2325e-02,  5.9007e-03, -1.1155e-02,\n",
      "         1.8465e-02, -1.4600e-02,  3.0168e-02,  1.6339e-02, -9.9461e-03,\n",
      "        -5.9667e-03, -3.3839e-02,  1.0676e-02, -2.7150e-02,  9.6626e-03,\n",
      "        -1.3330e-02,  1.3861e-03,  6.4766e-03,  8.0386e-03, -3.2744e-02,\n",
      "        -2.4520e-02,  9.1126e-04, -2.2925e-02,  1.4090e-02, -1.5885e-02,\n",
      "        -2.2560e-02,  2.9293e-02,  2.7180e-02, -3.7387e-03, -1.1794e-02,\n",
      "         3.2595e-02, -1.7689e-02,  2.1709e-02,  1.2892e-02, -1.3281e-02,\n",
      "         1.8477e-04, -7.7951e-03,  3.0167e-02, -2.6326e-02,  3.6643e-03,\n",
      "        -7.1754e-03, -2.4977e-02, -3.2391e-02, -3.1017e-02,  1.2399e-02,\n",
      "        -3.0800e-04, -2.4515e-02,  2.7135e-02, -1.5737e-02, -2.1788e-02,\n",
      "        -3.3102e-02, -8.8519e-03, -2.8756e-02,  4.9694e-03, -2.4366e-02,\n",
      "         1.7858e-02,  3.0602e-02, -8.8132e-04, -2.6225e-02,  3.5177e-02,\n",
      "         2.1333e-03,  4.8861e-03,  3.2795e-02, -1.4140e-02,  6.8577e-03,\n",
      "         1.4570e-02,  1.1123e-02,  5.5955e-04, -2.6578e-02,  1.3959e-02,\n",
      "         1.3954e-02,  2.2153e-02, -2.2178e-02,  8.6209e-03,  5.2497e-03,\n",
      "         3.1542e-02, -1.1244e-02,  1.1554e-02, -3.1807e-02,  6.1768e-03,\n",
      "         3.4330e-02,  7.0360e-03, -2.3647e-02,  1.5490e-02, -2.1009e-03,\n",
      "         1.9732e-02,  1.9183e-03, -2.0801e-03,  2.9912e-02, -1.7662e-02,\n",
      "        -2.5476e-02,  1.4565e-02,  3.0454e-02,  2.9532e-02,  3.5119e-02,\n",
      "         3.8108e-03, -2.0267e-02, -1.1227e-02, -1.2693e-02, -1.9172e-02,\n",
      "         6.4757e-03,  3.3216e-03,  1.0987e-02,  2.0274e-02, -4.8788e-03,\n",
      "        -3.5576e-02, -2.2097e-03,  2.6677e-02,  2.3432e-02, -1.2036e-02,\n",
      "         2.3486e-02, -2.6807e-02, -1.0952e-03, -3.4001e-02, -1.5435e-03,\n",
      "        -1.3857e-02, -3.5553e-02, -2.5136e-02, -2.5212e-03,  2.6451e-02,\n",
      "         1.1269e-02,  1.4882e-02,  1.6780e-03, -1.5312e-02, -1.5715e-02,\n",
      "        -1.1392e-02,  8.5402e-04,  1.0628e-02, -2.8361e-02,  3.1183e-02,\n",
      "         1.5713e-02,  2.0101e-02,  6.9032e-03,  2.8479e-02, -3.4046e-02,\n",
      "         2.1905e-02, -6.9618e-03,  1.6297e-02, -3.9532e-03, -2.6899e-02,\n",
      "        -1.3167e-02, -2.7395e-02, -2.5646e-04,  2.2862e-02,  1.2998e-02,\n",
      "        -1.1796e-03,  2.9535e-02, -1.8274e-02, -2.9944e-02,  5.6386e-03,\n",
      "        -6.6675e-03,  2.6724e-02,  1.9435e-02, -8.0494e-03, -7.4612e-03,\n",
      "         3.2540e-02,  9.3702e-04, -2.4315e-02, -2.7189e-02, -3.1781e-02,\n",
      "         2.3118e-02, -2.7252e-02, -3.4104e-02,  5.4405e-03, -4.3397e-04,\n",
      "        -1.6974e-02,  1.2789e-02,  5.1419e-03, -5.4801e-03, -1.3200e-02,\n",
      "         5.6623e-03,  2.9943e-02, -2.8815e-02,  3.5341e-02, -3.1929e-02,\n",
      "        -1.9074e-03, -8.7095e-03, -3.4076e-02, -2.7197e-02,  1.1767e-02,\n",
      "        -2.2709e-02,  1.3051e-02,  3.2551e-02,  2.1110e-02,  3.3972e-02,\n",
      "         1.1281e-02, -1.6900e-02,  2.8717e-02, -9.1437e-03, -3.0080e-02,\n",
      "        -6.4060e-03,  1.1757e-02, -2.8849e-02,  1.0316e-02, -1.7357e-02,\n",
      "         6.7229e-03, -1.3030e-02, -1.6361e-02,  7.5406e-03,  1.0327e-02,\n",
      "        -3.2232e-02, -3.0526e-02,  3.3145e-02, -1.3157e-02,  2.7076e-02,\n",
      "        -9.0361e-03,  2.3354e-02, -2.8844e-02,  2.5870e-02, -2.9023e-03,\n",
      "        -6.4310e-04, -1.0554e-02, -1.7237e-02,  2.9154e-02, -2.7070e-02,\n",
      "         1.2751e-02, -1.1307e-02, -1.6517e-02, -1.7007e-02,  3.4805e-02,\n",
      "         1.4644e-02,  4.5598e-03,  2.5584e-02,  3.0424e-02, -2.9068e-02,\n",
      "        -1.4116e-02, -3.5533e-02,  7.3603e-04, -4.5291e-03,  2.2788e-02,\n",
      "         1.2189e-02, -1.8167e-05,  3.5480e-02,  4.0690e-03, -2.7793e-02,\n",
      "        -4.8066e-03, -3.5686e-02, -2.3942e-02,  3.2808e-02,  2.6745e-02,\n",
      "        -3.2235e-02,  3.4086e-02, -2.0768e-02,  2.5532e-02, -2.3021e-02,\n",
      "        -3.0528e-02, -3.2868e-02,  2.2735e-02, -2.7678e-02, -2.0103e-02,\n",
      "        -2.7104e-02,  1.0852e-02,  2.3153e-02, -9.7866e-03,  1.9042e-02,\n",
      "         2.2816e-03, -8.3679e-03,  3.4146e-02, -1.2602e-02,  9.0086e-03,\n",
      "         3.3193e-02,  1.3979e-02,  2.2515e-02,  1.6279e-02, -1.5544e-02,\n",
      "        -3.4865e-02,  3.2532e-02, -1.6974e-02, -1.0988e-02, -6.6421e-03,\n",
      "         3.0466e-02, -1.2529e-02, -1.4286e-02,  4.0414e-03, -2.8952e-03,\n",
      "        -2.4889e-02,  1.2016e-02, -5.2718e-03,  2.4400e-02, -8.2964e-03,\n",
      "         2.5019e-02,  1.8074e-02, -3.2480e-02,  2.1034e-02, -8.5633e-03,\n",
      "        -1.9230e-02,  1.6645e-02, -1.4237e-02, -1.1175e-02,  1.0931e-02,\n",
      "        -1.4503e-02, -1.1741e-04,  3.1618e-03,  3.0925e-03,  5.0762e-03,\n",
      "        -2.2360e-02, -9.1048e-03,  1.5590e-02,  1.7871e-02, -3.5056e-02,\n",
      "         3.0647e-03,  8.4602e-03,  2.8445e-02,  1.0793e-02,  1.6793e-02,\n",
      "        -1.3315e-03, -6.5605e-03, -3.3756e-02, -1.9669e-02,  3.4889e-02,\n",
      "        -1.1445e-02, -3.0650e-02,  2.4518e-02, -2.8994e-02, -1.5405e-02,\n",
      "         1.4209e-04,  2.4241e-02, -3.4644e-02,  3.9981e-03,  2.6833e-02,\n",
      "        -1.3143e-02, -2.3257e-02, -2.8006e-02,  2.1164e-02,  1.6946e-03,\n",
      "        -2.1747e-02,  1.2792e-02,  1.6767e-02, -2.2215e-02, -1.3067e-03,\n",
      "        -2.7873e-02, -3.3995e-02, -5.6756e-03, -1.8196e-02, -2.7520e-02,\n",
      "        -2.8974e-02, -2.7752e-02,  1.5596e-02, -3.0185e-02, -1.1298e-02,\n",
      "         1.2833e-02, -3.0761e-02, -2.0516e-02, -2.7694e-02,  1.2788e-02,\n",
      "        -3.1617e-02, -1.7691e-02,  1.8872e-02, -3.0539e-02, -3.2914e-02,\n",
      "        -1.1378e-02, -1.7471e-02, -2.0433e-02,  1.3007e-03, -2.5061e-02,\n",
      "         6.4085e-03,  7.8337e-03,  5.3064e-03, -3.5654e-02, -2.1901e-02,\n",
      "         7.0449e-03, -1.7622e-02, -6.9286e-03,  3.3345e-03,  1.2483e-02,\n",
      "        -3.3158e-02,  2.2240e-02, -1.9430e-02, -3.0069e-02, -1.7186e-03,\n",
      "        -3.1875e-02,  2.3061e-02,  9.1278e-03, -2.9559e-02,  2.5441e-02,\n",
      "        -3.5144e-02, -2.1630e-02, -1.3540e-02,  1.7778e-03,  3.4488e-02,\n",
      "        -1.6632e-02,  1.4086e-02, -1.8832e-02,  2.5658e-02, -1.6533e-02,\n",
      "        -8.0396e-03, -1.6982e-02, -1.2839e-02, -2.5365e-02, -1.4100e-02,\n",
      "         2.3816e-02,  2.5141e-02,  1.3540e-02,  3.0928e-02, -2.5820e-02,\n",
      "        -1.1933e-02, -2.1791e-02,  1.7509e-02, -3.5533e-02,  1.9477e-02,\n",
      "        -1.1821e-02,  2.2403e-02, -1.8351e-02,  3.4062e-02, -3.4948e-02,\n",
      "        -1.7924e-02,  1.7842e-02,  1.7393e-02,  3.2544e-02,  2.7783e-02,\n",
      "        -1.6863e-02, -4.6516e-04,  1.6262e-02,  3.9442e-03, -2.5233e-02,\n",
      "         1.1013e-02, -2.9525e-02,  2.7883e-02,  2.4238e-02, -2.4449e-02,\n",
      "        -1.4300e-02, -2.1035e-02,  1.3682e-02,  1.4096e-02,  1.9820e-02,\n",
      "         2.2801e-02,  1.8911e-02, -2.4582e-02, -3.3125e-02, -2.5487e-02,\n",
      "        -1.7877e-02, -1.9277e-02, -1.4121e-02, -1.4331e-02,  2.6590e-02,\n",
      "         1.2374e-02, -2.1088e-02, -2.7136e-02,  1.2780e-02,  3.3979e-02,\n",
      "         2.6338e-02, -2.7659e-02,  2.0710e-02, -3.3658e-03,  1.1204e-03,\n",
      "         2.0478e-02,  1.7657e-02,  2.4454e-02,  1.5072e-02,  7.4155e-03,\n",
      "         3.5383e-02,  1.9162e-03,  2.5362e-02, -2.8310e-02, -1.2082e-02,\n",
      "        -3.3108e-02,  2.8556e-02, -3.0754e-03,  2.1726e-02, -1.1184e-02,\n",
      "         2.7755e-02,  2.3863e-02, -1.1722e-02,  7.9174e-03,  3.4116e-02,\n",
      "        -2.7327e-02, -1.2870e-02,  1.7989e-02,  2.8138e-02,  1.5345e-02,\n",
      "         3.3294e-02, -1.6264e-02, -2.0940e-02, -3.2940e-02, -3.9056e-04,\n",
      "        -1.0277e-02, -2.8673e-02,  3.3093e-02, -8.9260e-03, -2.4681e-02,\n",
      "        -1.5137e-02, -2.9373e-03, -2.0726e-02, -2.0957e-02,  6.7996e-03,\n",
      "        -1.8412e-02,  1.1609e-02,  2.7381e-02,  3.3131e-02,  1.6474e-03,\n",
      "         6.5324e-03, -3.2564e-02, -9.3192e-03,  2.4267e-02,  2.9920e-02,\n",
      "        -1.6026e-02,  2.4425e-02,  2.8679e-02,  2.2189e-02, -1.9982e-02,\n",
      "        -2.3734e-02, -1.2934e-03, -2.3888e-02,  1.9538e-02, -2.8125e-02,\n",
      "        -8.1103e-03, -2.6197e-02,  8.8082e-03,  2.2464e-02, -3.4208e-02,\n",
      "        -5.6052e-03,  2.6455e-02,  3.4748e-02,  2.4286e-03, -2.9040e-02,\n",
      "         9.2863e-03,  1.5582e-02, -1.6018e-02, -1.3817e-02, -2.8843e-02,\n",
      "        -3.1334e-02,  6.3971e-03, -3.2271e-02, -9.8980e-03,  1.0670e-02,\n",
      "        -9.4995e-03, -1.8813e-02,  1.9973e-02,  9.9968e-03, -1.9276e-02,\n",
      "        -2.7489e-02, -3.3115e-02, -1.4632e-02, -1.8371e-02, -2.4072e-02,\n",
      "        -7.4121e-03, -3.5370e-02,  1.9304e-03, -2.1126e-02, -2.2706e-02,\n",
      "        -2.3399e-02, -9.6868e-03,  2.2201e-02, -1.6571e-02, -1.6466e-02,\n",
      "         8.2080e-03,  2.2268e-02,  2.9390e-02,  2.4640e-02,  1.4510e-02,\n",
      "        -2.5323e-02,  1.5430e-02,  1.0382e-02, -1.5511e-02,  2.4957e-02,\n",
      "         2.3400e-02, -4.9140e-03,  2.9500e-02,  1.3330e-02, -1.6897e-03,\n",
      "        -1.3510e-03,  2.7423e-02,  1.6505e-02,  4.6938e-03, -2.8148e-02,\n",
      "        -1.3623e-02,  5.0276e-03,  1.0808e-02, -3.0929e-02,  3.4057e-02,\n",
      "        -1.7363e-02, -9.7705e-03,  2.8458e-02, -2.5087e-02,  9.1110e-03,\n",
      "         2.6061e-02, -1.9553e-02,  1.3026e-02,  3.1218e-02, -2.4462e-02,\n",
      "        -1.0140e-02, -2.6453e-02,  1.1161e-02, -1.4521e-02, -1.6472e-02,\n",
      "         2.2667e-02,  3.5450e-02, -2.5136e-02, -1.7924e-02,  3.5006e-02,\n",
      "        -1.3185e-02,  1.5539e-02,  2.8377e-02,  3.6066e-03, -3.4848e-02,\n",
      "         3.4143e-02, -4.6018e-03,  4.8971e-03,  3.0396e-02, -2.3703e-02,\n",
      "         2.1616e-02, -2.8829e-02, -2.3188e-02, -1.3885e-02,  1.7497e-03,\n",
      "        -1.2359e-02, -6.4737e-03, -2.4639e-02,  4.8750e-03,  2.4750e-02,\n",
      "        -1.5228e-02, -1.2063e-02, -2.5004e-02, -5.1826e-03,  6.7497e-03,\n",
      "        -1.7103e-02, -5.3040e-03, -3.0127e-02,  2.2475e-02,  4.0828e-03,\n",
      "        -2.7085e-02, -9.3536e-03, -1.7961e-03, -1.5137e-02, -1.9158e-03,\n",
      "        -2.8533e-02,  2.3689e-02, -3.3104e-02, -7.2918e-03], device='cuda:0')\n",
      "tensor([[-0.0310, -0.0085, -0.0261,  ...,  0.0071, -0.0014, -0.0321],\n",
      "        [ 0.0194, -0.0076,  0.0281,  ...,  0.0284,  0.0124, -0.0122],\n",
      "        [ 0.0306,  0.0328,  0.0250,  ...,  0.0035, -0.0114, -0.0009],\n",
      "        ...,\n",
      "        [-0.0352, -0.0210, -0.0300,  ..., -0.0203, -0.0317, -0.0321],\n",
      "        [-0.0085, -0.0095,  0.0144,  ...,  0.0316,  0.0223, -0.0148],\n",
      "        [-0.0077,  0.0250, -0.0232,  ..., -0.0086, -0.0148, -0.0265]],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.0220, -0.0250,  0.0102,  0.0242,  0.0348, -0.0246, -0.0355, -0.0348,\n",
      "         0.0181, -0.0336], device='cuda:0')\n",
      "tensor([[-0.0061,  0.2928,  0.0771,  ...,  0.2904, -0.0878, -0.0797],\n",
      "        [-0.0432, -0.3022,  0.1353,  ..., -0.3057,  0.0174, -0.0680],\n",
      "        [-0.2283, -0.0359,  0.0330,  ...,  0.0546, -0.1164, -0.0074],\n",
      "        ...,\n",
      "        [-0.1176, -0.0880,  0.1608,  ..., -0.1829,  0.0400, -0.1710],\n",
      "        [ 0.1651, -0.2783, -0.2045,  ..., -0.1539, -0.1663,  0.2377],\n",
      "        [ 0.1978, -0.3132, -0.0711,  ..., -0.0743,  0.1707, -0.0012]],\n",
      "       device='cuda:0')\n",
      "tensor([ 0.0354, -0.1210,  0.0802, -0.1757, -0.0532, -0.1179,  0.2092,  0.0050,\n",
      "        -0.2210,  0.1305,  0.1073, -0.3074,  0.1893, -0.1078,  0.0288,  0.0734,\n",
      "        -0.1428,  0.3046,  0.2203, -0.0568,  0.0670, -0.2524, -0.0416, -0.0396,\n",
      "        -0.0408, -0.0544, -0.2425,  0.1826, -0.0458, -0.3019, -0.3094, -0.2212,\n",
      "         0.2456, -0.2458, -0.2029,  0.1895,  0.2841,  0.2091,  0.0199, -0.1940,\n",
      "         0.1905, -0.0926, -0.0270, -0.1466, -0.3117, -0.1232, -0.2277,  0.0387,\n",
      "         0.1223,  0.2493,  0.1291, -0.0124, -0.0651,  0.0663, -0.0201, -0.1069,\n",
      "         0.2173,  0.0602, -0.2655, -0.2090, -0.1737, -0.0191,  0.2815, -0.1253,\n",
      "         0.3059, -0.1899, -0.0909,  0.0340,  0.2374,  0.1306,  0.2571, -0.0117,\n",
      "        -0.2666, -0.1253,  0.1534, -0.2688,  0.0365, -0.1544, -0.1187,  0.1447,\n",
      "        -0.1586,  0.1287, -0.3144, -0.0424, -0.2152,  0.2341, -0.2927,  0.2778,\n",
      "         0.3137,  0.2365,  0.2891,  0.2244,  0.1818,  0.0450, -0.2676, -0.1279,\n",
      "         0.1310, -0.2483,  0.2760,  0.0575, -0.1515, -0.2616,  0.1064,  0.0647,\n",
      "        -0.2636,  0.0683,  0.0490,  0.1861,  0.2119,  0.1707,  0.0579,  0.1804,\n",
      "        -0.2289, -0.2695, -0.1852, -0.2936, -0.1455, -0.0888, -0.0704, -0.0068,\n",
      "         0.0680, -0.1005, -0.1638, -0.0832, -0.0251,  0.3004, -0.0183, -0.1747,\n",
      "         0.0889, -0.1456,  0.2183, -0.0688, -0.0223,  0.0261,  0.2130,  0.2587,\n",
      "        -0.0105, -0.0677,  0.2027,  0.2018, -0.1958,  0.0960,  0.1502, -0.2165,\n",
      "         0.2820, -0.0309,  0.1024,  0.0118,  0.0077,  0.0086, -0.2308,  0.2304,\n",
      "         0.1383, -0.1138, -0.0562,  0.1552,  0.0372, -0.0004,  0.1429, -0.2811,\n",
      "        -0.0142, -0.1736, -0.1763, -0.2945,  0.0776,  0.0123,  0.2308, -0.1389,\n",
      "         0.2594,  0.0228, -0.0229, -0.3104, -0.2672,  0.0588,  0.1547, -0.2814,\n",
      "         0.1203, -0.2619, -0.2306, -0.2584,  0.0849,  0.1032,  0.2091, -0.0428,\n",
      "         0.2996, -0.1744, -0.0895,  0.2036, -0.0470,  0.1586,  0.1518,  0.0415,\n",
      "         0.2360, -0.1356, -0.2303,  0.1742, -0.2985, -0.2937,  0.2637, -0.2801,\n",
      "         0.1359,  0.0306,  0.0826, -0.2011, -0.1173,  0.0664,  0.2378,  0.2513,\n",
      "         0.0992,  0.1091, -0.3117,  0.1457,  0.2435, -0.2184,  0.1142,  0.3087,\n",
      "         0.0019, -0.2893,  0.2155,  0.1594,  0.1453,  0.0835,  0.2702,  0.1196,\n",
      "        -0.3081,  0.2467, -0.1823, -0.1262,  0.1320,  0.0871,  0.0564,  0.3109,\n",
      "         0.2895,  0.2493,  0.1014,  0.2200, -0.0049,  0.1843, -0.0129,  0.0902,\n",
      "        -0.1930,  0.1005,  0.1143,  0.0522, -0.0407,  0.3134, -0.0092,  0.1961,\n",
      "         0.2481, -0.2404,  0.2476,  0.2636,  0.1017,  0.3016,  0.0448,  0.1052,\n",
      "         0.2514,  0.2388, -0.2532, -0.1717,  0.2335,  0.2406,  0.0372,  0.1688,\n",
      "        -0.0011, -0.2242, -0.2993, -0.2079,  0.2587,  0.2580, -0.0889, -0.0383,\n",
      "        -0.0062,  0.2955,  0.0576, -0.1232, -0.2713, -0.2862, -0.1050,  0.1077,\n",
      "         0.1187, -0.0787, -0.1210,  0.1758, -0.1941, -0.1931, -0.1551, -0.1603,\n",
      "        -0.2607, -0.2313,  0.0747, -0.2031,  0.0476, -0.1621,  0.2695, -0.2960,\n",
      "        -0.1671,  0.1981,  0.2933,  0.0871, -0.1023, -0.2440, -0.1779, -0.2226,\n",
      "        -0.0665,  0.2183, -0.2150, -0.2167,  0.0451,  0.1049,  0.2148, -0.1326,\n",
      "        -0.2073, -0.0576,  0.2361, -0.0296,  0.1810,  0.0908, -0.1935,  0.0489,\n",
      "         0.2928,  0.2764, -0.1987,  0.2864, -0.0906, -0.0804,  0.0641,  0.2332,\n",
      "        -0.0087, -0.1782, -0.1937, -0.1187,  0.1531, -0.2660, -0.2589,  0.2847,\n",
      "        -0.0033, -0.1334,  0.2552, -0.1078,  0.2757, -0.1659,  0.2691,  0.2223,\n",
      "        -0.2176, -0.0390, -0.0995, -0.2307, -0.0932,  0.1774,  0.2865, -0.1260,\n",
      "         0.0220, -0.2553, -0.1461,  0.0524, -0.0557, -0.2380, -0.2976,  0.2497,\n",
      "        -0.1404, -0.1588,  0.0168, -0.0549, -0.0772,  0.1479,  0.2421, -0.1351,\n",
      "        -0.2296, -0.1110,  0.2918, -0.1969, -0.1428, -0.0422,  0.2298, -0.2573,\n",
      "        -0.1619, -0.1065, -0.0172, -0.1961,  0.1269,  0.1085,  0.2883, -0.2179,\n",
      "        -0.2458,  0.1868, -0.1414, -0.2417,  0.1009, -0.0583, -0.1959, -0.0583,\n",
      "         0.0466, -0.2321,  0.0719,  0.2878, -0.2397,  0.1910,  0.1403,  0.2818,\n",
      "        -0.1003, -0.1672, -0.2838,  0.1365,  0.2963,  0.2777,  0.2716,  0.2138,\n",
      "        -0.1523, -0.0816, -0.0271, -0.0018, -0.2707, -0.3048,  0.0883,  0.1691,\n",
      "        -0.0175,  0.0883, -0.2440, -0.1632, -0.1782, -0.2812,  0.1410, -0.1571,\n",
      "        -0.2475,  0.1288, -0.0194, -0.0734,  0.2336, -0.0554, -0.2642, -0.0972,\n",
      "         0.0316, -0.0116, -0.0635,  0.1407, -0.0786,  0.1779,  0.0837, -0.2784,\n",
      "         0.0346,  0.0380,  0.1927, -0.0605, -0.1805,  0.0097, -0.1375, -0.2192,\n",
      "         0.1333,  0.0406,  0.1347, -0.2855,  0.2135,  0.0955,  0.2396, -0.0146,\n",
      "         0.2651, -0.1800,  0.1304, -0.0840, -0.0848, -0.0387, -0.1774,  0.1021,\n",
      "         0.1042, -0.2917, -0.3139, -0.2671, -0.2997,  0.0182,  0.2480,  0.0175,\n",
      "         0.2423,  0.3006, -0.1659,  0.2437, -0.1458,  0.0466, -0.2291,  0.1904,\n",
      "         0.2947,  0.0009,  0.1399,  0.2250,  0.1210,  0.0503,  0.1853, -0.2146,\n",
      "        -0.1116, -0.1400, -0.0959, -0.1946, -0.1789, -0.3050, -0.0894,  0.0850,\n",
      "         0.2060, -0.2763, -0.2721,  0.1815,  0.0155, -0.1182,  0.2011,  0.1822,\n",
      "        -0.0945, -0.3047, -0.2444, -0.1482,  0.1213,  0.3051,  0.2526,  0.0071,\n",
      "        -0.1496,  0.0914, -0.2124, -0.2178, -0.2762,  0.2926, -0.0713,  0.0920,\n",
      "         0.1856, -0.2915,  0.0564,  0.2028, -0.2291,  0.1811,  0.0517,  0.2142,\n",
      "        -0.2066, -0.1554,  0.2550, -0.2789, -0.2462,  0.1249, -0.3097,  0.1856,\n",
      "        -0.2063,  0.2293,  0.2265,  0.2549,  0.1550,  0.0435,  0.0110, -0.1677,\n",
      "        -0.0268, -0.2274, -0.0823, -0.0142, -0.0240,  0.1344,  0.2985, -0.0520,\n",
      "        -0.1288, -0.0133,  0.1527, -0.0307, -0.2457,  0.2312, -0.2131,  0.1031,\n",
      "        -0.1956,  0.1865, -0.1076,  0.1387, -0.2594, -0.0265,  0.0733,  0.2360,\n",
      "        -0.2513,  0.1779,  0.0735,  0.0676,  0.0361,  0.1586,  0.1826,  0.0580,\n",
      "        -0.2023, -0.0203, -0.1407,  0.0410, -0.2002,  0.0559,  0.0577,  0.1397,\n",
      "        -0.2409,  0.0298, -0.2694, -0.2083, -0.2231, -0.2909,  0.2357,  0.2615,\n",
      "        -0.0880, -0.0685, -0.0415,  0.0822, -0.1228, -0.2566, -0.1104,  0.1450,\n",
      "        -0.0079,  0.0829,  0.2224,  0.0161,  0.0761, -0.2952, -0.2085, -0.2428,\n",
      "        -0.0919, -0.1442,  0.0572, -0.2228,  0.0127, -0.0453,  0.0925,  0.1339,\n",
      "        -0.2084,  0.2124,  0.0143,  0.0353, -0.2590, -0.0899,  0.0860, -0.1185,\n",
      "         0.2710, -0.2964,  0.2715,  0.0843,  0.1547,  0.0829, -0.1620, -0.1920,\n",
      "        -0.3042, -0.3120,  0.0389,  0.2733, -0.2914,  0.0112, -0.2509, -0.1148,\n",
      "        -0.2313,  0.1063, -0.2300, -0.2352,  0.2595, -0.1162, -0.0926,  0.3000,\n",
      "        -0.0796, -0.2329,  0.1830,  0.0136, -0.1083, -0.1262,  0.1197, -0.3097,\n",
      "        -0.1989, -0.1960,  0.3036,  0.3064,  0.0418,  0.2309, -0.2452,  0.0506,\n",
      "        -0.1238, -0.0835, -0.1729, -0.0040,  0.1810,  0.0738,  0.2831, -0.0941,\n",
      "        -0.3129,  0.2325, -0.1234, -0.1093, -0.3062,  0.2392,  0.0110,  0.1117,\n",
      "         0.1079, -0.2460, -0.2791, -0.1272, -0.2808, -0.0700,  0.1446, -0.2959,\n",
      "        -0.3020,  0.0359,  0.2978,  0.0248, -0.2208,  0.2128,  0.0812, -0.0265,\n",
      "         0.0281,  0.1838, -0.2379,  0.2590,  0.1642,  0.2738, -0.2918,  0.0326,\n",
      "         0.1963,  0.2724,  0.2938,  0.1781, -0.0527,  0.0276,  0.1410, -0.0855,\n",
      "         0.2029, -0.0304,  0.0968,  0.1433, -0.0007,  0.2570, -0.0119, -0.1503,\n",
      "        -0.2110, -0.2560,  0.2641, -0.1795, -0.0645,  0.0950,  0.0523, -0.2175,\n",
      "        -0.2594,  0.2505,  0.1144, -0.1329, -0.2819,  0.1174, -0.1815,  0.1855,\n",
      "        -0.2028, -0.0647, -0.0125, -0.0576, -0.2104,  0.2651,  0.1460,  0.1395,\n",
      "        -0.1622,  0.1506, -0.0440, -0.0990, -0.0292, -0.2519,  0.1290, -0.0536,\n",
      "        -0.2227, -0.2962,  0.1195, -0.3136,  0.2372,  0.1762,  0.0107, -0.1930,\n",
      "        -0.2478,  0.2441, -0.0726,  0.2950,  0.2073,  0.2597,  0.1005,  0.0398,\n",
      "         0.2919,  0.0184, -0.0540, -0.2972, -0.0180, -0.1946,  0.0825, -0.1018,\n",
      "        -0.1564,  0.2730, -0.2867,  0.1154, -0.0003, -0.0083, -0.1688,  0.2403],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.0133, -0.0329,  0.0047,  ...,  0.0113, -0.0004,  0.0123],\n",
      "        [-0.0216, -0.0202,  0.0340,  ...,  0.0074, -0.0323,  0.0278],\n",
      "        [-0.0053,  0.0182, -0.0002,  ...,  0.0114, -0.0179, -0.0230],\n",
      "        ...,\n",
      "        [ 0.0248,  0.0117, -0.0102,  ..., -0.0236, -0.0137, -0.0352],\n",
      "        [ 0.0256,  0.0012,  0.0040,  ..., -0.0011, -0.0314,  0.0333],\n",
      "        [-0.0033,  0.0289,  0.0272,  ...,  0.0088, -0.0276, -0.0115]],\n",
      "       device='cuda:0')\n",
      "tensor([-5.2906e-05, -3.1558e-02,  2.6030e-02,  6.4112e-04,  2.2037e-02,\n",
      "         2.7146e-02, -1.7181e-02,  2.7826e-02,  1.5145e-02,  1.0347e-02,\n",
      "         1.1224e-02,  2.1040e-03, -1.5198e-02,  2.6361e-02, -3.0060e-02,\n",
      "        -2.5925e-02, -7.5951e-03, -3.7394e-03, -1.6458e-03, -1.3192e-02,\n",
      "        -9.5819e-03, -1.3266e-02, -2.3299e-02,  3.4049e-02, -1.1503e-02,\n",
      "        -1.6944e-02,  2.5872e-02, -3.0304e-02, -1.1719e-02,  3.4514e-02,\n",
      "        -3.3230e-02, -2.6487e-02,  1.1325e-02, -2.1455e-02, -3.0151e-02,\n",
      "        -2.1120e-02, -1.4371e-02,  3.5148e-02, -1.4720e-02, -1.8137e-02,\n",
      "        -2.5885e-02,  2.4267e-02, -9.2249e-03, -2.9195e-02, -2.5689e-02,\n",
      "        -1.8183e-02, -1.8624e-02,  2.8843e-03,  2.7664e-02,  6.8011e-03,\n",
      "        -1.0937e-02, -2.5383e-02, -7.4704e-03, -3.5681e-02,  2.6731e-02,\n",
      "        -2.9921e-02, -2.0183e-02,  3.5517e-03, -7.4947e-03, -1.4674e-02,\n",
      "        -2.8051e-02,  2.7985e-02,  1.8934e-02, -2.2540e-02, -2.8414e-02,\n",
      "         1.2363e-02,  1.9731e-03,  2.3739e-02,  2.5643e-02,  1.9364e-02,\n",
      "        -2.6144e-02, -4.5176e-03, -1.5373e-02, -2.1125e-02,  1.1806e-02,\n",
      "        -1.2670e-02, -1.6038e-02,  3.1932e-02, -1.0050e-02,  3.3841e-02,\n",
      "        -1.0060e-02,  6.7564e-03,  1.1012e-02,  1.0507e-02, -1.4564e-03,\n",
      "        -2.6597e-02, -1.1455e-02, -2.8682e-02, -1.5000e-02, -1.1844e-02,\n",
      "         5.2379e-03, -3.4409e-02, -2.1470e-02, -3.3129e-02, -3.2571e-02,\n",
      "         1.8541e-02, -1.4523e-02, -2.1809e-02,  2.9531e-02,  2.3182e-02,\n",
      "        -2.3402e-02,  1.5020e-02,  1.0927e-02, -1.6134e-02, -4.1048e-03,\n",
      "        -6.3940e-03, -2.4202e-02, -2.2680e-02,  1.1898e-02,  1.5145e-02,\n",
      "        -1.7965e-02,  1.0485e-02, -1.2841e-02,  4.1633e-03,  1.5354e-02,\n",
      "         2.0209e-02,  3.3330e-02, -1.4584e-02,  4.7359e-03,  1.9711e-02,\n",
      "         1.3076e-02, -8.1975e-03, -8.8761e-04, -3.3539e-02,  5.5415e-03,\n",
      "        -2.3119e-02,  2.1103e-02,  2.8623e-02,  2.3159e-02, -3.0081e-02,\n",
      "        -2.7108e-02, -1.2259e-02, -3.5546e-03,  1.6188e-02, -1.9912e-02,\n",
      "         1.7291e-02,  1.1459e-02,  5.6008e-03,  3.0851e-02, -1.4879e-02,\n",
      "        -2.0630e-02,  1.1105e-02, -2.3518e-02,  2.4087e-02,  1.3346e-02,\n",
      "         3.0882e-02, -1.3953e-02, -5.7324e-03, -2.6676e-02, -3.3660e-02,\n",
      "        -6.2525e-03, -1.0414e-02, -3.4113e-02,  2.3680e-02, -1.6551e-02,\n",
      "        -1.8085e-02,  1.0958e-02, -2.7803e-02, -2.5336e-02,  3.3837e-02,\n",
      "        -5.1950e-03,  7.1583e-03,  3.1489e-02,  2.7585e-02, -1.3942e-02,\n",
      "         1.2779e-02, -5.3959e-03, -6.3782e-03,  8.5257e-03, -3.0727e-02,\n",
      "         1.9328e-02,  2.4305e-02, -3.0136e-04,  1.0343e-02,  7.5289e-04,\n",
      "        -2.2330e-02, -2.4341e-02, -2.7739e-02,  2.5211e-02,  2.0519e-02,\n",
      "         2.8171e-02,  4.5851e-03,  3.2285e-02,  1.4455e-03, -2.6090e-02,\n",
      "         3.2936e-02,  2.0871e-02,  4.5548e-03,  1.9426e-02, -4.3207e-03,\n",
      "         7.5315e-03, -3.2251e-02, -3.5448e-02,  3.2251e-02, -1.0165e-02,\n",
      "         3.0499e-02,  2.7522e-02, -1.0103e-02, -1.8056e-03,  7.8607e-03,\n",
      "        -3.1834e-02,  1.0854e-02,  4.0965e-03, -2.9183e-02,  2.3074e-02,\n",
      "         3.3773e-02, -2.0155e-02, -1.4322e-02, -1.6690e-03,  1.7760e-03,\n",
      "        -3.5574e-02, -1.3297e-02, -2.1337e-02, -2.1487e-02,  2.8388e-02,\n",
      "        -3.1352e-02,  1.2735e-02, -6.0587e-06,  3.2203e-02, -1.4948e-02,\n",
      "         3.1733e-02,  5.1630e-03, -6.7417e-03, -2.0576e-03,  2.2411e-02,\n",
      "        -1.0611e-02, -1.6565e-02,  1.7761e-02, -3.0678e-02, -3.0052e-02,\n",
      "        -2.3886e-02,  7.2170e-03, -2.9197e-02, -1.2305e-04,  6.9533e-04,\n",
      "         3.3356e-02,  2.0914e-02, -2.6914e-02, -5.5531e-03,  2.1902e-02,\n",
      "        -3.5462e-02, -3.5703e-02, -3.2681e-02, -3.5237e-02,  1.8559e-02,\n",
      "        -3.1314e-02, -3.0534e-02, -2.9541e-02, -1.9351e-02,  2.6716e-02,\n",
      "         9.5747e-03, -6.7083e-04, -2.3354e-02,  3.3911e-02, -2.4039e-02,\n",
      "        -1.0080e-02, -3.4004e-02, -3.0071e-02,  2.0435e-02,  3.4421e-02,\n",
      "         2.2524e-02, -2.1675e-02, -1.5613e-02,  3.1392e-02, -2.5448e-03,\n",
      "        -2.2099e-02, -3.3913e-03,  5.0250e-03, -1.7538e-02,  9.3751e-03,\n",
      "         1.2836e-02, -1.8323e-02,  2.4503e-02, -1.7228e-02,  1.0975e-03,\n",
      "         2.9668e-02,  2.8427e-02, -2.7072e-02, -2.1216e-02, -1.2365e-03,\n",
      "         3.5203e-02,  3.2973e-02, -3.4980e-02, -3.7158e-03, -3.2362e-02,\n",
      "        -3.3633e-02, -4.2072e-03,  3.3431e-02,  1.0249e-02, -7.4205e-03,\n",
      "        -2.1738e-02, -2.8375e-02, -2.4812e-02, -1.9215e-02, -7.3844e-03,\n",
      "        -2.2516e-02, -2.7506e-02,  1.0101e-02, -2.4307e-02,  3.2859e-02,\n",
      "        -1.1540e-02,  8.1691e-03, -8.7155e-03,  1.1153e-02, -4.7077e-03,\n",
      "        -3.4879e-02,  1.3285e-02,  7.4725e-03,  2.1040e-02, -3.1858e-02,\n",
      "         1.2695e-02, -2.7902e-04, -1.9974e-02, -1.9472e-02, -1.3916e-03,\n",
      "         6.4892e-03, -1.3786e-03, -2.1699e-02, -1.6377e-02,  1.5692e-02,\n",
      "         2.8739e-02, -2.1732e-02, -1.2465e-02, -3.1326e-03, -3.3472e-02,\n",
      "         2.9789e-02,  3.0863e-02,  5.6955e-03, -7.7283e-03, -3.1070e-02,\n",
      "        -1.2202e-02,  2.2412e-02, -3.3837e-02,  1.9856e-02, -2.5708e-02,\n",
      "        -1.1114e-02,  3.3792e-03,  6.6854e-03, -6.5668e-03, -8.7939e-03,\n",
      "        -3.4659e-02,  1.9510e-02,  3.7110e-03,  2.7288e-02, -8.8213e-03,\n",
      "         3.0914e-02,  2.6555e-02,  1.9695e-02,  3.1000e-02,  4.5888e-03,\n",
      "        -1.4043e-02, -2.6977e-02, -7.0709e-03,  7.6339e-03, -3.5503e-02,\n",
      "        -1.5498e-02, -7.2378e-03,  3.6380e-04,  3.0133e-02,  2.7881e-02,\n",
      "         1.5632e-02,  4.7263e-03,  2.4036e-02, -1.8165e-02, -2.7148e-02,\n",
      "        -1.4192e-02,  2.7938e-02, -1.4023e-02,  5.8618e-03, -2.1129e-02,\n",
      "         1.1084e-02, -1.1894e-02,  2.8811e-02,  4.3526e-03,  2.9186e-02,\n",
      "        -3.0891e-03,  3.4275e-03,  2.2768e-02,  2.9770e-02, -1.4947e-02,\n",
      "        -1.3837e-02,  2.4908e-03, -4.6659e-03, -1.9391e-02, -3.3903e-02,\n",
      "        -2.7554e-03,  2.3317e-02, -4.3052e-03, -3.1731e-02,  6.4587e-04,\n",
      "         2.5103e-02,  9.1154e-03,  1.8109e-02, -5.3109e-03, -3.2800e-02,\n",
      "         6.2949e-03, -6.0801e-03, -1.9289e-02,  6.1133e-03, -2.0985e-02,\n",
      "         8.7761e-03,  1.9699e-02, -2.2109e-02, -3.0341e-03, -3.2478e-02,\n",
      "         8.2777e-03, -2.3551e-02,  1.7636e-02,  4.2250e-03,  2.7898e-02,\n",
      "         3.2308e-02,  1.3574e-02, -7.1844e-03,  1.8277e-02, -2.1587e-02,\n",
      "        -2.3298e-02, -2.6062e-02,  2.0410e-02, -2.5856e-02,  3.3668e-02,\n",
      "        -3.1851e-02, -1.4996e-02,  1.6558e-02,  3.2793e-02,  3.0937e-02,\n",
      "        -9.6016e-03, -2.4475e-02,  1.3044e-02,  9.0446e-03,  3.0065e-03,\n",
      "        -9.4760e-04, -2.3686e-02,  3.6541e-04,  1.4605e-02, -1.2750e-02,\n",
      "         1.4512e-02,  2.2617e-02, -2.3649e-02, -3.6779e-03, -1.1088e-02,\n",
      "        -2.1393e-02, -3.2424e-02, -1.8936e-02, -2.6656e-02,  2.7447e-02,\n",
      "         7.1139e-03, -2.8109e-02, -2.4968e-03,  1.2823e-02, -2.4393e-02,\n",
      "        -3.3344e-02, -2.3023e-02, -7.2032e-03,  4.4411e-03, -2.4095e-02,\n",
      "        -4.1537e-03,  8.1273e-03, -1.4279e-02, -3.4505e-02,  2.6945e-02,\n",
      "         1.7792e-05,  1.2569e-02, -1.3327e-02,  1.3386e-02, -8.7862e-03,\n",
      "         1.5349e-02, -2.5430e-02,  3.1846e-02, -9.8059e-03,  2.6505e-02,\n",
      "        -1.1552e-02,  2.7944e-02,  2.2241e-02, -5.5977e-04, -2.9597e-02,\n",
      "        -8.9244e-03, -2.1910e-02,  1.7634e-02,  1.4637e-02,  1.5784e-02,\n",
      "         2.9166e-02,  1.6498e-02,  1.9564e-02, -6.3512e-03, -3.0118e-02,\n",
      "        -1.5403e-03, -2.1495e-02, -3.5491e-02, -3.5426e-02,  8.6450e-03,\n",
      "         5.1163e-03,  1.4673e-02,  1.4080e-02,  1.6570e-02,  6.4524e-03,\n",
      "         2.1655e-02, -6.8991e-03,  1.3010e-02, -2.6609e-02,  1.7895e-02,\n",
      "         2.4483e-02,  1.5470e-02,  2.1937e-02, -3.1584e-02, -1.2597e-02,\n",
      "        -1.4820e-02, -6.1713e-04,  1.7187e-02,  1.8091e-02,  8.2470e-03,\n",
      "        -2.4151e-02, -2.9179e-02,  6.6149e-03,  2.9349e-02, -1.6519e-02,\n",
      "         1.7351e-02, -2.8776e-02, -1.8903e-02, -2.8551e-02,  2.5497e-02,\n",
      "         7.9683e-03,  3.5162e-02,  2.0888e-02, -3.0237e-02,  3.3501e-02,\n",
      "        -3.3647e-02,  2.9630e-02,  1.1394e-02,  2.6237e-02, -7.7651e-03,\n",
      "        -5.4256e-04, -3.1249e-03,  1.6213e-02, -2.6477e-02,  3.2714e-02,\n",
      "         3.0541e-02, -1.9772e-02,  7.5687e-03,  3.3894e-02,  1.7835e-02,\n",
      "         9.5956e-03,  3.3105e-02,  2.4803e-02,  3.4272e-02,  1.1735e-02,\n",
      "        -1.6869e-02, -2.9481e-02,  3.0225e-02,  2.2988e-02,  6.1889e-03,\n",
      "         9.0579e-03,  1.6838e-02, -1.1454e-02, -2.8249e-03,  2.0681e-02,\n",
      "         1.4736e-02, -3.0097e-02, -1.4886e-02, -2.8995e-02,  2.0393e-02,\n",
      "        -2.6516e-02, -3.2103e-02, -2.1576e-02, -2.0293e-02,  3.2334e-02,\n",
      "         3.4472e-02,  1.7371e-02,  3.3314e-02, -6.9681e-03,  1.7242e-02,\n",
      "        -1.0565e-02,  2.6247e-02,  5.9985e-03,  3.0468e-02,  4.0823e-03,\n",
      "         2.9620e-03, -2.5229e-02, -2.5049e-02,  8.6701e-03, -8.1583e-03,\n",
      "        -4.4439e-03, -3.5291e-02, -4.4332e-03, -5.3185e-03,  9.2419e-03,\n",
      "        -3.1273e-02, -2.7585e-02, -2.5059e-02, -1.0785e-02,  3.4775e-02,\n",
      "         1.0241e-02, -1.7156e-02, -1.7528e-02, -2.3723e-02,  1.7166e-02,\n",
      "        -3.3454e-02,  1.4325e-02, -1.3371e-02,  1.5313e-02, -2.1552e-02,\n",
      "         8.3805e-03, -2.5357e-02,  2.6314e-04, -3.4064e-02,  3.2926e-02,\n",
      "        -2.9328e-03,  2.8419e-02,  8.2221e-03, -1.5336e-02, -2.3525e-02,\n",
      "        -3.2103e-02, -1.5866e-02, -3.3254e-02,  4.4830e-03, -1.3746e-02,\n",
      "        -1.2145e-02, -2.2447e-02,  1.2603e-02, -3.7703e-03, -3.4569e-02,\n",
      "        -1.6775e-02,  3.2008e-02, -3.5304e-02,  5.4651e-03,  1.6146e-02,\n",
      "         2.8436e-02,  1.6949e-02, -3.2947e-02,  1.6428e-02, -1.5781e-02,\n",
      "        -5.5116e-03, -2.8354e-03, -3.4473e-02,  3.2937e-02,  3.9153e-03,\n",
      "        -2.8930e-03, -2.1048e-02,  3.0901e-02,  1.1264e-02,  2.8780e-02,\n",
      "         2.0588e-02, -1.6795e-03, -1.0567e-02, -1.1277e-03,  8.3309e-03,\n",
      "         3.3992e-02, -9.0595e-03, -3.1081e-02, -5.0426e-03,  1.2441e-03,\n",
      "         2.5319e-02,  2.6277e-02,  1.6714e-02,  2.5890e-02,  2.8107e-03,\n",
      "         2.9929e-02,  5.3880e-03,  1.1385e-02,  1.1686e-02, -1.2267e-02,\n",
      "         2.8471e-02, -2.3096e-02,  3.0135e-02, -2.0202e-02,  2.5644e-02,\n",
      "         3.3483e-02, -2.8429e-02, -2.8555e-02,  1.4240e-02,  1.9737e-02,\n",
      "        -5.2780e-03,  1.9424e-02,  1.7661e-02,  2.9049e-02,  1.7183e-02,\n",
      "         2.1352e-02,  2.5320e-02, -4.6149e-03,  1.0157e-02, -1.1742e-02,\n",
      "         2.0574e-02,  9.4934e-03,  1.6733e-02, -2.9457e-03, -2.4653e-02,\n",
      "         2.8388e-02,  3.3370e-02, -3.1712e-02, -8.3865e-03,  3.1065e-02,\n",
      "        -1.4182e-02, -7.8021e-03,  8.5831e-03, -7.9906e-03,  3.0672e-02,\n",
      "        -2.9026e-02,  8.7343e-03,  8.8265e-03, -2.4591e-02, -3.5196e-02,\n",
      "        -3.4343e-02,  2.3798e-02, -1.0426e-02,  3.0944e-02, -2.0970e-02,\n",
      "        -2.0756e-02, -1.5773e-02,  1.4271e-02, -2.2270e-02,  1.1066e-02,\n",
      "         2.9248e-02, -2.7532e-02,  3.2087e-02,  1.3180e-02,  1.6571e-02,\n",
      "        -2.3483e-02, -1.2931e-02, -1.3589e-02,  4.9969e-03, -3.5726e-03,\n",
      "         1.8511e-02, -2.4161e-02,  2.1216e-02,  3.0717e-02,  1.1838e-02,\n",
      "        -8.3807e-03, -6.2785e-03,  1.8263e-03,  3.1290e-02,  2.2377e-02,\n",
      "         7.6081e-03, -2.5539e-02,  5.4034e-03,  1.8558e-02,  2.6288e-02,\n",
      "        -1.4086e-02, -4.4350e-03, -1.6857e-02, -2.5377e-03,  1.6605e-02,\n",
      "         1.6993e-02,  1.9846e-02, -3.2803e-02, -2.1285e-02,  5.0154e-03,\n",
      "        -3.0065e-02,  2.3349e-02,  2.5931e-02, -8.4100e-03,  2.2579e-02,\n",
      "         3.3570e-02, -2.2944e-02,  3.3779e-02, -8.1012e-03,  9.4511e-03,\n",
      "        -1.5003e-02, -3.7147e-03,  8.2992e-03, -1.5645e-02,  3.3554e-02,\n",
      "        -4.8257e-03,  1.3649e-02,  1.0428e-02, -3.1149e-02,  3.3984e-02,\n",
      "         6.9604e-03, -6.7847e-04,  1.8846e-03,  2.9073e-02, -1.8320e-02,\n",
      "         2.5890e-02, -2.0727e-03,  3.0301e-02,  3.3230e-02,  7.4130e-03,\n",
      "        -7.8611e-03, -5.6250e-03,  1.0842e-02,  6.5163e-04,  3.0954e-03,\n",
      "        -1.2217e-07,  1.4294e-02, -8.7935e-03,  1.1378e-03], device='cuda:0')\n",
      "epoch : 1/3000, training loss = 0.055058,validation loss = 0.032982\n",
      "epoch : 2/3000, training loss = 0.028894,validation loss = 0.025824\n",
      "epoch : 3/3000, training loss = 0.024450,validation loss = 0.023483\n",
      "epoch : 4/3000, training loss = 0.023082,validation loss = 0.022614\n",
      "epoch : 5/3000, training loss = 0.021746,validation loss = 0.020952\n",
      "epoch : 6/3000, training loss = 0.019911,validation loss = 0.019124\n",
      "epoch : 7/3000, training loss = 0.018556,validation loss = 0.018534\n",
      "epoch : 8/3000, training loss = 0.017660,validation loss = 0.017127\n",
      "epoch : 9/3000, training loss = 0.016633,validation loss = 0.016575\n",
      "epoch : 10/3000, training loss = 0.016107,validation loss = 0.016124\n",
      "epoch : 11/3000, training loss = 0.015796,validation loss = 0.016046\n",
      "epoch : 12/3000, training loss = 0.015233,validation loss = 0.015366\n",
      "epoch : 13/3000, training loss = 0.014669,validation loss = 0.015146\n",
      "epoch : 14/3000, training loss = 0.014528,validation loss = 0.015057\n",
      "epoch : 15/3000, training loss = 0.014396,validation loss = 0.014966\n",
      "epoch : 16/3000, training loss = 0.014268,validation loss = 0.014834\n",
      "epoch : 17/3000, training loss = 0.014138,validation loss = 0.014828\n",
      "epoch : 18/3000, training loss = 0.013981,validation loss = 0.014701\n",
      "epoch : 19/3000, training loss = 0.013448,validation loss = 0.014115\n",
      "epoch : 20/3000, training loss = 0.013354,validation loss = 0.014171\n",
      "epoch : 21/3000, training loss = 0.013278,validation loss = 0.014166\n",
      "epoch : 22/3000, training loss = 0.013170,validation loss = 0.014113\n",
      "epoch : 23/3000, training loss = 0.013099,validation loss = 0.014014\n",
      "epoch : 24/3000, training loss = 0.013044,validation loss = 0.013948\n",
      "epoch : 25/3000, training loss = 0.013037,validation loss = 0.014019\n",
      "epoch : 26/3000, training loss = 0.013020,validation loss = 0.014068\n",
      "epoch : 27/3000, training loss = 0.012920,validation loss = 0.013879\n",
      "epoch : 28/3000, training loss = 0.012798,validation loss = 0.013804\n",
      "epoch : 29/3000, training loss = 0.012727,validation loss = 0.013848\n",
      "epoch : 30/3000, training loss = 0.012700,validation loss = 0.013926\n",
      "epoch : 31/3000, training loss = 0.012758,validation loss = 0.013932\n",
      "epoch : 32/3000, training loss = 0.012789,validation loss = 0.014133\n",
      "epoch : 33/3000, training loss = 0.012795,validation loss = 0.013922\n",
      "epoch : 34/3000, training loss = 0.012643,validation loss = 0.013834\n",
      "epoch : 35/3000, training loss = 0.012572,validation loss = 0.013740\n",
      "epoch : 36/3000, training loss = 0.012481,validation loss = 0.013672\n",
      "epoch : 37/3000, training loss = 0.012447,validation loss = 0.013760\n",
      "epoch : 38/3000, training loss = 0.012431,validation loss = 0.013761\n",
      "epoch : 39/3000, training loss = 0.012392,validation loss = 0.013791\n",
      "epoch : 40/3000, training loss = 0.012370,validation loss = 0.013857\n",
      "epoch : 41/3000, training loss = 0.012354,validation loss = 0.013794\n",
      "epoch : 42/3000, training loss = 0.012388,validation loss = 0.013888\n",
      "epoch : 43/3000, training loss = 0.012413,validation loss = 0.013858\n",
      "epoch : 44/3000, training loss = 0.012467,validation loss = 0.013972\n",
      "epoch : 45/3000, training loss = 0.012425,validation loss = 0.013870\n",
      "epoch : 46/3000, training loss = 0.012331,validation loss = 0.013775\n",
      "epoch : 47/3000, training loss = 0.012295,validation loss = 0.013814\n",
      "epoch : 48/3000, training loss = 0.012207,validation loss = 0.013829\n",
      "epoch : 49/3000, training loss = 0.012122,validation loss = 0.013796\n",
      "epoch : 50/3000, training loss = 0.012095,validation loss = 0.013800\n",
      "epoch : 51/3000, training loss = 0.012085,validation loss = 0.013590\n",
      "epoch : 52/3000, training loss = 0.011826,validation loss = 0.013599\n",
      "epoch : 53/3000, training loss = 0.011808,validation loss = 0.013549\n",
      "epoch : 54/3000, training loss = 0.011803,validation loss = 0.013653\n",
      "epoch : 55/3000, training loss = 0.011809,validation loss = 0.013498\n",
      "epoch : 56/3000, training loss = 0.011822,validation loss = 0.013726\n",
      "epoch : 57/3000, training loss = 0.011858,validation loss = 0.013626\n",
      "epoch : 58/3000, training loss = 0.011969,validation loss = 0.013775\n",
      "epoch : 59/3000, training loss = 0.011961,validation loss = 0.013879\n",
      "epoch : 60/3000, training loss = 0.011880,validation loss = 0.013941\n",
      "epoch : 61/3000, training loss = 0.011824,validation loss = 0.013818\n",
      "epoch : 62/3000, training loss = 0.011705,validation loss = 0.013789\n",
      "epoch : 65/3000, training loss = 0.011618,validation loss = 0.013721\n",
      "epoch : 66/3000, training loss = 0.011620,validation loss = 0.013673\n",
      "epoch : 67/3000, training loss = 0.011602,validation loss = 0.013478\n",
      "epoch : 68/3000, training loss = 0.011601,validation loss = 0.013718\n",
      "epoch : 69/3000, training loss = 0.011603,validation loss = 0.013727\n",
      "epoch : 70/3000, training loss = 0.011603,validation loss = 0.013649\n",
      "epoch : 71/3000, training loss = 0.011604,validation loss = 0.013760\n",
      "epoch : 72/3000, training loss = 0.011620,validation loss = 0.013740\n",
      "epoch : 73/3000, training loss = 0.011628,validation loss = 0.013674\n",
      "epoch : 74/3000, training loss = 0.011645,validation loss = 0.013747\n",
      "epoch : 75/3000, training loss = 0.011605,validation loss = 0.013673\n",
      "epoch : 76/3000, training loss = 0.011584,validation loss = 0.013711\n",
      "epoch : 77/3000, training loss = 0.011604,validation loss = 0.013814\n",
      "epoch : 78/3000, training loss = 0.011604,validation loss = 0.013928\n",
      "epoch : 79/3000, training loss = 0.011621,validation loss = 0.013793\n",
      "epoch : 80/3000, training loss = 0.011605,validation loss = 0.013995\n",
      "epoch : 81/3000, training loss = 0.011557,validation loss = 0.013989\n",
      "epoch : 82/3000, training loss = 0.011565,validation loss = 0.014001\n",
      "epoch : 83/3000, training loss = 0.011576,validation loss = 0.013996\n",
      "epoch : 84/3000, training loss = 0.011623,validation loss = 0.014123\n",
      "epoch : 85/3000, training loss = 0.011618,validation loss = 0.014039\n",
      "epoch : 86/3000, training loss = 0.011527,validation loss = 0.013994\n",
      "epoch : 87/3000, training loss = 0.011446,validation loss = 0.013950\n",
      "epoch : 88/3000, training loss = 0.011375,validation loss = 0.013862\n",
      "epoch : 89/3000, training loss = 0.011353,validation loss = 0.013772\n",
      "epoch : 90/3000, training loss = 0.011302,validation loss = 0.013736\n",
      "epoch : 91/3000, training loss = 0.011293,validation loss = 0.013654\n",
      "epoch : 92/3000, training loss = 0.011271,validation loss = 0.013685\n",
      "epoch : 93/3000, training loss = 0.011265,validation loss = 0.013880\n",
      "epoch : 94/3000, training loss = 0.011271,validation loss = 0.013738\n",
      "epoch : 95/3000, training loss = 0.011278,validation loss = 0.013577\n",
      "epoch : 96/3000, training loss = 0.011273,validation loss = 0.013671\n",
      "epoch : 97/3000, training loss = 0.011293,validation loss = 0.013773\n",
      "epoch : 98/3000, training loss = 0.011353,validation loss = 0.013939\n",
      "epoch : 99/3000, training loss = 0.011413,validation loss = 0.013854\n",
      "epoch : 100/3000, training loss = 0.011394,validation loss = 0.013987\n",
      "epoch : 101/3000, training loss = 0.011412,validation loss = 0.013943\n",
      "epoch : 102/3000, training loss = 0.011461,validation loss = 0.013782\n",
      "epoch : 103/3000, training loss = 0.011428,validation loss = 0.013771\n",
      "epoch : 104/3000, training loss = 0.011340,validation loss = 0.013884\n",
      "epoch : 105/3000, training loss = 0.011228,validation loss = 0.013709\n",
      "epoch : 106/3000, training loss = 0.011162,validation loss = 0.013650\n",
      "epoch : 107/3000, training loss = 0.011111,validation loss = 0.013755\n",
      "epoch : 108/3000, training loss = 0.011113,validation loss = 0.013699\n",
      "epoch : 109/3000, training loss = 0.011109,validation loss = 0.013713\n",
      "epoch : 110/3000, training loss = 0.011139,validation loss = 0.013726\n",
      "epoch : 111/3000, training loss = 0.011130,validation loss = 0.013639\n",
      "epoch : 112/3000, training loss = 0.011120,validation loss = 0.013642\n",
      "epoch : 113/3000, training loss = 0.011143,validation loss = 0.013776\n",
      "epoch : 114/3000, training loss = 0.011181,validation loss = 0.013693\n",
      "epoch : 115/3000, training loss = 0.011166,validation loss = 0.013658\n",
      "epoch : 116/3000, training loss = 0.011114,validation loss = 0.013744\n",
      "epoch : 117/3000, training loss = 0.011116,validation loss = 0.013707\n",
      "epoch : 118/3000, training loss = 0.011143,validation loss = 0.013744\n",
      "epoch : 119/3000, training loss = 0.011169,validation loss = 0.013772\n",
      "epoch : 120/3000, training loss = 0.011203,validation loss = 0.013824\n",
      "epoch : 121/3000, training loss = 0.011189,validation loss = 0.013922\n",
      "epoch : 122/3000, training loss = 0.011140,validation loss = 0.013788\n",
      "epoch : 123/3000, training loss = 0.011055,validation loss = 0.013720\n",
      "epoch : 124/3000, training loss = 0.011040,validation loss = 0.013743\n",
      "epoch : 125/3000, training loss = 0.011063,validation loss = 0.013857\n",
      "epoch : 126/3000, training loss = 0.011037,validation loss = 0.013758\n",
      "epoch : 127/3000, training loss = 0.011041,validation loss = 0.013779\n",
      "epoch : 128/3000, training loss = 0.011048,validation loss = 0.013832\n",
      "epoch : 129/3000, training loss = 0.011050,validation loss = 0.013971\n",
      "epoch : 130/3000, training loss = 0.011058,validation loss = 0.013886\n",
      "epoch : 131/3000, training loss = 0.011100,validation loss = 0.013951\n",
      "epoch : 132/3000, training loss = 0.011104,validation loss = 0.013847\n",
      "epoch : 133/3000, training loss = 0.011151,validation loss = 0.013845\n",
      "epoch : 134/3000, training loss = 0.011175,validation loss = 0.013875\n",
      "epoch : 135/3000, training loss = 0.011177,validation loss = 0.013921\n",
      "epoch : 136/3000, training loss = 0.011162,validation loss = 0.013985\n",
      "epoch : 137/3000, training loss = 0.011239,validation loss = 0.013965\n",
      "epoch : 138/3000, training loss = 0.011260,validation loss = 0.014064\n",
      "epoch : 139/3000, training loss = 0.011277,validation loss = 0.013986\n",
      "epoch : 140/3000, training loss = 0.011178,validation loss = 0.013904\n",
      "epoch : 141/3000, training loss = 0.011106,validation loss = 0.013874\n",
      "epoch : 142/3000, training loss = 0.010989,validation loss = 0.013812\n",
      "epoch : 143/3000, training loss = 0.010904,validation loss = 0.013735\n",
      "epoch : 144/3000, training loss = 0.010894,validation loss = 0.013747\n",
      "epoch : 145/3000, training loss = 0.010951,validation loss = 0.013799\n",
      "epoch : 146/3000, training loss = 0.010954,validation loss = 0.013756\n",
      "epoch : 147/3000, training loss = 0.010921,validation loss = 0.013680\n",
      "epoch : 148/3000, training loss = 0.010911,validation loss = 0.013725\n",
      "epoch : 149/3000, training loss = 0.010920,validation loss = 0.013808\n",
      "epoch : 150/3000, training loss = 0.010955,validation loss = 0.013712\n",
      "epoch : 151/3000, training loss = 0.010972,validation loss = 0.013801\n",
      "epoch : 152/3000, training loss = 0.010914,validation loss = 0.013745\n",
      "epoch : 153/3000, training loss = 0.010888,validation loss = 0.013759\n",
      "epoch : 154/3000, training loss = 0.010927,validation loss = 0.013711\n",
      "epoch : 155/3000, training loss = 0.010944,validation loss = 0.013836\n",
      "epoch : 156/3000, training loss = 0.010974,validation loss = 0.013904\n",
      "epoch : 157/3000, training loss = 0.010966,validation loss = 0.013841\n",
      "epoch : 158/3000, training loss = 0.010912,validation loss = 0.013823\n",
      "epoch : 159/3000, training loss = 0.010907,validation loss = 0.013820\n",
      "epoch : 160/3000, training loss = 0.010900,validation loss = 0.014141\n",
      "epoch : 161/3000, training loss = 0.010900,validation loss = 0.014170\n",
      "epoch : 162/3000, training loss = 0.010916,validation loss = 0.014003\n",
      "epoch : 163/3000, training loss = 0.010947,validation loss = 0.013974\n",
      "epoch : 164/3000, training loss = 0.010925,validation loss = 0.013944\n",
      "epoch : 165/3000, training loss = 0.010954,validation loss = 0.014150\n",
      "epoch : 166/3000, training loss = 0.011028,validation loss = 0.014051\n",
      "epoch : 167/3000, training loss = 0.011056,validation loss = 0.013861\n",
      "epoch : 168/3000, training loss = 0.011067,validation loss = 0.013778\n",
      "epoch : 169/3000, training loss = 0.011086,validation loss = 0.013943\n",
      "epoch : 170/3000, training loss = 0.011083,validation loss = 0.013918\n",
      "epoch : 171/3000, training loss = 0.010993,validation loss = 0.013927\n",
      "epoch : 172/3000, training loss = 0.010858,validation loss = 0.013826\n",
      "epoch : 173/3000, training loss = 0.010782,validation loss = 0.013857\n",
      "epoch : 174/3000, training loss = 0.010798,validation loss = 0.013751\n",
      "epoch : 175/3000, training loss = 0.010816,validation loss = 0.013739\n",
      "epoch : 176/3000, training loss = 0.010805,validation loss = 0.013823\n",
      "epoch : 177/3000, training loss = 0.010834,validation loss = 0.013750\n",
      "epoch : 178/3000, training loss = 0.010831,validation loss = 0.013765\n",
      "epoch : 179/3000, training loss = 0.010822,validation loss = 0.013756\n",
      "epoch : 180/3000, training loss = 0.010820,validation loss = 0.013828\n",
      "epoch : 181/3000, training loss = 0.010812,validation loss = 0.013789\n",
      "epoch : 182/3000, training loss = 0.010796,validation loss = 0.013801\n",
      "epoch : 183/3000, training loss = 0.010812,validation loss = 0.013913\n",
      "epoch : 184/3000, training loss = 0.010804,validation loss = 0.013957\n",
      "epoch : 185/3000, training loss = 0.010822,validation loss = 0.013985\n",
      "epoch : 186/3000, training loss = 0.010801,validation loss = 0.013921\n",
      "epoch : 187/3000, training loss = 0.010806,validation loss = 0.013993\n",
      "epoch : 188/3000, training loss = 0.010795,validation loss = 0.013910\n",
      "epoch : 189/3000, training loss = 0.010836,validation loss = 0.013879\n",
      "epoch : 190/3000, training loss = 0.010820,validation loss = 0.013931\n",
      "epoch : 191/3000, training loss = 0.010789,validation loss = 0.013927\n",
      "epoch : 192/3000, training loss = 0.010787,validation loss = 0.013839\n",
      "epoch : 193/3000, training loss = 0.010752,validation loss = 0.013802\n",
      "epoch : 194/3000, training loss = 0.010740,validation loss = 0.013896\n",
      "epoch : 195/3000, training loss = 0.010815,validation loss = 0.014007\n",
      "epoch : 196/3000, training loss = 0.010851,validation loss = 0.014206\n",
      "epoch : 197/3000, training loss = 0.010923,validation loss = 0.014224\n",
      "epoch : 198/3000, training loss = 0.010900,validation loss = 0.014157\n",
      "epoch : 199/3000, training loss = 0.010956,validation loss = 0.013978\n",
      "epoch : 200/3000, training loss = 0.011068,validation loss = 0.013916\n",
      "epoch : 201/3000, training loss = 0.011046,validation loss = 0.013896\n",
      "epoch : 202/3000, training loss = 0.010929,validation loss = 0.013929\n",
      "epoch : 203/3000, training loss = 0.010823,validation loss = 0.013840\n",
      "epoch : 204/3000, training loss = 0.010712,validation loss = 0.013844\n",
      "epoch : 205/3000, training loss = 0.010660,validation loss = 0.013848\n",
      "epoch : 206/3000, training loss = 0.010659,validation loss = 0.013806\n",
      "epoch : 207/3000, training loss = 0.010683,validation loss = 0.013826\n",
      "epoch : 208/3000, training loss = 0.010724,validation loss = 0.013930\n",
      "epoch : 209/3000, training loss = 0.010703,validation loss = 0.013811\n",
      "epoch : 210/3000, training loss = 0.010729,validation loss = 0.013973\n",
      "epoch : 211/3000, training loss = 0.010694,validation loss = 0.013811\n",
      "epoch : 212/3000, training loss = 0.010717,validation loss = 0.013854\n",
      "epoch : 213/3000, training loss = 0.010703,validation loss = 0.013848\n",
      "epoch : 214/3000, training loss = 0.010700,validation loss = 0.013816\n",
      "epoch : 215/3000, training loss = 0.010694,validation loss = 0.013814\n",
      "epoch : 216/3000, training loss = 0.010668,validation loss = 0.013869\n",
      "epoch : 217/3000, training loss = 0.010659,validation loss = 0.013889\n",
      "epoch : 218/3000, training loss = 0.010708,validation loss = 0.014033\n",
      "epoch : 219/3000, training loss = 0.010702,validation loss = 0.013884\n",
      "epoch : 220/3000, training loss = 0.010680,validation loss = 0.014028\n",
      "epoch : 221/3000, training loss = 0.010710,validation loss = 0.014031\n",
      "epoch : 222/3000, training loss = 0.010767,validation loss = 0.013939\n",
      "epoch : 223/3000, training loss = 0.010779,validation loss = 0.013911\n",
      "epoch : 224/3000, training loss = 0.010753,validation loss = 0.013885\n",
      "epoch : 225/3000, training loss = 0.010772,validation loss = 0.013974\n",
      "epoch : 226/3000, training loss = 0.010708,validation loss = 0.013863\n",
      "epoch : 227/3000, training loss = 0.010644,validation loss = 0.013894\n",
      "epoch : 228/3000, training loss = 0.010636,validation loss = 0.013957\n",
      "epoch : 229/3000, training loss = 0.010647,validation loss = 0.013886\n",
      "epoch : 230/3000, training loss = 0.010644,validation loss = 0.013899\n",
      "epoch : 231/3000, training loss = 0.010671,validation loss = 0.013876\n",
      "epoch : 232/3000, training loss = 0.010696,validation loss = 0.013995\n",
      "epoch : 233/3000, training loss = 0.010762,validation loss = 0.014009\n",
      "epoch : 234/3000, training loss = 0.010770,validation loss = 0.014125\n",
      "epoch : 235/3000, training loss = 0.010819,validation loss = 0.014144\n",
      "epoch : 236/3000, training loss = 0.010794,validation loss = 0.013979\n",
      "epoch : 237/3000, training loss = 0.010721,validation loss = 0.013963\n",
      "epoch : 238/3000, training loss = 0.010664,validation loss = 0.014050\n",
      "epoch : 239/3000, training loss = 0.010648,validation loss = 0.014218\n",
      "epoch : 240/3000, training loss = 0.010616,validation loss = 0.014124\n",
      "epoch : 241/3000, training loss = 0.010627,validation loss = 0.014215\n",
      "epoch : 242/3000, training loss = 0.010665,validation loss = 0.014025\n",
      "epoch : 243/3000, training loss = 0.010662,validation loss = 0.014058\n",
      "epoch : 244/3000, training loss = 0.010672,validation loss = 0.014137\n",
      "epoch : 245/3000, training loss = 0.010677,validation loss = 0.014103\n",
      "epoch : 246/3000, training loss = 0.010731,validation loss = 0.014075\n",
      "epoch : 247/3000, training loss = 0.010765,validation loss = 0.014207\n",
      "epoch : 248/3000, training loss = 0.010772,validation loss = 0.014190\n",
      "epoch : 249/3000, training loss = 0.010811,validation loss = 0.014056\n",
      "epoch : 250/3000, training loss = 0.010868,validation loss = 0.014041\n",
      "epoch : 251/3000, training loss = 0.010894,validation loss = 0.014156\n",
      "epoch : 252/3000, training loss = 0.011021,validation loss = 0.014411\n",
      "epoch : 253/3000, training loss = 0.011085,validation loss = 0.014184\n",
      "epoch : 254/3000, training loss = 0.011014,validation loss = 0.014081\n",
      "epoch : 255/3000, training loss = 0.010824,validation loss = 0.014162\n",
      "epoch : 256/3000, training loss = 0.010649,validation loss = 0.013963\n",
      "epoch : 257/3000, training loss = 0.010534,validation loss = 0.013918\n",
      "epoch : 258/3000, training loss = 0.010536,validation loss = 0.013945\n",
      "epoch : 259/3000, training loss = 0.010566,validation loss = 0.013957\n",
      "epoch : 260/3000, training loss = 0.010626,validation loss = 0.013952\n",
      "epoch : 261/3000, training loss = 0.010601,validation loss = 0.013888\n",
      "epoch : 262/3000, training loss = 0.010613,validation loss = 0.013950\n",
      "epoch : 263/3000, training loss = 0.010597,validation loss = 0.014054\n",
      "epoch : 264/3000, training loss = 0.010589,validation loss = 0.014003\n",
      "epoch : 265/3000, training loss = 0.010592,validation loss = 0.013919\n",
      "epoch : 266/3000, training loss = 0.010594,validation loss = 0.014023\n",
      "epoch : 267/3000, training loss = 0.010585,validation loss = 0.013939\n",
      "epoch : 268/3000, training loss = 0.010592,validation loss = 0.014008\n",
      "epoch : 269/3000, training loss = 0.010585,validation loss = 0.014041\n",
      "epoch : 270/3000, training loss = 0.010602,validation loss = 0.014010\n",
      "epoch : 271/3000, training loss = 0.010638,validation loss = 0.014036\n",
      "epoch : 272/3000, training loss = 0.010700,validation loss = 0.013992\n",
      "epoch : 273/3000, training loss = 0.010638,validation loss = 0.014058\n",
      "epoch : 274/3000, training loss = 0.010560,validation loss = 0.014040\n",
      "epoch : 275/3000, training loss = 0.010494,validation loss = 0.014045\n",
      "epoch : 276/3000, training loss = 0.010527,validation loss = 0.014061\n",
      "epoch : 277/3000, training loss = 0.010566,validation loss = 0.014047\n",
      "epoch : 278/3000, training loss = 0.010591,validation loss = 0.014013\n",
      "epoch : 279/3000, training loss = 0.010592,validation loss = 0.013985\n",
      "epoch : 280/3000, training loss = 0.010580,validation loss = 0.014102\n",
      "epoch : 281/3000, training loss = 0.010616,validation loss = 0.014147\n",
      "epoch : 282/3000, training loss = 0.010607,validation loss = 0.014068\n",
      "epoch : 283/3000, training loss = 0.010634,validation loss = 0.014101\n",
      "epoch : 284/3000, training loss = 0.010684,validation loss = 0.014063\n",
      "epoch : 285/3000, training loss = 0.010695,validation loss = 0.014112\n",
      "epoch : 286/3000, training loss = 0.010590,validation loss = 0.014174\n",
      "epoch : 287/3000, training loss = 0.010570,validation loss = 0.014390\n",
      "epoch : 288/3000, training loss = 0.010540,validation loss = 0.014151\n",
      "epoch : 289/3000, training loss = 0.010545,validation loss = 0.014111\n",
      "epoch : 290/3000, training loss = 0.010564,validation loss = 0.014054\n",
      "epoch : 291/3000, training loss = 0.010546,validation loss = 0.014092\n",
      "epoch : 292/3000, training loss = 0.010564,validation loss = 0.014215\n",
      "epoch : 293/3000, training loss = 0.010584,validation loss = 0.014218\n",
      "epoch : 294/3000, training loss = 0.010584,validation loss = 0.014100\n",
      "epoch : 295/3000, training loss = 0.010611,validation loss = 0.014014\n",
      "epoch : 296/3000, training loss = 0.010612,validation loss = 0.014186\n",
      "epoch : 297/3000, training loss = 0.010652,validation loss = 0.014315\n",
      "epoch : 298/3000, training loss = 0.010817,validation loss = 0.014296\n",
      "epoch : 299/3000, training loss = 0.010736,validation loss = 0.014235\n",
      "epoch : 300/3000, training loss = 0.010605,validation loss = 0.014116\n",
      "epoch : 301/3000, training loss = 0.010538,validation loss = 0.014079\n",
      "epoch : 302/3000, training loss = 0.010493,validation loss = 0.014111\n",
      "epoch : 303/3000, training loss = 0.010498,validation loss = 0.014062\n",
      "epoch : 304/3000, training loss = 0.010515,validation loss = 0.014086\n",
      "epoch : 305/3000, training loss = 0.010510,validation loss = 0.014163\n",
      "epoch : 306/3000, training loss = 0.010551,validation loss = 0.014103\n",
      "epoch : 307/3000, training loss = 0.010569,validation loss = 0.014141\n",
      "epoch : 308/3000, training loss = 0.010536,validation loss = 0.014093\n",
      "epoch : 309/3000, training loss = 0.010497,validation loss = 0.014028\n",
      "epoch : 310/3000, training loss = 0.010542,validation loss = 0.014207\n",
      "epoch : 311/3000, training loss = 0.010546,validation loss = 0.014044\n",
      "epoch : 312/3000, training loss = 0.010559,validation loss = 0.014091\n",
      "epoch : 313/3000, training loss = 0.010564,validation loss = 0.014123\n",
      "epoch : 314/3000, training loss = 0.010607,validation loss = 0.014095\n",
      "epoch : 315/3000, training loss = 0.010602,validation loss = 0.014312\n",
      "epoch : 316/3000, training loss = 0.010541,validation loss = 0.014126\n",
      "epoch : 317/3000, training loss = 0.010527,validation loss = 0.014257\n",
      "epoch : 318/3000, training loss = 0.010563,validation loss = 0.014296\n",
      "epoch : 319/3000, training loss = 0.010614,validation loss = 0.014527\n",
      "epoch : 320/3000, training loss = 0.010646,validation loss = 0.014373\n",
      "epoch : 321/3000, training loss = 0.010628,validation loss = 0.014182\n",
      "epoch : 322/3000, training loss = 0.010547,validation loss = 0.014233\n",
      "epoch : 323/3000, training loss = 0.010540,validation loss = 0.014166\n",
      "epoch : 324/3000, training loss = 0.010557,validation loss = 0.014193\n",
      "epoch : 325/3000, training loss = 0.010538,validation loss = 0.014317\n",
      "epoch : 326/3000, training loss = 0.010620,validation loss = 0.014324\n",
      "epoch : 327/3000, training loss = 0.010632,validation loss = 0.014319\n",
      "epoch : 328/3000, training loss = 0.010714,validation loss = 0.014223\n",
      "epoch : 329/3000, training loss = 0.010836,validation loss = 0.014309\n",
      "epoch : 330/3000, training loss = 0.010888,validation loss = 0.014249\n",
      "epoch : 331/3000, training loss = 0.010863,validation loss = 0.014441\n",
      "epoch : 332/3000, training loss = 0.010773,validation loss = 0.014634\n",
      "epoch : 333/3000, training loss = 0.010674,validation loss = 0.014448\n",
      "epoch : 334/3000, training loss = 0.010526,validation loss = 0.014233\n",
      "epoch : 335/3000, training loss = 0.010480,validation loss = 0.014269\n",
      "epoch : 336/3000, training loss = 0.010505,validation loss = 0.014274\n",
      "epoch : 337/3000, training loss = 0.010516,validation loss = 0.014192\n",
      "epoch : 338/3000, training loss = 0.010462,validation loss = 0.014136\n",
      "epoch : 339/3000, training loss = 0.010478,validation loss = 0.014201\n",
      "epoch : 340/3000, training loss = 0.010492,validation loss = 0.014150\n",
      "epoch : 341/3000, training loss = 0.010492,validation loss = 0.014156\n",
      "epoch : 342/3000, training loss = 0.010531,validation loss = 0.014301\n",
      "epoch : 343/3000, training loss = 0.010497,validation loss = 0.014234\n",
      "epoch : 344/3000, training loss = 0.010493,validation loss = 0.014366\n",
      "epoch : 345/3000, training loss = 0.010519,validation loss = 0.014275\n",
      "epoch : 346/3000, training loss = 0.010525,validation loss = 0.014215\n",
      "epoch : 347/3000, training loss = 0.010530,validation loss = 0.014367\n",
      "epoch : 348/3000, training loss = 0.010520,validation loss = 0.014236\n",
      "epoch : 349/3000, training loss = 0.010472,validation loss = 0.014191\n",
      "epoch : 350/3000, training loss = 0.010464,validation loss = 0.014191\n",
      "epoch : 351/3000, training loss = 0.010491,validation loss = 0.014235\n",
      "epoch : 352/3000, training loss = 0.010493,validation loss = 0.014148\n",
      "epoch : 353/3000, training loss = 0.010558,validation loss = 0.014206\n",
      "epoch : 354/3000, training loss = 0.010532,validation loss = 0.014172\n",
      "epoch : 355/3000, training loss = 0.010504,validation loss = 0.014173\n",
      "epoch : 356/3000, training loss = 0.010521,validation loss = 0.014314\n",
      "epoch : 357/3000, training loss = 0.010482,validation loss = 0.014251\n",
      "epoch : 358/3000, training loss = 0.010498,validation loss = 0.014339\n",
      "epoch : 359/3000, training loss = 0.010540,validation loss = 0.014312\n",
      "epoch : 360/3000, training loss = 0.010563,validation loss = 0.014495\n",
      "epoch : 361/3000, training loss = 0.010541,validation loss = 0.014488\n",
      "epoch : 362/3000, training loss = 0.010617,validation loss = 0.014523\n",
      "epoch : 363/3000, training loss = 0.010505,validation loss = 0.014141\n",
      "epoch : 364/3000, training loss = 0.010460,validation loss = 0.014334\n",
      "epoch : 365/3000, training loss = 0.010473,validation loss = 0.014302\n",
      "epoch : 366/3000, training loss = 0.010489,validation loss = 0.014494\n",
      "epoch : 367/3000, training loss = 0.010472,validation loss = 0.014253\n",
      "epoch : 368/3000, training loss = 0.010473,validation loss = 0.014185\n",
      "epoch : 369/3000, training loss = 0.010452,validation loss = 0.014343\n",
      "epoch : 370/3000, training loss = 0.010519,validation loss = 0.014376\n",
      "epoch : 371/3000, training loss = 0.010537,validation loss = 0.014242\n",
      "epoch : 372/3000, training loss = 0.010546,validation loss = 0.014185\n",
      "epoch : 373/3000, training loss = 0.010557,validation loss = 0.014241\n",
      "epoch : 374/3000, training loss = 0.010562,validation loss = 0.014258\n",
      "epoch : 375/3000, training loss = 0.010556,validation loss = 0.014384\n",
      "epoch : 376/3000, training loss = 0.010613,validation loss = 0.014411\n",
      "epoch : 377/3000, training loss = 0.010669,validation loss = 0.014468\n",
      "epoch : 378/3000, training loss = 0.010634,validation loss = 0.014499\n",
      "epoch : 379/3000, training loss = 0.010577,validation loss = 0.014350\n",
      "epoch : 380/3000, training loss = 0.010560,validation loss = 0.014371\n",
      "epoch : 381/3000, training loss = 0.010519,validation loss = 0.014343\n",
      "epoch : 382/3000, training loss = 0.010506,validation loss = 0.014278\n",
      "epoch : 383/3000, training loss = 0.010457,validation loss = 0.014201\n",
      "epoch : 384/3000, training loss = 0.010428,validation loss = 0.014316\n",
      "epoch : 385/3000, training loss = 0.010435,validation loss = 0.014212\n",
      "epoch : 386/3000, training loss = 0.010449,validation loss = 0.014223\n",
      "epoch : 387/3000, training loss = 0.010533,validation loss = 0.014330\n",
      "epoch : 388/3000, training loss = 0.010485,validation loss = 0.014348\n",
      "epoch : 389/3000, training loss = 0.010441,validation loss = 0.014319\n",
      "epoch : 390/3000, training loss = 0.010446,validation loss = 0.014422\n",
      "epoch : 391/3000, training loss = 0.010447,validation loss = 0.014268\n",
      "epoch : 392/3000, training loss = 0.010482,validation loss = 0.014311\n",
      "epoch : 393/3000, training loss = 0.010489,validation loss = 0.014294\n",
      "epoch : 394/3000, training loss = 0.010513,validation loss = 0.014407\n",
      "epoch : 395/3000, training loss = 0.010485,validation loss = 0.014340\n",
      "epoch : 396/3000, training loss = 0.010524,validation loss = 0.014366\n",
      "epoch : 397/3000, training loss = 0.010543,validation loss = 0.014484\n",
      "epoch : 398/3000, training loss = 0.010520,validation loss = 0.014350\n",
      "epoch : 399/3000, training loss = 0.010506,validation loss = 0.014227\n",
      "epoch : 400/3000, training loss = 0.010552,validation loss = 0.014329\n",
      "epoch : 401/3000, training loss = 0.010552,validation loss = 0.014294\n",
      "epoch : 402/3000, training loss = 0.010523,validation loss = 0.014402\n",
      "epoch : 403/3000, training loss = 0.010560,validation loss = 0.014557\n",
      "epoch : 404/3000, training loss = 0.010543,validation loss = 0.014752\n",
      "epoch : 405/3000, training loss = 0.010591,validation loss = 0.014632\n",
      "epoch : 406/3000, training loss = 0.010626,validation loss = 0.014554\n",
      "epoch : 407/3000, training loss = 0.010637,validation loss = 0.014634\n",
      "epoch : 408/3000, training loss = 0.010651,validation loss = 0.014460\n",
      "epoch : 409/3000, training loss = 0.010703,validation loss = 0.014670\n",
      "epoch : 410/3000, training loss = 0.010783,validation loss = 0.014705\n",
      "epoch : 411/3000, training loss = 0.010894,validation loss = 0.015176\n",
      "epoch : 412/3000, training loss = 0.010890,validation loss = 0.015025\n",
      "epoch : 413/3000, training loss = 0.010713,validation loss = 0.014743\n",
      "epoch : 414/3000, training loss = 0.010575,validation loss = 0.014649\n",
      "epoch : 415/3000, training loss = 0.010391,validation loss = 0.014233\n",
      "epoch : 416/3000, training loss = 0.010331,validation loss = 0.014188\n",
      "epoch : 417/3000, training loss = 0.010339,validation loss = 0.014181\n",
      "epoch : 418/3000, training loss = 0.010360,validation loss = 0.014033\n",
      "epoch : 419/3000, training loss = 0.010364,validation loss = 0.014143\n",
      "epoch : 420/3000, training loss = 0.010354,validation loss = 0.014095\n",
      "epoch : 421/3000, training loss = 0.010321,validation loss = 0.014014\n",
      "epoch : 422/3000, training loss = 0.010368,validation loss = 0.014122\n",
      "epoch : 423/3000, training loss = 0.010398,validation loss = 0.014196\n",
      "epoch : 424/3000, training loss = 0.010341,validation loss = 0.014147\n",
      "epoch : 425/3000, training loss = 0.010413,validation loss = 0.014146\n",
      "epoch : 426/3000, training loss = 0.010354,validation loss = 0.014183\n",
      "epoch : 427/3000, training loss = 0.010331,validation loss = 0.014253\n",
      "epoch : 428/3000, training loss = 0.010408,validation loss = 0.014151\n",
      "epoch : 429/3000, training loss = 0.010337,validation loss = 0.014150\n",
      "epoch : 430/3000, training loss = 0.010344,validation loss = 0.014142\n",
      "epoch : 431/3000, training loss = 0.010373,validation loss = 0.014073\n",
      "epoch : 432/3000, training loss = 0.010311,validation loss = 0.014172\n",
      "epoch : 433/3000, training loss = 0.010325,validation loss = 0.014184\n",
      "epoch : 434/3000, training loss = 0.010341,validation loss = 0.014296\n",
      "epoch : 435/3000, training loss = 0.010383,validation loss = 0.014100\n",
      "epoch : 436/3000, training loss = 0.010380,validation loss = 0.014102\n",
      "epoch : 437/3000, training loss = 0.010416,validation loss = 0.014167\n",
      "epoch : 438/3000, training loss = 0.010388,validation loss = 0.014108\n",
      "epoch : 439/3000, training loss = 0.010353,validation loss = 0.014078\n",
      "epoch : 440/3000, training loss = 0.010312,validation loss = 0.014235\n",
      "epoch : 441/3000, training loss = 0.010314,validation loss = 0.014191\n",
      "epoch : 442/3000, training loss = 0.010343,validation loss = 0.014219\n",
      "epoch : 443/3000, training loss = 0.010328,validation loss = 0.014261\n",
      "epoch : 444/3000, training loss = 0.010318,validation loss = 0.014279\n",
      "epoch : 445/3000, training loss = 0.010359,validation loss = 0.014261\n",
      "epoch : 446/3000, training loss = 0.010346,validation loss = 0.014093\n",
      "epoch : 447/3000, training loss = 0.010176,validation loss = 0.014090\n",
      "epoch : 448/3000, training loss = 0.010175,validation loss = 0.013946\n",
      "epoch : 449/3000, training loss = 0.010220,validation loss = 0.014048\n",
      "epoch : 450/3000, training loss = 0.010282,validation loss = 0.014084\n",
      "epoch : 451/3000, training loss = 0.010269,validation loss = 0.014052\n",
      "epoch : 452/3000, training loss = 0.010212,validation loss = 0.014075\n",
      "epoch : 453/3000, training loss = 0.010208,validation loss = 0.014010\n",
      "epoch : 454/3000, training loss = 0.010225,validation loss = 0.014203\n",
      "epoch : 455/3000, training loss = 0.010199,validation loss = 0.013976\n",
      "epoch : 456/3000, training loss = 0.010166,validation loss = 0.014060\n",
      "epoch : 457/3000, training loss = 0.010134,validation loss = 0.014082\n",
      "epoch : 459/3000, training loss = 0.010172,validation loss = 0.014160\n",
      "epoch : 460/3000, training loss = 0.010164,validation loss = 0.014151\n",
      "epoch : 461/3000, training loss = 0.010166,validation loss = 0.014123\n",
      "epoch : 462/3000, training loss = 0.010206,validation loss = 0.014025\n",
      "epoch : 463/3000, training loss = 0.010212,validation loss = 0.013943\n",
      "epoch : 464/3000, training loss = 0.010173,validation loss = 0.014063\n",
      "epoch : 465/3000, training loss = 0.010166,validation loss = 0.014025\n",
      "epoch : 466/3000, training loss = 0.010200,validation loss = 0.014129\n",
      "epoch : 467/3000, training loss = 0.010190,validation loss = 0.014190\n",
      "epoch : 468/3000, training loss = 0.010238,validation loss = 0.014073\n",
      "epoch : 469/3000, training loss = 0.010260,validation loss = 0.014115\n",
      "epoch : 470/3000, training loss = 0.010281,validation loss = 0.014229\n",
      "epoch : 471/3000, training loss = 0.010302,validation loss = 0.014311\n",
      "epoch : 472/3000, training loss = 0.010328,validation loss = 0.014348\n",
      "epoch : 473/3000, training loss = 0.010248,validation loss = 0.014222\n",
      "epoch : 474/3000, training loss = 0.010213,validation loss = 0.014183\n",
      "epoch : 475/3000, training loss = 0.010174,validation loss = 0.014106\n",
      "epoch : 476/3000, training loss = 0.010102,validation loss = 0.014130\n",
      "epoch : 477/3000, training loss = 0.010150,validation loss = 0.014140\n",
      "epoch : 478/3000, training loss = 0.010143,validation loss = 0.014131\n",
      "epoch : 479/3000, training loss = 0.010117,validation loss = 0.014011\n",
      "epoch : 480/3000, training loss = 0.010184,validation loss = 0.014007\n",
      "epoch : 481/3000, training loss = 0.010128,validation loss = 0.014049\n",
      "epoch : 482/3000, training loss = 0.010151,validation loss = 0.014043\n",
      "epoch : 483/3000, training loss = 0.010136,validation loss = 0.014069\n",
      "epoch : 484/3000, training loss = 0.010129,validation loss = 0.014005\n",
      "epoch : 485/3000, training loss = 0.010194,validation loss = 0.014037\n",
      "epoch : 486/3000, training loss = 0.010188,validation loss = 0.014079\n",
      "epoch : 487/3000, training loss = 0.010206,validation loss = 0.014414\n",
      "epoch : 488/3000, training loss = 0.010422,validation loss = 0.014096\n",
      "epoch : 489/3000, training loss = 0.010198,validation loss = 0.014137\n",
      "epoch : 490/3000, training loss = 0.010425,validation loss = 0.014883\n",
      "epoch : 491/3000, training loss = 0.010855,validation loss = 0.014208\n",
      "epoch : 492/3000, training loss = 0.010304,validation loss = 0.014342\n",
      "epoch : 493/3000, training loss = 0.010136,validation loss = 0.014268\n",
      "epoch : 494/3000, training loss = 0.010080,validation loss = 0.014099\n",
      "epoch : 495/3000, training loss = 0.010078,validation loss = 0.014089\n",
      "epoch : 496/3000, training loss = 0.010106,validation loss = 0.014189\n",
      "epoch : 497/3000, training loss = 0.010139,validation loss = 0.014227\n",
      "epoch : 498/3000, training loss = 0.010189,validation loss = 0.014189\n",
      "epoch : 499/3000, training loss = 0.010164,validation loss = 0.014209\n",
      "epoch : 500/3000, training loss = 0.010150,validation loss = 0.014364\n",
      "epoch : 501/3000, training loss = 0.010181,validation loss = 0.014193\n",
      "epoch : 502/3000, training loss = 0.010158,validation loss = 0.014238\n",
      "epoch : 503/3000, training loss = 0.010208,validation loss = 0.014379\n",
      "epoch : 504/3000, training loss = 0.010252,validation loss = 0.014242\n",
      "epoch : 505/3000, training loss = 0.010320,validation loss = 0.014282\n",
      "epoch : 506/3000, training loss = 0.010378,validation loss = 0.014451\n",
      "epoch : 507/3000, training loss = 0.010313,validation loss = 0.014424\n",
      "epoch : 508/3000, training loss = 0.010392,validation loss = 0.014697\n",
      "epoch : 509/3000, training loss = 0.010371,validation loss = 0.014627\n",
      "epoch : 510/3000, training loss = 0.010294,validation loss = 0.014568\n",
      "epoch : 511/3000, training loss = 0.010234,validation loss = 0.014417\n",
      "epoch : 512/3000, training loss = 0.010117,validation loss = 0.014267\n",
      "epoch : 513/3000, training loss = 0.010106,validation loss = 0.014219\n",
      "epoch : 514/3000, training loss = 0.010121,validation loss = 0.014104\n",
      "epoch : 515/3000, training loss = 0.010106,validation loss = 0.014176\n",
      "epoch : 516/3000, training loss = 0.010098,validation loss = 0.014186\n",
      "epoch : 517/3000, training loss = 0.010102,validation loss = 0.014105\n",
      "epoch : 518/3000, training loss = 0.010131,validation loss = 0.014071\n",
      "epoch : 519/3000, training loss = 0.010103,validation loss = 0.014201\n",
      "epoch : 520/3000, training loss = 0.010100,validation loss = 0.014011\n",
      "epoch : 521/3000, training loss = 0.010128,validation loss = 0.014065\n",
      "epoch : 522/3000, training loss = 0.010136,validation loss = 0.013982\n",
      "epoch : 523/3000, training loss = 0.010093,validation loss = 0.014131\n",
      "epoch : 524/3000, training loss = 0.010106,validation loss = 0.014097\n",
      "epoch : 525/3000, training loss = 0.010174,validation loss = 0.014166\n",
      "epoch : 526/3000, training loss = 0.010209,validation loss = 0.014048\n",
      "epoch : 527/3000, training loss = 0.010119,validation loss = 0.013955\n",
      "epoch : 528/3000, training loss = 0.010065,validation loss = 0.013961\n",
      "epoch : 529/3000, training loss = 0.010091,validation loss = 0.014099\n",
      "epoch : 530/3000, training loss = 0.010148,validation loss = 0.014119\n",
      "epoch : 531/3000, training loss = 0.010205,validation loss = 0.014092\n",
      "epoch : 532/3000, training loss = 0.010142,validation loss = 0.014135\n",
      "epoch : 533/3000, training loss = 0.010144,validation loss = 0.014238\n",
      "epoch : 534/3000, training loss = 0.010173,validation loss = 0.014298\n",
      "epoch : 535/3000, training loss = 0.010132,validation loss = 0.014228\n",
      "epoch : 536/3000, training loss = 0.010131,validation loss = 0.014338\n",
      "epoch : 537/3000, training loss = 0.010153,validation loss = 0.014269\n",
      "epoch : 538/3000, training loss = 0.010160,validation loss = 0.014232\n",
      "epoch : 539/3000, training loss = 0.010201,validation loss = 0.014471\n",
      "epoch : 540/3000, training loss = 0.010144,validation loss = 0.014386\n",
      "epoch : 541/3000, training loss = 0.010199,validation loss = 0.014299\n",
      "epoch : 542/3000, training loss = 0.010291,validation loss = 0.014205\n",
      "epoch : 543/3000, training loss = 0.010237,validation loss = 0.014303\n",
      "epoch : 544/3000, training loss = 0.010197,validation loss = 0.014240\n",
      "epoch : 545/3000, training loss = 0.010116,validation loss = 0.014245\n",
      "epoch : 546/3000, training loss = 0.010085,validation loss = 0.014194\n",
      "epoch : 547/3000, training loss = 0.010052,validation loss = 0.014230\n",
      "epoch : 548/3000, training loss = 0.010074,validation loss = 0.014243\n",
      "epoch : 549/3000, training loss = 0.010101,validation loss = 0.014152\n",
      "epoch : 550/3000, training loss = 0.010069,validation loss = 0.014238\n",
      "epoch : 551/3000, training loss = 0.010109,validation loss = 0.014204\n",
      "epoch : 552/3000, training loss = 0.010110,validation loss = 0.014344\n",
      "epoch : 553/3000, training loss = 0.010115,validation loss = 0.014159\n",
      "epoch : 554/3000, training loss = 0.010083,validation loss = 0.014199\n",
      "epoch : 555/3000, training loss = 0.010122,validation loss = 0.014407\n",
      "epoch : 556/3000, training loss = 0.010152,validation loss = 0.014149\n",
      "epoch : 557/3000, training loss = 0.010169,validation loss = 0.014368\n",
      "epoch : 558/3000, training loss = 0.010183,validation loss = 0.014444\n",
      "epoch : 559/3000, training loss = 0.010154,validation loss = 0.014253\n",
      "epoch : 560/3000, training loss = 0.010150,validation loss = 0.014395\n",
      "epoch : 561/3000, training loss = 0.010100,validation loss = 0.014302\n",
      "epoch : 562/3000, training loss = 0.010159,validation loss = 0.014333\n",
      "epoch : 563/3000, training loss = 0.010129,validation loss = 0.014182\n",
      "epoch : 564/3000, training loss = 0.010130,validation loss = 0.014323\n",
      "epoch : 565/3000, training loss = 0.010120,validation loss = 0.014407\n",
      "epoch : 566/3000, training loss = 0.010091,validation loss = 0.014373\n",
      "epoch : 567/3000, training loss = 0.010121,validation loss = 0.014366\n",
      "epoch : 568/3000, training loss = 0.010138,validation loss = 0.014416\n",
      "epoch : 569/3000, training loss = 0.010186,validation loss = 0.014507\n",
      "epoch : 570/3000, training loss = 0.010224,validation loss = 0.014334\n",
      "epoch : 571/3000, training loss = 0.010257,validation loss = 0.014291\n",
      "epoch : 572/3000, training loss = 0.010351,validation loss = 0.014390\n",
      "epoch : 573/3000, training loss = 0.010357,validation loss = 0.014327\n",
      "epoch : 574/3000, training loss = 0.010315,validation loss = 0.014145\n",
      "epoch : 575/3000, training loss = 0.010272,validation loss = 0.014142\n",
      "epoch : 576/3000, training loss = 0.010293,validation loss = 0.014120\n",
      "epoch : 577/3000, training loss = 0.010300,validation loss = 0.014046\n",
      "epoch : 578/3000, training loss = 0.010329,validation loss = 0.014138\n",
      "epoch : 579/3000, training loss = 0.010505,validation loss = 0.014167\n",
      "epoch : 580/3000, training loss = 0.010213,validation loss = 0.014309\n",
      "epoch : 581/3000, training loss = 0.010095,validation loss = 0.014248\n",
      "epoch : 582/3000, training loss = 0.010137,validation loss = 0.014200\n",
      "epoch : 583/3000, training loss = 0.010186,validation loss = 0.014266\n",
      "epoch : 584/3000, training loss = 0.010109,validation loss = 0.014096\n",
      "epoch : 585/3000, training loss = 0.010094,validation loss = 0.014119\n",
      "epoch : 586/3000, training loss = 0.010058,validation loss = 0.014117\n",
      "epoch : 587/3000, training loss = 0.010053,validation loss = 0.014094\n",
      "epoch : 588/3000, training loss = 0.010076,validation loss = 0.014145\n",
      "epoch : 589/3000, training loss = 0.010095,validation loss = 0.014227\n",
      "epoch : 590/3000, training loss = 0.010097,validation loss = 0.014302\n",
      "epoch : 591/3000, training loss = 0.010094,validation loss = 0.014161\n",
      "epoch : 592/3000, training loss = 0.010120,validation loss = 0.014116\n",
      "epoch : 593/3000, training loss = 0.010120,validation loss = 0.014071\n",
      "epoch : 594/3000, training loss = 0.010056,validation loss = 0.014004\n",
      "epoch : 595/3000, training loss = 0.010081,validation loss = 0.014154\n",
      "epoch : 596/3000, training loss = 0.010036,validation loss = 0.014154\n",
      "epoch : 597/3000, training loss = 0.010048,validation loss = 0.014149\n",
      "epoch : 598/3000, training loss = 0.010089,validation loss = 0.014207\n",
      "epoch : 599/3000, training loss = 0.010089,validation loss = 0.014119\n",
      "epoch : 600/3000, training loss = 0.010103,validation loss = 0.014197\n",
      "epoch : 601/3000, training loss = 0.010145,validation loss = 0.014301\n",
      "epoch : 602/3000, training loss = 0.010155,validation loss = 0.014218\n",
      "epoch : 603/3000, training loss = 0.010167,validation loss = 0.014300\n",
      "epoch : 604/3000, training loss = 0.010142,validation loss = 0.014201\n",
      "epoch : 605/3000, training loss = 0.010083,validation loss = 0.014152\n",
      "epoch : 606/3000, training loss = 0.010028,validation loss = 0.014192\n",
      "epoch : 607/3000, training loss = 0.010037,validation loss = 0.014137\n",
      "epoch : 608/3000, training loss = 0.010085,validation loss = 0.014315\n",
      "epoch : 609/3000, training loss = 0.010076,validation loss = 0.014181\n",
      "epoch : 610/3000, training loss = 0.010048,validation loss = 0.014224\n",
      "epoch : 611/3000, training loss = 0.010070,validation loss = 0.014149\n",
      "epoch : 612/3000, training loss = 0.010063,validation loss = 0.014245\n",
      "epoch : 613/3000, training loss = 0.010104,validation loss = 0.014182\n",
      "epoch : 614/3000, training loss = 0.010156,validation loss = 0.014270\n",
      "epoch : 615/3000, training loss = 0.010120,validation loss = 0.014260\n",
      "epoch : 616/3000, training loss = 0.010075,validation loss = 0.014151\n",
      "epoch : 617/3000, training loss = 0.010114,validation loss = 0.014219\n",
      "epoch : 618/3000, training loss = 0.010149,validation loss = 0.014277\n",
      "epoch : 619/3000, training loss = 0.010088,validation loss = 0.014240\n",
      "epoch : 620/3000, training loss = 0.010091,validation loss = 0.014239\n",
      "epoch : 621/3000, training loss = 0.010129,validation loss = 0.014154\n",
      "epoch : 622/3000, training loss = 0.010132,validation loss = 0.014194\n",
      "epoch : 623/3000, training loss = 0.010120,validation loss = 0.014178\n",
      "epoch : 624/3000, training loss = 0.010162,validation loss = 0.014272\n",
      "epoch : 625/3000, training loss = 0.010140,validation loss = 0.014218\n",
      "epoch : 626/3000, training loss = 0.010178,validation loss = 0.014259\n",
      "epoch : 627/3000, training loss = 0.010192,validation loss = 0.014187\n",
      "epoch : 628/3000, training loss = 0.010169,validation loss = 0.014386\n",
      "epoch : 629/3000, training loss = 0.010196,validation loss = 0.014302\n",
      "epoch : 630/3000, training loss = 0.010376,validation loss = 0.014314\n",
      "epoch : 631/3000, training loss = 0.010511,validation loss = 0.014326\n",
      "epoch : 632/3000, training loss = 0.010463,validation loss = 0.014302\n",
      "epoch : 633/3000, training loss = 0.010313,validation loss = 0.014126\n",
      "epoch : 634/3000, training loss = 0.010137,validation loss = 0.014105\n",
      "epoch : 635/3000, training loss = 0.010025,validation loss = 0.014063\n",
      "epoch : 636/3000, training loss = 0.009941,validation loss = 0.014187\n",
      "epoch : 637/3000, training loss = 0.009983,validation loss = 0.014175\n",
      "epoch : 638/3000, training loss = 0.010062,validation loss = 0.014120\n",
      "epoch : 639/3000, training loss = 0.010078,validation loss = 0.014127\n",
      "epoch : 640/3000, training loss = 0.010100,validation loss = 0.014120\n",
      "epoch : 641/3000, training loss = 0.010003,validation loss = 0.014198\n",
      "epoch : 642/3000, training loss = 0.010042,validation loss = 0.014209\n",
      "epoch : 643/3000, training loss = 0.010021,validation loss = 0.014293\n",
      "epoch : 644/3000, training loss = 0.010009,validation loss = 0.014135\n",
      "epoch : 645/3000, training loss = 0.009995,validation loss = 0.014132\n",
      "epoch : 646/3000, training loss = 0.010075,validation loss = 0.014298\n",
      "epoch : 647/3000, training loss = 0.010081,validation loss = 0.014271\n",
      "epoch : 648/3000, training loss = 0.010024,validation loss = 0.014189\n",
      "epoch : 649/3000, training loss = 0.010010,validation loss = 0.014234\n",
      "epoch : 650/3000, training loss = 0.009988,validation loss = 0.014163\n",
      "epoch : 651/3000, training loss = 0.010044,validation loss = 0.014195\n",
      "epoch : 652/3000, training loss = 0.010031,validation loss = 0.014237\n",
      "epoch : 653/3000, training loss = 0.010144,validation loss = 0.014409\n",
      "epoch : 654/3000, training loss = 0.010073,validation loss = 0.014304\n",
      "epoch : 655/3000, training loss = 0.010125,validation loss = 0.014160\n",
      "epoch : 656/3000, training loss = 0.010029,validation loss = 0.014123\n",
      "epoch : 657/3000, training loss = 0.010026,validation loss = 0.014126\n",
      "epoch : 658/3000, training loss = 0.010027,validation loss = 0.014184\n",
      "epoch : 659/3000, training loss = 0.010009,validation loss = 0.014241\n",
      "epoch : 660/3000, training loss = 0.009986,validation loss = 0.014139\n",
      "epoch : 661/3000, training loss = 0.010090,validation loss = 0.014291\n",
      "epoch : 662/3000, training loss = 0.010072,validation loss = 0.014236\n",
      "epoch : 663/3000, training loss = 0.010078,validation loss = 0.014218\n",
      "epoch : 664/3000, training loss = 0.010077,validation loss = 0.014182\n",
      "epoch : 665/3000, training loss = 0.010091,validation loss = 0.014268\n",
      "epoch : 666/3000, training loss = 0.010091,validation loss = 0.014399\n",
      "epoch : 667/3000, training loss = 0.010104,validation loss = 0.014427\n",
      "epoch : 668/3000, training loss = 0.010077,validation loss = 0.014480\n",
      "epoch : 669/3000, training loss = 0.010086,validation loss = 0.014303\n",
      "epoch : 670/3000, training loss = 0.010025,validation loss = 0.014291\n",
      "epoch : 671/3000, training loss = 0.010024,validation loss = 0.014334\n",
      "epoch : 672/3000, training loss = 0.010044,validation loss = 0.014268\n",
      "epoch : 673/3000, training loss = 0.009984,validation loss = 0.014307\n",
      "epoch : 674/3000, training loss = 0.010027,validation loss = 0.014283\n",
      "epoch : 675/3000, training loss = 0.010063,validation loss = 0.014168\n",
      "epoch : 676/3000, training loss = 0.010020,validation loss = 0.014167\n",
      "epoch : 677/3000, training loss = 0.010046,validation loss = 0.014143\n",
      "epoch : 678/3000, training loss = 0.010087,validation loss = 0.014151\n",
      "epoch : 679/3000, training loss = 0.010092,validation loss = 0.014234\n",
      "epoch : 680/3000, training loss = 0.010102,validation loss = 0.014238\n",
      "epoch : 681/3000, training loss = 0.010063,validation loss = 0.014207\n",
      "epoch : 682/3000, training loss = 0.010099,validation loss = 0.014253\n",
      "epoch : 683/3000, training loss = 0.010093,validation loss = 0.014344\n",
      "epoch : 684/3000, training loss = 0.010131,validation loss = 0.014181\n",
      "epoch : 685/3000, training loss = 0.010102,validation loss = 0.014170\n",
      "epoch : 687/3000, training loss = 0.010014,validation loss = 0.014212\n",
      "epoch : 688/3000, training loss = 0.010000,validation loss = 0.014134\n",
      "epoch : 689/3000, training loss = 0.009950,validation loss = 0.014102\n",
      "epoch : 690/3000, training loss = 0.009967,validation loss = 0.014263\n",
      "epoch : 691/3000, training loss = 0.010026,validation loss = 0.014316\n",
      "epoch : 692/3000, training loss = 0.010052,validation loss = 0.014167\n",
      "epoch : 693/3000, training loss = 0.010016,validation loss = 0.014155\n",
      "epoch : 694/3000, training loss = 0.010011,validation loss = 0.014204\n",
      "epoch : 695/3000, training loss = 0.010035,validation loss = 0.014221\n",
      "epoch : 696/3000, training loss = 0.010077,validation loss = 0.014221\n",
      "epoch : 697/3000, training loss = 0.010119,validation loss = 0.014353\n",
      "epoch : 698/3000, training loss = 0.010132,validation loss = 0.014234\n",
      "epoch : 699/3000, training loss = 0.010101,validation loss = 0.014214\n",
      "epoch : 700/3000, training loss = 0.010043,validation loss = 0.014361\n",
      "epoch : 701/3000, training loss = 0.010038,validation loss = 0.014315\n",
      "epoch : 702/3000, training loss = 0.010104,validation loss = 0.014381\n",
      "epoch : 703/3000, training loss = 0.010093,validation loss = 0.014376\n",
      "epoch : 704/3000, training loss = 0.010070,validation loss = 0.014309\n",
      "epoch : 705/3000, training loss = 0.010045,validation loss = 0.014556\n",
      "epoch : 706/3000, training loss = 0.010038,validation loss = 0.014483\n",
      "epoch : 707/3000, training loss = 0.010055,validation loss = 0.014374\n",
      "epoch : 708/3000, training loss = 0.010011,validation loss = 0.014246\n",
      "epoch : 709/3000, training loss = 0.010059,validation loss = 0.014241\n",
      "epoch : 710/3000, training loss = 0.010090,validation loss = 0.014248\n",
      "epoch : 711/3000, training loss = 0.010061,validation loss = 0.014307\n",
      "epoch : 712/3000, training loss = 0.010055,validation loss = 0.014279\n",
      "epoch : 713/3000, training loss = 0.010078,validation loss = 0.014234\n",
      "epoch : 714/3000, training loss = 0.010081,validation loss = 0.014188\n",
      "epoch : 715/3000, training loss = 0.010115,validation loss = 0.014269\n",
      "epoch : 716/3000, training loss = 0.010123,validation loss = 0.014286\n",
      "epoch : 717/3000, training loss = 0.010133,validation loss = 0.014393\n",
      "epoch : 718/3000, training loss = 0.010102,validation loss = 0.014335\n",
      "epoch : 719/3000, training loss = 0.010068,validation loss = 0.014338\n",
      "epoch : 720/3000, training loss = 0.010046,validation loss = 0.014248\n",
      "epoch : 721/3000, training loss = 0.010026,validation loss = 0.014323\n",
      "epoch : 722/3000, training loss = 0.010016,validation loss = 0.014321\n",
      "epoch : 723/3000, training loss = 0.009982,validation loss = 0.014354\n",
      "epoch : 724/3000, training loss = 0.010025,validation loss = 0.014306\n",
      "epoch : 725/3000, training loss = 0.010033,validation loss = 0.014297\n",
      "epoch : 726/3000, training loss = 0.010059,validation loss = 0.014272\n",
      "epoch : 727/3000, training loss = 0.010014,validation loss = 0.014288\n",
      "epoch : 728/3000, training loss = 0.010018,validation loss = 0.014338\n",
      "epoch : 729/3000, training loss = 0.010016,validation loss = 0.014247\n",
      "epoch : 730/3000, training loss = 0.010020,validation loss = 0.014347\n",
      "epoch : 731/3000, training loss = 0.010022,validation loss = 0.014329\n",
      "epoch : 732/3000, training loss = 0.010064,validation loss = 0.014313\n",
      "epoch : 733/3000, training loss = 0.010060,validation loss = 0.014257\n",
      "epoch : 734/3000, training loss = 0.010036,validation loss = 0.014382\n",
      "epoch : 735/3000, training loss = 0.010059,validation loss = 0.014257\n",
      "epoch : 736/3000, training loss = 0.010054,validation loss = 0.014357\n",
      "epoch : 737/3000, training loss = 0.010066,validation loss = 0.014183\n",
      "epoch : 738/3000, training loss = 0.010135,validation loss = 0.014310\n",
      "epoch : 739/3000, training loss = 0.010200,validation loss = 0.014184\n",
      "epoch : 740/3000, training loss = 0.010178,validation loss = 0.014278\n",
      "epoch : 741/3000, training loss = 0.010104,validation loss = 0.014303\n",
      "epoch : 742/3000, training loss = 0.010025,validation loss = 0.014283\n",
      "epoch : 743/3000, training loss = 0.009981,validation loss = 0.014326\n",
      "epoch : 744/3000, training loss = 0.009966,validation loss = 0.014277\n",
      "epoch : 745/3000, training loss = 0.009990,validation loss = 0.014270\n",
      "epoch : 746/3000, training loss = 0.009995,validation loss = 0.014306\n",
      "epoch : 747/3000, training loss = 0.010039,validation loss = 0.014323\n",
      "epoch : 748/3000, training loss = 0.010008,validation loss = 0.014279\n",
      "epoch : 749/3000, training loss = 0.009986,validation loss = 0.014331\n",
      "epoch : 750/3000, training loss = 0.009951,validation loss = 0.014261\n",
      "epoch : 751/3000, training loss = 0.010013,validation loss = 0.014194\n",
      "epoch : 752/3000, training loss = 0.009968,validation loss = 0.014183\n",
      "epoch : 753/3000, training loss = 0.010001,validation loss = 0.014220\n",
      "epoch : 754/3000, training loss = 0.009972,validation loss = 0.014256\n",
      "epoch : 755/3000, training loss = 0.009996,validation loss = 0.014413\n",
      "epoch : 756/3000, training loss = 0.010051,validation loss = 0.014319\n",
      "epoch : 757/3000, training loss = 0.010013,validation loss = 0.014250\n",
      "epoch : 758/3000, training loss = 0.010029,validation loss = 0.014329\n",
      "epoch : 759/3000, training loss = 0.010010,validation loss = 0.014390\n",
      "epoch : 760/3000, training loss = 0.010024,validation loss = 0.014436\n",
      "epoch : 761/3000, training loss = 0.010104,validation loss = 0.014447\n",
      "epoch : 762/3000, training loss = 0.010093,validation loss = 0.014386\n",
      "epoch : 763/3000, training loss = 0.010099,validation loss = 0.014508\n",
      "epoch : 764/3000, training loss = 0.010071,validation loss = 0.014460\n",
      "epoch : 765/3000, training loss = 0.010048,validation loss = 0.014567\n",
      "epoch : 766/3000, training loss = 0.010031,validation loss = 0.014490\n",
      "epoch : 767/3000, training loss = 0.010032,validation loss = 0.014578\n",
      "epoch : 768/3000, training loss = 0.010020,validation loss = 0.014463\n",
      "epoch : 769/3000, training loss = 0.010056,validation loss = 0.014446\n",
      "epoch : 770/3000, training loss = 0.010051,validation loss = 0.014444\n",
      "epoch : 771/3000, training loss = 0.010028,validation loss = 0.014508\n",
      "epoch : 772/3000, training loss = 0.010009,validation loss = 0.014377\n",
      "epoch : 773/3000, training loss = 0.010028,validation loss = 0.014443\n",
      "epoch : 774/3000, training loss = 0.010018,validation loss = 0.014385\n",
      "epoch : 775/3000, training loss = 0.010016,validation loss = 0.014505\n",
      "epoch : 776/3000, training loss = 0.010057,validation loss = 0.014425\n",
      "epoch : 777/3000, training loss = 0.010053,validation loss = 0.014413\n",
      "epoch : 778/3000, training loss = 0.010072,validation loss = 0.014354\n",
      "epoch : 779/3000, training loss = 0.010113,validation loss = 0.014505\n",
      "epoch : 780/3000, training loss = 0.010156,validation loss = 0.014577\n",
      "epoch : 781/3000, training loss = 0.010228,validation loss = 0.014463\n",
      "epoch : 782/3000, training loss = 0.010207,validation loss = 0.014540\n",
      "epoch : 783/3000, training loss = 0.010209,validation loss = 0.014437\n",
      "epoch : 784/3000, training loss = 0.010188,validation loss = 0.014516\n",
      "epoch : 785/3000, training loss = 0.010123,validation loss = 0.014608\n",
      "epoch : 786/3000, training loss = 0.010057,validation loss = 0.014448\n",
      "epoch : 787/3000, training loss = 0.010016,validation loss = 0.014447\n",
      "epoch : 788/3000, training loss = 0.010026,validation loss = 0.014385\n",
      "epoch : 789/3000, training loss = 0.009984,validation loss = 0.014349\n",
      "epoch : 790/3000, training loss = 0.009966,validation loss = 0.014304\n",
      "epoch : 791/3000, training loss = 0.009979,validation loss = 0.014248\n",
      "epoch : 792/3000, training loss = 0.009979,validation loss = 0.014394\n",
      "epoch : 793/3000, training loss = 0.009987,validation loss = 0.014381\n",
      "epoch : 794/3000, training loss = 0.009995,validation loss = 0.014415\n",
      "epoch : 795/3000, training loss = 0.009993,validation loss = 0.014271\n",
      "epoch : 796/3000, training loss = 0.009959,validation loss = 0.014492\n",
      "epoch : 797/3000, training loss = 0.009964,validation loss = 0.014402\n",
      "epoch : 798/3000, training loss = 0.009958,validation loss = 0.014354\n",
      "epoch : 799/3000, training loss = 0.010006,validation loss = 0.014218\n",
      "epoch : 800/3000, training loss = 0.009944,validation loss = 0.014250\n",
      "epoch : 801/3000, training loss = 0.009902,validation loss = 0.014207\n",
      "epoch : 802/3000, training loss = 0.009924,validation loss = 0.014362\n",
      "epoch : 803/3000, training loss = 0.009943,validation loss = 0.014259\n",
      "epoch : 804/3000, training loss = 0.009907,validation loss = 0.014318\n",
      "epoch : 805/3000, training loss = 0.009934,validation loss = 0.014447\n",
      "epoch : 806/3000, training loss = 0.009977,validation loss = 0.014289\n",
      "epoch : 807/3000, training loss = 0.010026,validation loss = 0.014379\n",
      "epoch : 808/3000, training loss = 0.010120,validation loss = 0.014321\n",
      "epoch : 809/3000, training loss = 0.009972,validation loss = 0.014408\n",
      "epoch : 810/3000, training loss = 0.009887,validation loss = 0.014260\n",
      "epoch : 811/3000, training loss = 0.009854,validation loss = 0.014251\n",
      "epoch : 812/3000, training loss = 0.009895,validation loss = 0.014338\n",
      "epoch : 813/3000, training loss = 0.009936,validation loss = 0.014332\n",
      "epoch : 814/3000, training loss = 0.009958,validation loss = 0.014373\n",
      "epoch : 815/3000, training loss = 0.009957,validation loss = 0.014364\n",
      "epoch : 816/3000, training loss = 0.009936,validation loss = 0.014409\n",
      "epoch : 817/3000, training loss = 0.009898,validation loss = 0.014291\n",
      "epoch : 818/3000, training loss = 0.009917,validation loss = 0.014542\n",
      "epoch : 819/3000, training loss = 0.009956,validation loss = 0.014438\n",
      "epoch : 820/3000, training loss = 0.009928,validation loss = 0.014448\n",
      "epoch : 821/3000, training loss = 0.010022,validation loss = 0.014383\n",
      "epoch : 822/3000, training loss = 0.010078,validation loss = 0.014378\n",
      "epoch : 823/3000, training loss = 0.010009,validation loss = 0.014389\n",
      "epoch : 824/3000, training loss = 0.009940,validation loss = 0.014280\n",
      "epoch : 825/3000, training loss = 0.009930,validation loss = 0.014299\n",
      "epoch : 826/3000, training loss = 0.009880,validation loss = 0.014237\n",
      "epoch : 827/3000, training loss = 0.009890,validation loss = 0.014334\n",
      "epoch : 828/3000, training loss = 0.009918,validation loss = 0.014304\n",
      "epoch : 829/3000, training loss = 0.009936,validation loss = 0.014306\n",
      "epoch : 830/3000, training loss = 0.009949,validation loss = 0.014433\n",
      "epoch : 831/3000, training loss = 0.009914,validation loss = 0.014417\n",
      "epoch : 832/3000, training loss = 0.009939,validation loss = 0.014326\n",
      "epoch : 833/3000, training loss = 0.009941,validation loss = 0.014395\n",
      "epoch : 834/3000, training loss = 0.009922,validation loss = 0.014324\n",
      "epoch : 835/3000, training loss = 0.009956,validation loss = 0.014527\n",
      "epoch : 836/3000, training loss = 0.009961,validation loss = 0.014391\n",
      "epoch : 837/3000, training loss = 0.009947,validation loss = 0.014514\n",
      "epoch : 838/3000, training loss = 0.009973,validation loss = 0.014617\n",
      "epoch : 839/3000, training loss = 0.009996,validation loss = 0.014582\n",
      "epoch : 840/3000, training loss = 0.010012,validation loss = 0.014663\n",
      "epoch : 841/3000, training loss = 0.010059,validation loss = 0.014537\n",
      "epoch : 842/3000, training loss = 0.010055,validation loss = 0.014368\n",
      "epoch : 843/3000, training loss = 0.010036,validation loss = 0.014432\n",
      "epoch : 844/3000, training loss = 0.010030,validation loss = 0.014381\n",
      "epoch : 845/3000, training loss = 0.010044,validation loss = 0.014483\n",
      "epoch : 846/3000, training loss = 0.010027,validation loss = 0.014540\n",
      "epoch : 847/3000, training loss = 0.009976,validation loss = 0.014581\n",
      "epoch : 848/3000, training loss = 0.010019,validation loss = 0.014538\n",
      "epoch : 849/3000, training loss = 0.009955,validation loss = 0.014413\n",
      "epoch : 850/3000, training loss = 0.009913,validation loss = 0.014378\n",
      "epoch : 851/3000, training loss = 0.009928,validation loss = 0.014417\n",
      "epoch : 852/3000, training loss = 0.009945,validation loss = 0.014314\n",
      "epoch : 853/3000, training loss = 0.009957,validation loss = 0.014346\n",
      "epoch : 854/3000, training loss = 0.009910,validation loss = 0.014339\n",
      "epoch : 855/3000, training loss = 0.009935,validation loss = 0.014495\n",
      "epoch : 856/3000, training loss = 0.009973,validation loss = 0.014359\n",
      "epoch : 857/3000, training loss = 0.009970,validation loss = 0.014360\n",
      "epoch : 858/3000, training loss = 0.009929,validation loss = 0.014377\n",
      "epoch : 859/3000, training loss = 0.009927,validation loss = 0.014378\n",
      "epoch : 860/3000, training loss = 0.010058,validation loss = 0.014692\n",
      "epoch : 861/3000, training loss = 0.010093,validation loss = 0.014399\n",
      "epoch : 862/3000, training loss = 0.009965,validation loss = 0.014671\n",
      "epoch : 863/3000, training loss = 0.009921,validation loss = 0.014486\n",
      "epoch : 864/3000, training loss = 0.009895,validation loss = 0.014533\n",
      "epoch : 865/3000, training loss = 0.009902,validation loss = 0.014336\n",
      "epoch : 866/3000, training loss = 0.009916,validation loss = 0.014415\n",
      "epoch : 867/3000, training loss = 0.009895,validation loss = 0.014459\n",
      "epoch : 868/3000, training loss = 0.009898,validation loss = 0.014496\n",
      "epoch : 869/3000, training loss = 0.009960,validation loss = 0.014428\n",
      "epoch : 870/3000, training loss = 0.009918,validation loss = 0.014383\n",
      "epoch : 871/3000, training loss = 0.009906,validation loss = 0.014420\n",
      "epoch : 872/3000, training loss = 0.009880,validation loss = 0.014337\n",
      "epoch : 873/3000, training loss = 0.009931,validation loss = 0.014238\n",
      "epoch : 874/3000, training loss = 0.009887,validation loss = 0.014294\n",
      "epoch : 875/3000, training loss = 0.009923,validation loss = 0.014368\n",
      "epoch : 876/3000, training loss = 0.009882,validation loss = 0.014365\n",
      "epoch : 877/3000, training loss = 0.009942,validation loss = 0.014288\n",
      "epoch : 878/3000, training loss = 0.009926,validation loss = 0.014299\n",
      "epoch : 879/3000, training loss = 0.009917,validation loss = 0.014203\n",
      "epoch : 880/3000, training loss = 0.009978,validation loss = 0.014341\n",
      "epoch : 881/3000, training loss = 0.009981,validation loss = 0.014317\n",
      "epoch : 882/3000, training loss = 0.009988,validation loss = 0.014314\n",
      "epoch : 883/3000, training loss = 0.010036,validation loss = 0.014354\n",
      "epoch : 884/3000, training loss = 0.010036,validation loss = 0.014447\n",
      "epoch : 885/3000, training loss = 0.010056,validation loss = 0.014380\n",
      "epoch : 886/3000, training loss = 0.010003,validation loss = 0.014277\n",
      "epoch : 887/3000, training loss = 0.009996,validation loss = 0.014378\n",
      "epoch : 888/3000, training loss = 0.009977,validation loss = 0.014420\n",
      "epoch : 889/3000, training loss = 0.010001,validation loss = 0.014493\n",
      "epoch : 890/3000, training loss = 0.009993,validation loss = 0.014602\n",
      "epoch : 891/3000, training loss = 0.009961,validation loss = 0.014714\n",
      "epoch : 892/3000, training loss = 0.009963,validation loss = 0.014636\n",
      "epoch : 893/3000, training loss = 0.009924,validation loss = 0.014590\n",
      "epoch : 894/3000, training loss = 0.009884,validation loss = 0.014432\n",
      "epoch : 895/3000, training loss = 0.009899,validation loss = 0.014717\n",
      "epoch : 896/3000, training loss = 0.009928,validation loss = 0.014463\n",
      "epoch : 897/3000, training loss = 0.009909,validation loss = 0.014387\n",
      "epoch : 898/3000, training loss = 0.009891,validation loss = 0.014482\n",
      "epoch : 899/3000, training loss = 0.009913,validation loss = 0.014381\n",
      "epoch : 900/3000, training loss = 0.009920,validation loss = 0.014345\n",
      "epoch : 901/3000, training loss = 0.009832,validation loss = 0.014332\n",
      "epoch : 902/3000, training loss = 0.009858,validation loss = 0.014336\n",
      "epoch : 903/3000, training loss = 0.009884,validation loss = 0.014395\n",
      "epoch : 904/3000, training loss = 0.009872,validation loss = 0.014350\n",
      "epoch : 905/3000, training loss = 0.009877,validation loss = 0.014512\n",
      "epoch : 906/3000, training loss = 0.009910,validation loss = 0.014358\n",
      "epoch : 907/3000, training loss = 0.009906,validation loss = 0.014299\n",
      "epoch : 908/3000, training loss = 0.009931,validation loss = 0.014317\n",
      "epoch : 909/3000, training loss = 0.009974,validation loss = 0.014437\n",
      "epoch : 910/3000, training loss = 0.010004,validation loss = 0.014443\n",
      "epoch : 911/3000, training loss = 0.010011,validation loss = 0.014409\n",
      "epoch : 912/3000, training loss = 0.009996,validation loss = 0.014349\n",
      "epoch : 913/3000, training loss = 0.010038,validation loss = 0.014305\n",
      "epoch : 914/3000, training loss = 0.009990,validation loss = 0.014321\n",
      "epoch : 915/3000, training loss = 0.009947,validation loss = 0.014337\n",
      "epoch : 916/3000, training loss = 0.009881,validation loss = 0.014253\n",
      "epoch : 917/3000, training loss = 0.009825,validation loss = 0.014270\n",
      "epoch : 918/3000, training loss = 0.009892,validation loss = 0.014329\n",
      "epoch : 919/3000, training loss = 0.009940,validation loss = 0.014382\n",
      "epoch : 920/3000, training loss = 0.009886,validation loss = 0.014292\n",
      "epoch : 921/3000, training loss = 0.009839,validation loss = 0.014381\n",
      "epoch : 922/3000, training loss = 0.009822,validation loss = 0.014351\n",
      "epoch : 923/3000, training loss = 0.009886,validation loss = 0.014456\n",
      "epoch : 924/3000, training loss = 0.009873,validation loss = 0.014324\n",
      "epoch : 925/3000, training loss = 0.009870,validation loss = 0.014431\n",
      "epoch : 926/3000, training loss = 0.009849,validation loss = 0.014318\n",
      "epoch : 927/3000, training loss = 0.009863,validation loss = 0.014520\n",
      "epoch : 928/3000, training loss = 0.009923,validation loss = 0.014635\n",
      "epoch : 929/3000, training loss = 0.009940,validation loss = 0.014367\n",
      "epoch : 930/3000, training loss = 0.009903,validation loss = 0.014260\n",
      "epoch : 931/3000, training loss = 0.009861,validation loss = 0.014329\n",
      "epoch : 932/3000, training loss = 0.009880,validation loss = 0.014401\n",
      "epoch : 933/3000, training loss = 0.009943,validation loss = 0.014367\n",
      "epoch : 934/3000, training loss = 0.009969,validation loss = 0.014475\n",
      "epoch : 935/3000, training loss = 0.009987,validation loss = 0.014500\n",
      "epoch : 936/3000, training loss = 0.010006,validation loss = 0.014357\n",
      "epoch : 937/3000, training loss = 0.009985,validation loss = 0.014363\n",
      "epoch : 938/3000, training loss = 0.009967,validation loss = 0.014380\n",
      "epoch : 939/3000, training loss = 0.009873,validation loss = 0.014448\n",
      "epoch : 940/3000, training loss = 0.009836,validation loss = 0.014310\n",
      "epoch : 941/3000, training loss = 0.009879,validation loss = 0.014505\n",
      "epoch : 942/3000, training loss = 0.009870,validation loss = 0.014366\n",
      "epoch : 943/3000, training loss = 0.009917,validation loss = 0.014268\n",
      "epoch : 944/3000, training loss = 0.009827,validation loss = 0.014375\n",
      "epoch : 945/3000, training loss = 0.009813,validation loss = 0.014412\n",
      "epoch : 946/3000, training loss = 0.009812,validation loss = 0.014347\n",
      "epoch : 947/3000, training loss = 0.009906,validation loss = 0.014387\n",
      "epoch : 948/3000, training loss = 0.009868,validation loss = 0.014405\n",
      "epoch : 949/3000, training loss = 0.009879,validation loss = 0.014222\n",
      "epoch : 950/3000, training loss = 0.009834,validation loss = 0.014425\n",
      "epoch : 951/3000, training loss = 0.009831,validation loss = 0.014319\n",
      "epoch : 952/3000, training loss = 0.009874,validation loss = 0.014423\n",
      "epoch : 953/3000, training loss = 0.009878,validation loss = 0.014356\n",
      "epoch : 954/3000, training loss = 0.009843,validation loss = 0.014463\n",
      "epoch : 955/3000, training loss = 0.009880,validation loss = 0.014570\n",
      "epoch : 956/3000, training loss = 0.009813,validation loss = 0.014472\n",
      "epoch : 957/3000, training loss = 0.009828,validation loss = 0.014509\n",
      "epoch : 958/3000, training loss = 0.009861,validation loss = 0.014666\n",
      "epoch : 959/3000, training loss = 0.009881,validation loss = 0.014565\n",
      "epoch : 960/3000, training loss = 0.009949,validation loss = 0.014658\n",
      "epoch : 961/3000, training loss = 0.009919,validation loss = 0.014483\n",
      "epoch : 962/3000, training loss = 0.009868,validation loss = 0.014386\n",
      "epoch : 963/3000, training loss = 0.009883,validation loss = 0.014358\n",
      "epoch : 964/3000, training loss = 0.009888,validation loss = 0.014300\n",
      "epoch : 965/3000, training loss = 0.009924,validation loss = 0.014440\n",
      "epoch : 966/3000, training loss = 0.009872,validation loss = 0.014550\n",
      "epoch : 967/3000, training loss = 0.009854,validation loss = 0.014614\n",
      "epoch : 968/3000, training loss = 0.009870,validation loss = 0.014501\n",
      "epoch : 969/3000, training loss = 0.009907,validation loss = 0.014323\n",
      "epoch : 970/3000, training loss = 0.009921,validation loss = 0.014394\n",
      "epoch : 971/3000, training loss = 0.009875,validation loss = 0.014363\n",
      "epoch : 972/3000, training loss = 0.009932,validation loss = 0.014449\n",
      "epoch : 973/3000, training loss = 0.009986,validation loss = 0.014607\n",
      "epoch : 974/3000, training loss = 0.009952,validation loss = 0.014499\n",
      "epoch : 975/3000, training loss = 0.009965,validation loss = 0.014512\n",
      "epoch : 976/3000, training loss = 0.010013,validation loss = 0.014622\n",
      "epoch : 977/3000, training loss = 0.010009,validation loss = 0.014312\n",
      "epoch : 978/3000, training loss = 0.009963,validation loss = 0.014459\n",
      "epoch : 979/3000, training loss = 0.009919,validation loss = 0.014424\n",
      "epoch : 980/3000, training loss = 0.009899,validation loss = 0.014249\n",
      "epoch : 981/3000, training loss = 0.009821,validation loss = 0.014259\n",
      "epoch : 982/3000, training loss = 0.009839,validation loss = 0.014224\n",
      "epoch : 983/3000, training loss = 0.009829,validation loss = 0.014337\n",
      "epoch : 984/3000, training loss = 0.009835,validation loss = 0.014454\n",
      "epoch : 985/3000, training loss = 0.009859,validation loss = 0.014433\n",
      "epoch : 986/3000, training loss = 0.009827,validation loss = 0.014244\n",
      "epoch : 987/3000, training loss = 0.009842,validation loss = 0.014443\n",
      "epoch : 988/3000, training loss = 0.009843,validation loss = 0.014261\n",
      "epoch : 989/3000, training loss = 0.009836,validation loss = 0.014319\n",
      "epoch : 990/3000, training loss = 0.009821,validation loss = 0.014294\n",
      "epoch : 991/3000, training loss = 0.009858,validation loss = 0.014272\n",
      "epoch : 992/3000, training loss = 0.009891,validation loss = 0.014382\n",
      "epoch : 993/3000, training loss = 0.009814,validation loss = 0.014403\n",
      "epoch : 994/3000, training loss = 0.009831,validation loss = 0.014244\n",
      "epoch : 995/3000, training loss = 0.009890,validation loss = 0.014414\n",
      "epoch : 996/3000, training loss = 0.009864,validation loss = 0.014271\n",
      "epoch : 997/3000, training loss = 0.009850,validation loss = 0.014261\n",
      "epoch : 998/3000, training loss = 0.009865,validation loss = 0.014472\n",
      "epoch : 999/3000, training loss = 0.009893,validation loss = 0.014348\n",
      "epoch : 1000/3000, training loss = 0.009999,validation loss = 0.014463\n",
      "epoch : 1001/3000, training loss = 0.009973,validation loss = 0.014366\n",
      "epoch : 1002/3000, training loss = 0.009806,validation loss = 0.014360\n",
      "epoch : 1003/3000, training loss = 0.009762,validation loss = 0.014350\n",
      "epoch : 1004/3000, training loss = 0.009760,validation loss = 0.014352\n",
      "epoch : 1005/3000, training loss = 0.009810,validation loss = 0.014308\n",
      "epoch : 1006/3000, training loss = 0.009849,validation loss = 0.014478\n",
      "epoch : 1007/3000, training loss = 0.009897,validation loss = 0.014363\n",
      "epoch : 1008/3000, training loss = 0.009893,validation loss = 0.014374\n",
      "epoch : 1009/3000, training loss = 0.009868,validation loss = 0.014338\n",
      "epoch : 1010/3000, training loss = 0.009873,validation loss = 0.014414\n",
      "epoch : 1011/3000, training loss = 0.009834,validation loss = 0.014517\n",
      "epoch : 1012/3000, training loss = 0.009871,validation loss = 0.014402\n",
      "epoch : 1013/3000, training loss = 0.009885,validation loss = 0.014363\n",
      "epoch : 1014/3000, training loss = 0.010023,validation loss = 0.014485\n",
      "epoch : 1015/3000, training loss = 0.009991,validation loss = 0.014450\n",
      "epoch : 1016/3000, training loss = 0.009928,validation loss = 0.014437\n",
      "epoch : 1017/3000, training loss = 0.009884,validation loss = 0.014468\n",
      "epoch : 1018/3000, training loss = 0.009869,validation loss = 0.014383\n",
      "epoch : 1019/3000, training loss = 0.009864,validation loss = 0.014365\n",
      "epoch : 1020/3000, training loss = 0.009869,validation loss = 0.014503\n",
      "epoch : 1021/3000, training loss = 0.009814,validation loss = 0.014384\n",
      "epoch : 1022/3000, training loss = 0.009833,validation loss = 0.014445\n",
      "epoch : 1023/3000, training loss = 0.009882,validation loss = 0.014410\n",
      "epoch : 1024/3000, training loss = 0.009857,validation loss = 0.014299\n",
      "epoch : 1025/3000, training loss = 0.009834,validation loss = 0.014318\n",
      "epoch : 1026/3000, training loss = 0.009813,validation loss = 0.014373\n",
      "epoch : 1027/3000, training loss = 0.009853,validation loss = 0.014398\n",
      "epoch : 1028/3000, training loss = 0.009889,validation loss = 0.014437\n",
      "epoch : 1029/3000, training loss = 0.009925,validation loss = 0.014583\n",
      "epoch : 1030/3000, training loss = 0.009949,validation loss = 0.014423\n",
      "epoch : 1031/3000, training loss = 0.009935,validation loss = 0.014424\n",
      "epoch : 1032/3000, training loss = 0.009915,validation loss = 0.014440\n",
      "epoch : 1033/3000, training loss = 0.009921,validation loss = 0.014525\n",
      "epoch : 1034/3000, training loss = 0.009933,validation loss = 0.014509\n",
      "epoch : 1035/3000, training loss = 0.009961,validation loss = 0.014381\n",
      "epoch : 1036/3000, training loss = 0.009895,validation loss = 0.014458\n",
      "epoch : 1037/3000, training loss = 0.009911,validation loss = 0.014444\n",
      "epoch : 1038/3000, training loss = 0.009888,validation loss = 0.014499\n",
      "epoch : 1039/3000, training loss = 0.009875,validation loss = 0.014453\n",
      "epoch : 1040/3000, training loss = 0.009900,validation loss = 0.014456\n",
      "epoch : 1041/3000, training loss = 0.009901,validation loss = 0.014535\n",
      "epoch : 1042/3000, training loss = 0.009816,validation loss = 0.014436\n",
      "epoch : 1043/3000, training loss = 0.009855,validation loss = 0.014511\n",
      "epoch : 1044/3000, training loss = 0.009896,validation loss = 0.014474\n",
      "epoch : 1045/3000, training loss = 0.009906,validation loss = 0.014384\n",
      "epoch : 1046/3000, training loss = 0.009853,validation loss = 0.014502\n",
      "epoch : 1047/3000, training loss = 0.009822,validation loss = 0.014318\n",
      "epoch : 1048/3000, training loss = 0.009811,validation loss = 0.014335\n",
      "epoch : 1049/3000, training loss = 0.009820,validation loss = 0.014406\n",
      "epoch : 1050/3000, training loss = 0.009842,validation loss = 0.014369\n",
      "epoch : 1051/3000, training loss = 0.009916,validation loss = 0.014375\n",
      "epoch : 1052/3000, training loss = 0.009918,validation loss = 0.014435\n",
      "epoch : 1053/3000, training loss = 0.009879,validation loss = 0.014467\n",
      "epoch : 1054/3000, training loss = 0.009908,validation loss = 0.014515\n",
      "epoch : 1055/3000, training loss = 0.010016,validation loss = 0.014605\n",
      "epoch : 1056/3000, training loss = 0.009971,validation loss = 0.014796\n",
      "epoch : 1057/3000, training loss = 0.009935,validation loss = 0.014689\n",
      "epoch : 1058/3000, training loss = 0.009846,validation loss = 0.014648\n",
      "epoch : 1059/3000, training loss = 0.009788,validation loss = 0.014659\n",
      "epoch : 1060/3000, training loss = 0.009798,validation loss = 0.014585\n",
      "epoch : 1061/3000, training loss = 0.009884,validation loss = 0.014532\n",
      "epoch : 1062/3000, training loss = 0.009777,validation loss = 0.014626\n",
      "epoch : 1063/3000, training loss = 0.009808,validation loss = 0.014591\n",
      "epoch : 1064/3000, training loss = 0.009814,validation loss = 0.014676\n",
      "epoch : 1065/3000, training loss = 0.009862,validation loss = 0.014462\n",
      "epoch : 1066/3000, training loss = 0.009882,validation loss = 0.014484\n",
      "epoch : 1067/3000, training loss = 0.009841,validation loss = 0.014562\n",
      "epoch : 1068/3000, training loss = 0.009791,validation loss = 0.014332\n",
      "epoch : 1069/3000, training loss = 0.009819,validation loss = 0.014591\n",
      "epoch : 1070/3000, training loss = 0.009823,validation loss = 0.014404\n",
      "epoch : 1071/3000, training loss = 0.009834,validation loss = 0.014421\n",
      "epoch : 1072/3000, training loss = 0.009866,validation loss = 0.014512\n",
      "epoch : 1073/3000, training loss = 0.009844,validation loss = 0.014356\n",
      "epoch : 1074/3000, training loss = 0.009878,validation loss = 0.014514\n",
      "epoch : 1075/3000, training loss = 0.009832,validation loss = 0.014527\n",
      "epoch : 1076/3000, training loss = 0.009898,validation loss = 0.014476\n",
      "epoch : 1077/3000, training loss = 0.009853,validation loss = 0.014438\n",
      "epoch : 1078/3000, training loss = 0.009905,validation loss = 0.014470\n",
      "epoch : 1079/3000, training loss = 0.009907,validation loss = 0.014388\n",
      "epoch : 1080/3000, training loss = 0.009933,validation loss = 0.014458\n",
      "epoch : 1081/3000, training loss = 0.009981,validation loss = 0.014562\n",
      "epoch : 1082/3000, training loss = 0.010058,validation loss = 0.014480\n",
      "epoch : 1083/3000, training loss = 0.009883,validation loss = 0.014468\n",
      "epoch : 1084/3000, training loss = 0.009825,validation loss = 0.014386\n",
      "epoch : 1085/3000, training loss = 0.009797,validation loss = 0.014375\n",
      "epoch : 1086/3000, training loss = 0.009809,validation loss = 0.014478\n",
      "epoch : 1087/3000, training loss = 0.009864,validation loss = 0.014390\n",
      "epoch : 1088/3000, training loss = 0.009857,validation loss = 0.014436\n",
      "epoch : 1089/3000, training loss = 0.009803,validation loss = 0.014384\n",
      "epoch : 1090/3000, training loss = 0.009830,validation loss = 0.014380\n",
      "epoch : 1091/3000, training loss = 0.009867,validation loss = 0.014417\n",
      "epoch : 1092/3000, training loss = 0.009859,validation loss = 0.014544\n",
      "epoch : 1093/3000, training loss = 0.009875,validation loss = 0.014373\n",
      "epoch : 1094/3000, training loss = 0.009886,validation loss = 0.014386\n",
      "epoch : 1095/3000, training loss = 0.009850,validation loss = 0.014383\n",
      "epoch : 1096/3000, training loss = 0.009854,validation loss = 0.014467\n",
      "epoch : 1097/3000, training loss = 0.009864,validation loss = 0.014511\n",
      "epoch : 1098/3000, training loss = 0.009939,validation loss = 0.014486\n",
      "epoch : 1099/3000, training loss = 0.009922,validation loss = 0.014629\n",
      "epoch : 1100/3000, training loss = 0.009921,validation loss = 0.014701\n",
      "epoch : 1101/3000, training loss = 0.009903,validation loss = 0.014701\n",
      "epoch : 1102/3000, training loss = 0.009939,validation loss = 0.015008\n",
      "epoch : 1103/3000, training loss = 0.010013,validation loss = 0.014939\n",
      "epoch : 1104/3000, training loss = 0.010019,validation loss = 0.015128\n",
      "epoch : 1105/3000, training loss = 0.009980,validation loss = 0.014794\n",
      "epoch : 1106/3000, training loss = 0.009939,validation loss = 0.014516\n",
      "epoch : 1107/3000, training loss = 0.009862,validation loss = 0.014497\n",
      "epoch : 1108/3000, training loss = 0.009826,validation loss = 0.014609\n",
      "epoch : 1109/3000, training loss = 0.009814,validation loss = 0.014483\n",
      "epoch : 1110/3000, training loss = 0.009785,validation loss = 0.014517\n",
      "epoch : 1111/3000, training loss = 0.009800,validation loss = 0.014562\n",
      "epoch : 1112/3000, training loss = 0.009781,validation loss = 0.014560\n",
      "epoch : 1113/3000, training loss = 0.009817,validation loss = 0.014496\n",
      "epoch : 1114/3000, training loss = 0.009805,validation loss = 0.014505\n",
      "epoch : 1115/3000, training loss = 0.009781,validation loss = 0.014393\n",
      "epoch : 1116/3000, training loss = 0.009794,validation loss = 0.014537\n",
      "epoch : 1117/3000, training loss = 0.009803,validation loss = 0.014420\n",
      "epoch : 1118/3000, training loss = 0.009810,validation loss = 0.014396\n",
      "epoch : 1119/3000, training loss = 0.009875,validation loss = 0.014608\n",
      "epoch : 1120/3000, training loss = 0.009801,validation loss = 0.014617\n",
      "epoch : 1121/3000, training loss = 0.009719,validation loss = 0.014694\n",
      "epoch : 1122/3000, training loss = 0.009749,validation loss = 0.014648\n",
      "epoch : 1123/3000, training loss = 0.009825,validation loss = 0.014553\n",
      "epoch : 1124/3000, training loss = 0.009824,validation loss = 0.014346\n",
      "epoch : 1125/3000, training loss = 0.009812,validation loss = 0.014373\n",
      "epoch : 1126/3000, training loss = 0.009824,validation loss = 0.014522\n",
      "epoch : 1127/3000, training loss = 0.009843,validation loss = 0.014458\n",
      "epoch : 1128/3000, training loss = 0.009848,validation loss = 0.014475\n",
      "epoch : 1129/3000, training loss = 0.009845,validation loss = 0.014477\n",
      "epoch : 1130/3000, training loss = 0.009848,validation loss = 0.014515\n",
      "epoch : 1131/3000, training loss = 0.009948,validation loss = 0.014529\n",
      "epoch : 1132/3000, training loss = 0.009808,validation loss = 0.014438\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spectral_radiuses=[]\n",
    "average_jacobian=[]\n",
    "xinfinity=[]\n",
    "xpinfinity=[]\n",
    "training_loss=[]\n",
    "validation_loss=[]\n",
    "asymptotic_dist=[]\n",
    "#diverge=[]\n",
    "start=time.time()\n",
    "print(\"running\")\n",
    "#strength=0.1\n",
    "init_compute=True\n",
    "chao=[]\n",
    "noise_print=False\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "\n",
    "\n",
    "    for batch_features, _ in train_loader:\n",
    "        # reshape mini-batch data to [1000, 784] matrix\n",
    "        # load it to the active device\n",
    "        input_image =add_gaussian_noise(batch_features,noise_strength).view(batch_size, -1).to(device)\n",
    "        \n",
    "        \n",
    "        batch_feature=batch_features.view(batch_size,-1)\n",
    "        #input_image =add_powerlaw_noise(input_size,batch_size,noise_strength,batch_feature).view(batch_size, -1).to(device)\n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    " \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        #outputs = model(batch_features.view(batch_size,-1).to(device))\n",
    "        outputs = model(input_image)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        \n",
    " \n",
    "        train_loss = criterion(outputs,batch_feature.to(device))\n",
    "        \n",
    "        if init_compute==True and epoch%interval==0:\n",
    "            for layer in model.children():\n",
    "                if isinstance(layer, Linear):\n",
    "                    print(layer.state_dict()['weight'])\n",
    "                    print(layer.state_dict()['bias'])\n",
    "            init_compute=False\n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "        torch.save(model.state_dict(), \"epoch:\"+str(epoch+1)+\".pt\")\n",
    "        \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    training_loss.append(loss)\n",
    "\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        accuracy=validation(test_loader)\n",
    "        validation_loss.append(accuracy)\n",
    "        \n",
    "        \n",
    "    print(\"epoch : {}/{}, training loss = {:.6f},validation loss = {:.6f}\".format(epoch+1 , epochs, loss,accuracy))\n",
    "\n",
    "    \n",
    "end=time.time()\n",
    "print(end-start)\n",
    "\n",
    "print(chao)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d87237",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=\"noise:{:F} \\n perturbation:{:F}\".format(noise_strength,perturbation_strength)\n",
    "\n",
    "print(chao)\n",
    "font = {\n",
    "        'size'   : 14}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "intervals=50\n",
    "x=np.arange(1,epochs/intervals+1)\n",
    "print(len(x))\n",
    "print(len(chao))\n",
    "plt.scatter(x,y=chao,label=labels)\n",
    "plt.ylabel(\"fraction of images in chaos\")\n",
    "plt.xlabel(\"epoch/\"+str(interval))\n",
    "#plt.set_label(\"noise:{:.f},perturbation:{:.f}\".format(noise_strength,perturbation_strength))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"percentage of image in chaos.jpg\")\n",
    "\n",
    "np.savetxt(\"percentage of chaos.txt\",chao)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"asymptotic distance\")\n",
    "\n",
    "plt.scatter(x,y=asymptotic_dist,label=labels)\n",
    "#labels=\"noise:\"+str(noise_strength)+\", perturbation:\"+str(perturbation_strength)\n",
    "plt.ylabel(\"asymptotic distance\")\n",
    "plt.xlabel(\"epoch/\"+str(interval))\n",
    "plt.yscale(\"log\")\n",
    "#plt.set_label(\"noise:{:.f},perturbation:{:.f}\".format(noise_strength,perturbation_strength))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"asymptotic distance.jpg\",)\n",
    "\n",
    "plt.show()\n",
    "np.savetxt(\"asymptotic distance.txt\",asymptotic_dist)\n",
    "average_jacobian=np.asarray(average_jacobian)\n",
    "\n",
    "#print(\"average jacobian\")\n",
    "#plt.scatter(x,y=average_jacobian,label=\"noise:{:F}\\n perturbation:{:F}\".format(noise_strength,perturbation_strength))\n",
    "#plt.ylabel(\"average jacobian\")\n",
    "#plt.xlabel(\"epoch/\"+str(interval))\n",
    "#plt.yscale(\"log\")\n",
    "#plt.set_label(\"noise:{:.f},perturbation:{:.f}\".format(noise_strength,perturbation_strength))\n",
    "#plt.legend(loc=\"best\")\n",
    "#plt.savefig(\"average_jacobian.jpg\")\n",
    "#np.savetxt(\"average jacobian.txt\",average_jacobian)\n",
    "#plt.show()\n",
    "\n",
    "#print(\"spectral radius\")\n",
    "#plt.scatter(x,y=spectral_radiuses,label=\"noise:{:F},perturbation:{:F}\".format(noise_strength,perturbation_strength))\n",
    "#plt.ylabel(\"spectral radius\")\n",
    "#plt.xlabel(\"epoch/20\")\n",
    "#plt.yscale(\"log\")\n",
    "#plt.set_label(\"noise:{:.f},perturbation:{:.f}\".format(noise_strength,perturbation_strength))\n",
    "#plt.legend(loc=\"best\")\n",
    "#plt.savefig(\"spectral radius.jpg\")\n",
    "\n",
    "np.savetxt(\"spectral radius.txt\",spectral_radiuses)\n",
    "plt.show()\n",
    "\n",
    "print(\"loss function\")\n",
    "plt.plot(validation_loss,label=\"validation loss\")\n",
    "plt.plot(training_loss,label=\"training loss\")\n",
    "plt.ylabel(\"losss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "#plt.set_label(\"noise:{:.f},perturbation:{:.f}\".format(noise_strength,perturbation_strength))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.yscale(\"log\")\n",
    "plt.savefig(\"loss function.jpg\")\n",
    "plt.show()\n",
    "#print(\"asymptotic distance\")\n",
    "plot_loss=[]\n",
    "for i in range(len(validation_loss)):\n",
    "    if i%intervals==0:\n",
    "        plot_loss.append(validation_loss[i])\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('epoch/'+str(intervals))\n",
    "ax1.set_ylabel('validation loss', color=color)\n",
    "ax1.scatter(x,y=plot_loss, label=\"validation loss\",color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_yscale(\"log\")\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('asymptotic distance', color=color)  # we already handled the x-label with ax1\n",
    "ax2.scatter(x,y=asymptotic_dist, color=color,label=\"asymptotic distance\")\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_yscale(\"log\")\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()\n",
    "plt.scatter(x,plot_loss,label=\"validation loss\")\n",
    "        \n",
    "        \n",
    "        \n",
    "#plt.plot(x,plot_loss,label=\"validation loss\")\n",
    "plt.scatter(x,y=asymptotic_dist,label=\"asymptotic distance\")\n",
    "\n",
    "#labels=\"noise:\"+str(noise_strength)+\", perturbation:\"+str(perturbation_strength)\n",
    "plt.ylabel(\"asymptotic distance\")\n",
    "plt.xlabel(\"epoch/\"+str(interval))\n",
    "plt.yscale(\"log\")\n",
    "#plt.set_label(\"noise:{:.f},perturbation:{:.f}\".format(noise_strength,perturbation_strength))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.savefig(\"asymptotic distance.jpg\")\n",
    "\n",
    "plt.show()\n",
    "np.savetxt(\"loss_function.txt\",(validation_loss,training_loss))\n",
    "plt.show()\n",
    "\n",
    "plt.plot(validation_loss)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "plt.plot(training_loss)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
