{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e3577bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "702"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from typing import Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter, UninitializedParameter\n",
    "from torch import functional as F\n",
    "\n",
    "from torch.nn import Module\n",
    "from scipy.stats import levy_stable\n",
    "from torch.distributions.pareto import Pareto\n",
    "from torch.nn import init\n",
    "from torch.distributions.cauchy import Cauchy\n",
    "from torch.distributions.laplace import Laplace\n",
    "from numpy.random import standard_t\n",
    "seed=0\n",
    "#torch.cuda.manual_seed_all(seed)\n",
    "with torch.cuda.device('cuda:0'):\n",
    "    torch.cuda.empty_cache()\n",
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6998f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.71785524e+00 1.85446911e+03 8.28634792e-02 ... 3.28967344e-01\n",
      " 3.56443493e-01 1.45889748e+00]\n",
      "[8.58927618e-01 9.27234557e+02 4.14317396e-02 ... 1.64483672e-01\n",
      " 1.78221746e-01 7.29448740e-01]\n",
      "2000\n",
      "-15272948.10661609\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "a=0.5\n",
    "T=1\n",
    "epsilon=1\n",
    "m=T*epsilon\n",
    "s=(np.random.pareto(a, 2000)/2)\n",
    "print(s)\n",
    "print(s/2)\n",
    "#np.append()\n",
    "s=np.append(np.random.pareto(a, 2000)[:1000],(-1*np.random.pareto(a,2000)[:1000]))\n",
    "print(s.size)\n",
    "print(s.min())\n",
    "# shape and mode\n",
    "#s = (np.random.pareto(a, 1000) + 1) * m\n",
    "plt.hist(s,bins=1000,density=True)\n",
    "#plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9bfd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(tensor):\n",
    "    # Calculate the mean\n",
    "    mean = torch.mean(tensor)\n",
    "    # Calculate the variance\n",
    "    second = torch.mean(torch.pow(tensor,2))\n",
    "    third = torch.mean(torch.pow(tensor,3))\n",
    "    fourth=torch.mean(torch.pow(tensor,4))\n",
    "    # Calculate the standard deviation\n",
    "    var=torch.var(tensor)\n",
    "    std = torch.sqrt(var)\n",
    "    # Calculate the z-scores\n",
    "    zscores = (tensor - mean) / std\n",
    "    # Calculate the skewness\n",
    "    skew = torch.mean(zscores ** 3)\n",
    "    # Calculate the kurtosis\n",
    "    kurt = torch.mean(zscores ** 4) - 3\n",
    "    #print(\"fourth moment\")\n",
    "    #print(fourth)\n",
    "    #Return the results as a tuple\n",
    "    return mean, second, skew, kurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "976ab8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "weight_tuning=1\n",
    "J0=0\n",
    "J1=1\n",
    "print(J1)\n",
    "## initial =1\n",
    "class Linear(Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
    "\n",
    "    On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(*, H_{in})` where :math:`*` means any number of\n",
    "          dimensions including none and :math:`H_{in} = \\text{in\\_features}`.\n",
    "        - Output: :math:`(*, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
    "\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape\n",
    "            :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
    "            initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
    "            :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "        bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
    "                If :attr:`bias` is ``True``, the values are initialized from\n",
    "                :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
    "                :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> m = nn.Linear(20, 30)\n",
    "        >>> input = torch.randn(128, 20)\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "        torch.Size([128, 30])\n",
    "    \"\"\"\n",
    "    __constants__ = ['in_features', 'out_features']\n",
    "    in_features: int\n",
    "    out_features: int\n",
    "    weight: Tensor\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int,mu, bias: bool = True,\n",
    "                 device=None, dtype=None,J0=0,J1=10**-3) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super().__init__()\n",
    "        self.mu=mu\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.J0=J0\n",
    "        self.J1=J1\n",
    "        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        #if reset=True:\n",
    "            \n",
    "        # Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\n",
    "        # uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\n",
    "        # https://github.com/pytorch/pytorch/issues/57109\n",
    "        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
    "        bound =weight_tuning/ math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "        #init.uniform_(self.weight, -bound,bound)\n",
    "        #m=standard_t(self.mu,(self.in_features,self.out_features))\n",
    "        value=self.out_features*self.in_features\n",
    "        #print(int(value/2))\n",
    "        m=levy_stable.rvs(alpha=self.mu,loc=self.J0,scale=self.J1,beta=0, size=value)\n",
    "        \n",
    "        #m = levy_stable.rvs(alpha=0.5,beta=0,size=self.out_features*self.in_features)\n",
    "        \n",
    "        \n",
    "        a=torch.Tensor(m).resize(self.out_features,self.in_features)\n",
    "        #a=m.sample(torch.Size([self.in_features*self.out_features]))[:,0]\n",
    "        #a=a.resize(self.out_features,self.in_features)\n",
    "        self.weight=torch.nn.Parameter(a)\n",
    "        #init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.bias)\n",
    "            bound =weight_tuning/ math.sqrt(fan_in) if fan_in > 0 else 0\n",
    "            init.uniform_(self.bias, -bound, bound)\n",
    "            #init.uniform_(self.weight, -bound,bound)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "            self.in_features, self.out_features, self.bias is not None\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "28bad148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "#torch.cuda.is_available()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#from torch.autograd.functional import jacobian\n",
    "#import torch.autograd.functional\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "torch.set_default_dtype(torch.float64)\n",
    "batch_size=100\n",
    "input_size=784\n",
    "output_size=784\n",
    "hidden_state_size=10\n",
    "N=28\n",
    "no_of_layer=2\n",
    "\n",
    "\n",
    "\n",
    "font = {\n",
    "        'size'   : 26}\n",
    "plt.rc('font', **font)\n",
    "\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, input_shape,hidden_layer_shape,encoder_output_shape,n,mu,xinit=torch.eye(batch_size,input_size),xfinal=torch.eye(batch_size,input_size),hidden=torch.eye(10,input_size),J0=0,J1=10**-3):\n",
    "        super().__init__()\n",
    "        self.mu=mu\n",
    "        self.J0=J0\n",
    "        self.J1=J1\n",
    "        self.encoder_output_layer = Linear(\n",
    "            in_features=input_shape, out_features=encoder_output_shape\n",
    "        ,mu=self.mu,bias=False,J0=self.J0,J1=J1)\n",
    "        self.decoder_output_layer = Linear(\n",
    "            in_features=encoder_output_shape, out_features=input_shape\n",
    "        ,mu=self.mu,bias=False,J0=J0,J1=J1)\n",
    "\n",
    "        #self.encoder_input_layer.weight=torch.nn.Parameter(torch.rand(self.encoder_input_layer.weight.size))\n",
    "        \n",
    "        self.number_of_layers=n\n",
    "        self.xinit=xinit\n",
    "        self.xfinal=xfinal\n",
    "        self.hidden=hidden\n",
    "\n",
    "    def forward(self, features):\n",
    "        reconstructed=self.internals(features)\n",
    "        return reconstructed\n",
    "    \n",
    "    def internals(self,features):\n",
    "        code=self.encoder(features)\n",
    "        self.hidden=code\n",
    "        #print(\"hidden state:\"+str(code))\n",
    "        reconstructed=self.decoder(code)\n",
    "        return reconstructed\n",
    "    \n",
    "    def encoder(self,features):\n",
    "\n",
    "\n",
    "        code = self.encoder_output_layer(features)\n",
    "        result = torch.tanh(code)\n",
    "        return code\n",
    "    \n",
    "    def decoder(self,code):  \n",
    "        activation = self.decoder_output_layer(code)\n",
    "        reconstructed = torch.tanh(activation)\n",
    "        return reconstructed\n",
    "    \n",
    "    def xfinals(self):\n",
    "        return self.xfinal\n",
    "    \n",
    "    def xinits(self):\n",
    "        return self.xinit\n",
    "    \n",
    "    def hiddens(self):\n",
    "        return self.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f402f504",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#transform=\n",
    "train_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, transform=\n",
    "                                                transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "test_set = torchvision.datasets.FashionMNIST(\"./data\", download=True, train=False, transform=\n",
    "                                               transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "]))  \n",
    "train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                           batch_size=batch_size,shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set,\n",
    "                                          batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a3545a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# create a model from `AE` autoencoder class\n",
    "# load it to the specified device, either gpu or cpu\n",
    "mu=1\n",
    "#model = AE(input_size,output_size,hidden_state_size,no_of_layer,mu).to(device)\n",
    "\n",
    "# create an optimizer object\n",
    "# Adam optimizer with learning rate 1e-3\n",
    "\n",
    "#optimizer =torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# mean-squared error loss\n",
    "#criterion = nn.MSELoss()\n",
    "\n",
    "#print(model.encoder_input_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b9a5f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "epochs=1000\n",
    "asymptotic_repetition=150\n",
    "\n",
    "\n",
    "#projected_jacobian=[]\n",
    "ave_len=5\n",
    "\n",
    "\n",
    "no_of_images,no_of_repetition=20,50\n",
    "spectral_calculation=5\n",
    "colour=np.arange(1,no_of_repetition+1)\n",
    "perturbation_strength=10**(-6)\n",
    "noise_strength=10**(-3)\n",
    "interval=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "50453823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(inputs,strength):\n",
    "    #print(inputs.size())\n",
    "    noise = torch.randn_like(inputs)*strength*torch.max(inputs)\n",
    "    result=inputs + noise\n",
    "    return result\n",
    "def add_powerlaw_noise(input_size,batch_size,strength,images):\n",
    "    ##input size here is the side of the image 28 \n",
    "    m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))\n",
    "    res=m.sample(images.size())\n",
    "\n",
    "    temp=res/np.sqrt(res.var())/N/batch_size*strength\n",
    "    noise=temp-temp.mean()\n",
    "    #if print_bool:\n",
    "     #   plt.title(powerlaw_noise print)\n",
    "      #  plt.hist(noise.flatten(),density=True, bins='auto', histtype='stepfilled')\n",
    "        \n",
    "      #  plt.show()\n",
    "    #print(noise[:,:,:,:,0].size())\n",
    "    return noise[:,:,0]+images\n",
    "\n",
    "def validation(test_loader):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "     #valid_loss = criterion(outputs, batch_features.view(batch_size,-1).to(device))\n",
    "        loss=0\n",
    "        for batch_features, _ in test_loader:\n",
    "            #input=add_noise(batch_features,).view(100, -1).to(device)\n",
    "            input_image=add_gaussian_noise(batch_features,noise_strength).view(batch_size, -1).to(device)\n",
    "            #batch_feature = batch_features.view(batch_size, -1)\n",
    "            #input_image =add_powerlaw_noise(input_size,batch_size,noise_strength,batch_feature).view(batch_size, -1).to(device)\n",
    "            batch_feature = batch_features.view(batch_size, -1).to(device)\n",
    "            outputs = model(input_image)\n",
    "            valid_loss = criterion(outputs, input_image.to(device)) \n",
    "            #accuracy+=kl_div(outputs,batch_features)/batch_size\n",
    "            loss += valid_loss.item()\n",
    "    return loss/len(test_loader)\n",
    "            \n",
    "def kl_div(output_image,input_image):\n",
    "    accuracy=0\n",
    "    kl_loss = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "    for i in range(len(input_image)):\n",
    "        input_spectrum=torch.histogram(input_image[i].cpu(), bins=256, density=True)\n",
    "        output_spectrum=torch.histogram(output_image[i].cpu(), bins=256, density=True)\n",
    "        accuracy+=kl_loss(input_spectrum[i],output_spectrum[i])\n",
    "    return accuracy/len(input_image)\n",
    "\n",
    "def iteration(model,initial_inputs,no_of_repetition,epoch):\n",
    "    y_pred=initial_inputs.to(device)\n",
    "    for i in range(no_of_repetition):\n",
    "        x_train=y_pred\n",
    "        #if i==0:\n",
    "           # name=\"progress asymptotic iteration:\"+str(i)+\" \"\n",
    "           # visualisation(x_train[0].cpu().detach(),epoch,name,False)\n",
    "        #if i%5==1:\n",
    "            #name=\"progress asymptotic iteration:\"+str(i)+\" \"\n",
    "            #visualisation(x_train[0].cpu().detach(),epoch,name,False)\n",
    "        y_pred=model(x_train)\n",
    "    return x_train,y_pred,model \n",
    "def asymptotic_jacobian(model,initial_input,no_of_images,no_of_repetition):\n",
    "    ave_jac=1\n",
    "    overall_distribution=np.asarray([])\n",
    "    y_pred=initial_input.to(device)\n",
    "    jacobian=[]\n",
    "    sorteds=np.asarray([])\n",
    "    for j in range(no_of_repetition):\n",
    "        #print(\"here\")\n",
    "        x_train=y_pred\n",
    "        \n",
    "        y_pred=model(x_train)\n",
    "        mean_jacobian=torch.eye(input_size,output_size).to(device)\n",
    "        \n",
    "        for i in range(no_of_images):\n",
    "            x=x_train[i]\n",
    "            res=torch.autograd.functional.jacobian(model,x)\n",
    "            distribution=np.asarray([])\n",
    "            jacobian.append(res)\n",
    "            sorted=np.asarray([])\n",
    "            #if j==no_of_repetition-1\n",
    "            mean_jacobian*=res\n",
    "             #   distribution,sorted=spectral_distribution(res.cpu())\n",
    "              #  overall_distribution=np.concatenate((overall_distribution,distribution),axis=0)\n",
    "              #  spectral=spectral_radius(sorted)\n",
    "              #  sorteds=np.append(sorteds,spectral)\n",
    "\n",
    "        norm=torch.norm(mean_jacobian).cpu()\n",
    "        #mean_jacobian*=(1/np.sqrt(output_size)*norm)**(1/no_of_images)\n",
    "        #mean_jacobian=mean_jacobian/no_of_images\n",
    "        ave_jac*=(1/np.sqrt(output_size)*norm)**(1/no_of_repetition*1/no_of_images)\n",
    "        print(ave_jac)\n",
    "        print(norm)\n",
    "        print(norm**(1/no_of_repetition*1/no_of_images))\n",
    "    #average_jacobian=ave_jac/no_of_repetition\n",
    "    print(jacobian)\n",
    "    return ave_jac,overall_distribution,jacobian,sorteds\n",
    "\n",
    "def spectral_radius(jacobian,no_of_repetition,no_of_images):\n",
    "    #result=[]\n",
    "   \n",
    "    #output=initial_input.to(device)\n",
    "   # product=torch.eye(n=input_size,m=output_size).to(device)\n",
    "    \n",
    "    spectral=0\n",
    "\n",
    "    \n",
    "        \n",
    "    #res=torch.autograd.functional.jacobian(model.internals,initial_input)\n",
    "    #product=torch.matmul(product,res)\n",
    "    #output=model(initial_input)\n",
    "   # initial_input=output\n",
    "    \n",
    "    s,v=torch.linalg.eig(jacobian)\n",
    " #   #print(s)\n",
    "    abs=torch.abs(s)\n",
    "    \n",
    "    spectral=torch.max(abs).item()\n",
    "    return spectral\n",
    "\n",
    "def poincare_plot(model,initial_input,dimension_vector,no_of_repetition,colour,epoch):\n",
    "    xt=[]\n",
    "    xtminus=[]\n",
    "    \n",
    "    output=initial_input\n",
    "    \n",
    "    for i in range(no_of_repetition):\n",
    "        \n",
    "        initial_input=output\n",
    "        output=model(initial_input)\n",
    "        \n",
    "        xt.append(1/output_size*torch.dot(output,dimension_vector).item())\n",
    "        xtminus.append(1/output_size*torch.dot(initial_input,dimension_vector).item())\n",
    "    \n",
    "    return xt,xtminus\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "def asymptotic_distance(xinfinity_unperturbed,xinfinity_perturbed,perturbation):\n",
    "    result=[]\n",
    "    for i in range(len(xinfinity_unperturbed)):\n",
    "    \n",
    "        sum=0\n",
    "        \n",
    "        for j in range(len(xinfinity_unperturbed[i])):\n",
    "            \n",
    "            temp=np.linalg.norm(xinfinity_unperturbed[i][j]-xinfinity_perturbed[i][j])\n",
    "            sum+=temp\n",
    "        \n",
    "        result.append(1/output_size*1/len(xinfinity_unperturbed[i])*sum) \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def calculation(model,no_of_images,no_of_repetition,ave_jacobian,spectral_radiuses,image,epoch,mu,J0,J1):\n",
    "    spectral=0\n",
    "    \n",
    "    model_clone=AE(input_size,input_size,hidden_state_size,no_of_layer,mu,J0=J0,J1=J1).to(device)\n",
    "    model_clone.load_state_dict(copy.deepcopy(model.state_dict()))\n",
    "    distributions=np.asarray([])\n",
    "    x_train,y_pred,model_clone=iteration(model_clone,image,asymptotic_repetition,epoch)   \n",
    "    #ave_jac,distributions,jacobian,sorteds= asymptotic_jacobian(model_clone,x_train,no_of_images,no_of_repetition)\n",
    "    ave_jac=[]\n",
    "    #np.savetxt(\"jacobian epoch\"+str(epoch)+\".txt\",sorteds)\n",
    "    #print(\"distribution size\")\n",
    "    #print(distributions.size)\n",
    "    #print(\"sorted size\")\n",
    "    #print(sorteds.size)\n",
    "    #average_jacobian.append(ave_jac)\n",
    "    #x = [ele.real for ele in distributions]\n",
    "    ## extract imaginary part\n",
    "    #y = [ele.imag for ele in distributions]\n",
    "    #plt.title(\"real and imaginary part of eigenvalue\")\n",
    "    #plt.scatter(x, y)\n",
    "    #plt.ylabel('Imaginary')\n",
    "    #plt.xlabel('Real')\n",
    "    #plt.xscale(\"log\")\n",
    "    #plt.yscale(\"log\")\n",
    "    #plt.xlim(-1,1)\n",
    "    #plt.ylim(-1,1)\n",
    "    #plt.savefig(\"epoch:\"+str(epoch+1)+\"number of iteration:\"+str(no_of_repetition)+\"eigenvalue scatter plot.jpg\",bbox_inches = 'tight')\n",
    "    #plt.show()\n",
    "    #plt.title(\"modulus of eigenvalue in log log plot \")\n",
    "    #plt.hist(sorteds, density=True, bins='auto', histtype='stepfilled')\n",
    "    #plt.yscale(\"log\")\n",
    "    #plt.xscale(\"log\")\n",
    "\n",
    "    #plt.savefig(\"epoch:\"+str(epoch+1)+\"number of iteration:\"+str(no_of_repetition)+\"eigenvalue distribution.jpg\",bbox_inches = 'tight')\n",
    "    #plt.show()\n",
    "\n",
    "   # for i in range(no_of_images):\n",
    "    \n",
    "        #spectral+=spectral_radius(sorteds,no_of_repetition,no_of_images)\n",
    "    #spectral=sorteds.mean()\n",
    "    \n",
    "    #spectral_radiuses.append(spectral)\n",
    "    #print(spectral/no_of_images)\n",
    "    \n",
    "    return x_train,y_pred,average_jacobian,spectral_radiuses,model_clone\n",
    "\n",
    "\n",
    "def asymptotic_iteration(model_new,initial_inputs,perturbed_inputs,no_of_repetition,no_of_image,cutoff):\n",
    "    \n",
    "    y_pred_unperturbed=initial_inputs.to(device)\n",
    "    y_pred_perturbed=perturbed_inputs.to(device)\n",
    "\n",
    "    distance=[]\n",
    "    hidden_layer=[]\n",
    "    hiddens=[]\n",
    "    chaos=0\n",
    "    ave_jac=[]\n",
    "    #ave_jac,distributions,jacobian,sorteds= asymptotic_jacobian(model_new,y_pred_unperturbed,no_of_images,no_of_repetition)\n",
    "    for i in range(no_of_repetition):\n",
    "        x_train_unperturbed=y_pred_unperturbed\n",
    "        y_pred_unperturbed=model_new(x_train_unperturbed)\n",
    "        \n",
    "       \n",
    "        x_train_perturbed=y_pred_perturbed\n",
    "        y_pred_perturbed=model_new(x_train_perturbed)\n",
    "        \n",
    "\n",
    "        \n",
    "        hidden = model.hiddens()\n",
    "\n",
    "        hidden_layer.append(torch.sum(hidden,dim=1)[0].cpu().detach().numpy()/10)\n",
    "        hiddens.append(torch.sum(hidden,dim=1).cpu().detach().numpy()/10)\n",
    "        \n",
    "        #visualisation(y_pred.view(batch_size,-1)[0].cpu().detach(),epoch)\n",
    "\n",
    "\n",
    "        #diff=0\n",
    "        diff=torch.ones(1,device=device)\n",
    "        temp=0\n",
    "        #print(len(cutoff))\n",
    "        for j in range(no_of_image):\n",
    "            result=torch.norm(y_pred_unperturbed[j]-y_pred_perturbed[j])\n",
    "\n",
    "            if np.isinf(result.cpu().detach().numpy()) or np.isnan(result.cpu().detach().numpy())  :\n",
    "                result=torch.Tensor([1*10**38]).to(device)\n",
    "                print(\"infinity\")\n",
    "            if result.cpu().detach().numpy()<2**-52:\n",
    "                result=torch.Tensor([2**-52]).to(device)\n",
    "            diff*=result**(1/no_of_image)\n",
    "            \n",
    "            #print(j)\n",
    "            if result>cutoff[j] and i==no_of_repetition-1:\n",
    "                chaos+=1\n",
    "        if np.isinf(diff.cpu().detach().numpy()) or np.isnan(diff.cpu().detach().numpy()):\n",
    "            diff=torch.Tensor([1*10**38]).to(device)\n",
    "            print(\"infinity\")\n",
    "\n",
    "            \n",
    "        if diff.cpu().detach().numpy()<2**-52:\n",
    "            diff=torch.Tensor([2**-52]).to(device)\n",
    "        distance.append(diff.cpu().detach().numpy())\n",
    "\n",
    "    return distance,hidden_layer,chaos/no_of_image,ave_jac\n",
    "\n",
    "def visualisation(xfinals,epoch,name,bool):\n",
    "    plt.imshow(xfinals.reshape(N,N), cmap=\"gray\")\n",
    "    if bool:\n",
    "        plt.savefig(str(name)+\" epoch:\"+str(epoch+1)+\".jpg\")\n",
    "    plt.show()\n",
    "    print(str(name)+\" epoch:\"+str(epoch+1)+\".jpg\")\n",
    "    \n",
    "def divergence(values):\n",
    "    result=np.abs(values[-1]-values[-2])\n",
    "    if result>np.abs(values[2]-values[1]):\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def spectral_radius(sorted):\n",
    "    if len(sorted)==0:\n",
    "        return 0\n",
    "    return sorted[0]\n",
    "\n",
    "def spectral_distribution(input_matrix):\n",
    "    result=[]\n",
    "    count=0\n",
    "    s,v=torch.linalg.eig(input_matrix)\n",
    "    #return s\n",
    "    for i in range(len(s)):\n",
    "        if np.abs(s[i].cpu())<2**-52:\n",
    "            count+=1\n",
    "        \n",
    "    sorted, indices=torch.sort(torch.abs(s),dim=-1,descending=True)\n",
    "    #for index in indices.cpu():\n",
    "     #   if index<=len(s)-count:\n",
    "     #       result.append(s[index])\n",
    "    #print(sorted)\n",
    "    return s,sorted[:-count]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5e73d658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 60\u001b[0m\n\u001b[1;32m     57\u001b[0m model_clone\u001b[38;5;241m.\u001b[39mload_state_dict(copy\u001b[38;5;241m.\u001b[39mdeepcopy(model\u001b[38;5;241m.\u001b[39mstate_dict()))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m#spectral_radiuses=[]\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m x_train_perturbed,y_pred_perturbed,average_jacobian,spectral_radiuses,model_clone\u001b[38;5;241m=\u001b[39m\u001b[43mcalculation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mno_of_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43mno_of_repetition\u001b[49m\u001b[43m,\u001b[49m\u001b[43maverage_jacobian\u001b[49m\u001b[43m,\u001b[49m\u001b[43mspectral_radiuses\u001b[49m\u001b[43m,\u001b[49m\u001b[43mperturbed_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43mJ0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mJ1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m x_train_unperturbed,y_pred_unperturbed,average_jacobian,spectral_radiuses,model_clone\u001b[38;5;241m=\u001b[39mcalculation(model,no_of_images,no_of_repetition,average_jacobian,spectral_radiuses,input_image,epoch,mu,J0,J1)\n\u001b[1;32m     63\u001b[0m distance,hidden,chaos,ave_jac\u001b[38;5;241m=\u001b[39masymptotic_iteration(model_clone,y_pred_unperturbed,y_pred_perturbed,no_of_repetition,no_of_images,cutoff)\n",
      "Cell \u001b[0;32mIn[64], line 160\u001b[0m, in \u001b[0;36mcalculation\u001b[0;34m(model, no_of_images, no_of_repetition, ave_jacobian, spectral_radiuses, image, epoch, mu, J0, J1)\u001b[0m\n\u001b[1;32m    158\u001b[0m model_clone\u001b[38;5;241m.\u001b[39mload_state_dict(copy\u001b[38;5;241m.\u001b[39mdeepcopy(model\u001b[38;5;241m.\u001b[39mstate_dict()))\n\u001b[1;32m    159\u001b[0m distributions\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39masarray([])\n\u001b[0;32m--> 160\u001b[0m x_train,y_pred,model_clone\u001b[38;5;241m=\u001b[39m\u001b[43miteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_clone\u001b[49m\u001b[43m,\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43masymptotic_repetition\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m#ave_jac,distributions,jacobian,sorteds= asymptotic_jacobian(model_clone,x_train,no_of_images,no_of_repetition)\u001b[39;00m\n\u001b[1;32m    162\u001b[0m ave_jac\u001b[38;5;241m=\u001b[39m[]\n",
      "Cell \u001b[0;32mIn[64], line 58\u001b[0m, in \u001b[0;36miteration\u001b[0;34m(model, initial_inputs, no_of_repetition, epoch)\u001b[0m\n\u001b[1;32m     51\u001b[0m     x_train\u001b[38;5;241m=\u001b[39my_pred\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m#if i==0:\u001b[39;00m\n\u001b[1;32m     53\u001b[0m        \u001b[38;5;66;03m# name=\"progress asymptotic iteration:\"+str(i)+\" \"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m        \u001b[38;5;66;03m# visualisation(x_train[0].cpu().detach(),epoch,name,False)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m#if i%5==1:\u001b[39;00m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;66;03m#name=\"progress asymptotic iteration:\"+str(i)+\" \"\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m#visualisation(x_train[0].cpu().detach(),epoch,name,False)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     y_pred\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_train,y_pred,model\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[60], line 56\u001b[0m, in \u001b[0;36mAE.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, features):\n\u001b[0;32m---> 56\u001b[0m     reconstructed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minternals\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reconstructed\n",
      "Cell \u001b[0;32mIn[60], line 60\u001b[0m, in \u001b[0;36mAE.internals\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minternals\u001b[39m(\u001b[38;5;28mself\u001b[39m,features):\n\u001b[0;32m---> 60\u001b[0m     code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden\u001b[38;5;241m=\u001b[39mcode\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;66;03m#print(\"hidden state:\"+str(code))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[60], line 70\u001b[0m, in \u001b[0;36mAE.encoder\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencoder\u001b[39m(\u001b[38;5;28mself\u001b[39m,features):\n\u001b[1;32m     69\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_output_layer(features)\n\u001b[0;32m---> 70\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m code\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "spectral_radiuses=[]\n",
    "average_jacobian=[]\n",
    "xinfinity=[]\n",
    "xpinfinity=[]\n",
    "training_loss=[]\n",
    "validation_loss=[]\n",
    "asymptotic_dist=[]\n",
    "#diverge=[]\n",
    "start=time.time()\n",
    "print(\"running\")\n",
    "#strength=0.1\n",
    "#epochs=10\n",
    "mu=2\n",
    "dmu=1/epochs\n",
    "\n",
    "init_compute=True\n",
    "chao=[]\n",
    "noise_print=False\n",
    "repeats=100\n",
    "average_chaos=[]\n",
    "first_moment=[]\n",
    "second_moment=[]\n",
    "third_moment=[]\n",
    "fourth_moment=[]\n",
    "for i in range(repeats):\n",
    "    mu=1\n",
    "    chao=[]\n",
    "    asy_dist=[]\n",
    "    print(i)\n",
    "    for epoch in range(epochs+1):\n",
    "        loss = 0\n",
    "        #mu+=dmu\n",
    "        #print(mu)\n",
    "        if epoch==0:\n",
    "            for batch_features, _ in train_loader:\n",
    "            # reshape mini-batch data to [1000, 784] matrix\n",
    "            # load it to the active device\n",
    "                input_image =add_gaussian_noise(batch_features,noise_strength).view(batch_size, -1).to(device)\n",
    "        \n",
    "        model = AE(input_size,output_size,hidden_state_size,no_of_layer,mu,J0=J0,J1=J1).to(device)           \n",
    "        save_image=input_image\n",
    "        perturbed_inputs=add_gaussian_noise(input_image,perturbation_strength)\n",
    "        inits=0\n",
    "    \n",
    "        for value in model.parameters():\n",
    "\n",
    "            first,second,third,fourth=stats(value)\n",
    "            first_moment.append(first.cpu().detach().numpy())\n",
    "            second_moment.append(second.cpu().detach().numpy())\n",
    "            third_moment.append(third.cpu().detach().numpy())\n",
    "            fourth_moment.append(fourth.cpu().detach().numpy())\n",
    "            inits+=1\n",
    "        cutoff=[]\n",
    "        for j in range(no_of_images):\n",
    "            cutoff.append(torch.norm(perturbed_inputs[j]-input_image[j]))\n",
    "        model_clone=AE(input_size,input_size,hidden_state_size,no_of_layer,mu,J0=J0,J1=J1).to(device)\n",
    "        model_clone.load_state_dict(copy.deepcopy(model.state_dict()))\n",
    "\n",
    "        #spectral_radiuses=[]\n",
    "        x_train_perturbed,y_pred_perturbed,average_jacobian,spectral_radiuses,model_clone=calculation(model,no_of_images,no_of_repetition,average_jacobian,spectral_radiuses,perturbed_inputs,epoch,mu,J0,J1)\n",
    "        x_train_unperturbed,y_pred_unperturbed,average_jacobian,spectral_radiuses,model_clone=calculation(model,no_of_images,no_of_repetition,average_jacobian,spectral_radiuses,input_image,epoch,mu,J0,J1)\n",
    "\n",
    "        distance,hidden,chaos,ave_jac=asymptotic_iteration(model_clone,y_pred_unperturbed,y_pred_perturbed,no_of_repetition,no_of_images,cutoff)\n",
    "          \n",
    "        asy_dist.append(distance[-1])\n",
    "        chao.append(chaos)\n",
    "        #print(ave_jac)\n",
    "        mu+=dmu\n",
    "    print(\"here\\n\") \n",
    "    np.savetxt(\"average_chaos\"+str(i)+\".txt\",chao)\n",
    "    average_chaos.append(chao)\n",
    "    #print(average_chaos)\n",
    "    asymptotic_dist.append(asy_dist)\n",
    "    np.savetxt(\"asymptotic distance\"+str(i)+\".txt\",asy_dist)\n",
    "\n",
    "\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "\n",
    "#print(chao)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(moment):\n",
    "    moment=np.asarray(moment)\n",
    "    moment=moment.reshape(repeats,epochs+1,2)\n",
    "    #moment=np.mean(moment,axis=0)\n",
    "    moment=np.abs(moment)\n",
    "    moment=geometric_mean(moment)\n",
    "    return moment\n",
    "def moment_treatment(mu,moment,indicator):\n",
    "    \n",
    "    moment=np.asarray(moment)\n",
    "    moment=moment.reshape(repeats,epochs+1,2)\n",
    "    #moment=np.mean(moment,axis=0)\n",
    "    moment=np.abs(moment)\n",
    "    moment=geometric_mean(moment)\n",
    "    #if indicator==\"first\"\n",
    "        \n",
    "    plt.plot(mu,moment.T[0])\n",
    "    plt.ylabel(indicator+\" moment\")\n",
    "    plt.xlabel(\"exponent\")\n",
    "   \n",
    "    plt.title(indicator+\" moment vs exponent graph for t distribution first layer\")\n",
    "    np.savetxt(indicator+\" moment vs exponent graph for t distribution layer 1.txt\",moment.T[0])\n",
    "    plt.savefig(indicator+\" moment vs exponent graph for t distribution layer 1.jpg\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()\n",
    "    plt.plot(mu,moment.T[1])\n",
    "    plt.ylabel(indicator+\" moment\")\n",
    "    plt.xlabel(\"exponent\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title(indicator+\" moment vs exponent graph for t distribution second layer\")\n",
    "    np.savetxt(indicator+\" moment vs exponent graph for t distribution layer 2.txt\",moment.T[1])\n",
    "    plt.savefig(indicator+\" moment vs exponent graph for t distribution layer 2.jpg\")\n",
    "    plt.show()\n",
    "    \n",
    "def chaos_treatment(mu,chaos,indicator):\n",
    "    chaos=np.asarray(chaos)\n",
    "    if indicator==\"asymptotic distance\":\n",
    "        chaos=geometric_mean(chaos)\n",
    "        print(chaos.size)\n",
    "    else:\n",
    "        chaos=np.mean(chaos,axis=0)\n",
    "    \n",
    "    \n",
    "    plt.plot(mu,chaos)\n",
    "    plt.ylabel(indicator)\n",
    "    plt.xlabel(\"exponent\")\n",
    "    if indicator==\"asymptotic distance\":\n",
    "        print(\"here\")\n",
    "        plt.yscale(\"log\")\n",
    "    plt.title(indicator+\" vs exponent graph for t distribution \")\n",
    "    np.savetxt(indicator+\" vs exponent graph for t distribution.txt\",chaos)\n",
    "    plt.savefig(indicator+\" vs exponent graph for t distribution.jpg\")\n",
    "    plt.show()\n",
    " \n",
    "def geometric_mean(mean):\n",
    "    result=1\n",
    "    for i in range(len(mean)):\n",
    "        result*=mean[i]**(1/len(mean))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedc62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_value=np.linspace(1,2,epochs+1)\n",
    "\n",
    "    \n",
    "moment_treatment(mu_value,first_moment,\"first\")\n",
    "moment_treatment(mu_value,second_moment,\"second\")\n",
    "moment_treatment(mu_value,third_moment,\"third\")\n",
    "moment_treatment(mu_value,fourth_moment,\"fourth\")\n",
    "first_moment_1=processing(first_moment)\n",
    "second_moment_1=processing(second_moment)\n",
    "result=[]\n",
    "vertical=[]\n",
    "for i in range(len(first_moment_1)):\n",
    "    #print(first_moment_1[i]/second_moment_1[i])\n",
    "    result.append(first_moment_1[i]/second_moment_1[i])\n",
    "    vertical.append(1/second_moment_1[i])\n",
    "plt.plot(result[0],vertical[0])\n",
    "plt.ylabel(\"1/J1\")\n",
    "plt.xlabel(\"J0/J1\")\n",
    "plt.savefig(\"phase diagram first layer.jpg\")\n",
    "plt.show()\n",
    "plt.plot(result[1],vertical[1])\n",
    "plt.ylabel(\"1/J1\")\n",
    "plt.xlabel(\"J0/J1\")\n",
    "plt.savefig(\"phase diagram second layer.jpg\")\n",
    "#plt.savefig(\"\")\n",
    "#print(result)\n",
    "plt.show()\n",
    "chaos_treatment(mu_value,average_chaos,\"percentage chaos\")\n",
    "print(np.asarray(asymptotic_dist).size)\n",
    "chaos_treatment(mu_value,asymptotic_dist,\"asymptotic distance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74cff87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d1ab65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee311b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
